{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7aOzm2IR1Ts",
    "outputId": "b56e17ae-17e6-40d1-c688-f618ca0b847b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install llmtuner==0.5.1 -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nRkCz79MR4Ld",
    "outputId": "40227298-aa04-44b1-e50a-907c451ba800"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LLaMA-Factory'...\n",
      "remote: Enumerating objects: 7119, done.\u001b[K\n",
      "remote: Counting objects: 100% (1138/1138), done.\u001b[K\n",
      "remote: Compressing objects: 100% (440/440), done.\u001b[K\n",
      "remote: Total 7119 (delta 800), reused 984 (delta 695), pack-reused 5981\u001b[K\n",
      "Receiving objects: 100% (7119/7119), 204.90 MiB | 18.65 MiB/s, done.\n",
      "Resolving deltas: 100% (5146/5146), done.\n",
      "Updating files: 100% (141/141), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NBZqEIgkSIny",
    "outputId": "3e0c29f2-b0d5-45da-cf82-f226b204531a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/LLaMA-Factory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd LLaMA-Factory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q4By-mTTcS5E",
    "outputId": "71f39e97-15b4-4f1d-8713-d3e2f1144e01"
   },
   "outputs": [],
   "source": [
    "!pip install \"unsloth[colab] @ git+https://github.com/unslothai/unsloth.git\" -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3zdehDZCSLMT"
   },
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uMSeDqt2SR7R",
    "outputId": "e183fafe-7729-43af-fce3-235c15039dd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch>=1.13.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: transformers>=4.37.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (4.37.2)\n",
      "Requirement already satisfied: datasets>=2.14.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2.17.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.27.2)\n",
      "Requirement already satisfied: peft>=0.8.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.8.2)\n",
      "Requirement already satisfied: trl>=0.7.6 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.7.10)\n",
      "Requirement already satisfied: gradio<4.0.0,>=3.38.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (3.50.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.12.0)\n",
      "Requirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.7.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.1.99)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (4.25.2)\n",
      "Requirement already satisfied: jieba in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (0.42.1)\n",
      "Requirement already satisfied: rouge-chinese in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (1.0.3)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (3.8.1)\n",
      "Requirement already satisfied: uvicorn in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (0.27.1)\n",
      "Requirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (2.6.1)\n",
      "Requirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (0.109.2)\n",
      "Requirement already satisfied: sse-starlette in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (2.0.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (3.8.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1->-r requirements.txt (line 1)) (12.3.101)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.37.2->-r requirements.txt (line 2)) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.37.2->-r requirements.txt (line 2)) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.37.2->-r requirements.txt (line 2)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.37.2->-r requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.37.2->-r requirements.txt (line 2)) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.37.2->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.37.2->-r requirements.txt (line 2)) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.37.2->-r requirements.txt (line 2)) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.37.2->-r requirements.txt (line 2)) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.3->-r requirements.txt (line 3)) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.3->-r requirements.txt (line 3)) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.3->-r requirements.txt (line 3)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.3->-r requirements.txt (line 3)) (2.2.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.3->-r requirements.txt (line 3)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.3->-r requirements.txt (line 3)) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.3->-r requirements.txt (line 3)) (3.9.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->-r requirements.txt (line 4)) (5.9.0)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl>=0.7.6->-r requirements.txt (line 6)) (0.7.2)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (23.2.1)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (5.2.0)\n",
      "Requirement already satisfied: ffmpy in /opt/conda/lib/python3.10/site-packages (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (0.3.2)\n",
      "Requirement already satisfied: gradio-client==0.6.1 in /opt/conda/lib/python3.10/site-packages (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (0.6.1)\n",
      "Requirement already satisfied: httpx in /opt/conda/lib/python3.10/site-packages (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (0.26.0)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (6.1.1)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (2.1.3)\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (3.9.14)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (10.0.1)\n",
      "Requirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in /opt/conda/lib/python3.10/site-packages (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (0.0.9)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (2.10.0)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (11.0.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge-chinese->-r requirements.txt (line 13)) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->-r requirements.txt (line 14)) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->-r requirements.txt (line 14)) (1.3.2)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn->-r requirements.txt (line 15)) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 16)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /opt/conda/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 16)) (2.16.2)\n",
      "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /opt/conda/lib/python3.10/site-packages (from fastapi->-r requirements.txt (line 17)) (0.36.3)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from sse-starlette->-r requirements.txt (line 18)) (4.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 19)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 19)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 19)) (4.48.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 19)) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 19)) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 19)) (2.8.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (4.19.2)\n",
      "Requirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (0.12.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.3->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.3->-r requirements.txt (line 3)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.3->-r requirements.txt (line 3)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.3->-r requirements.txt (line 3)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.3->-r requirements.txt (line 3)) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.3->-r requirements.txt (line 3)) (4.0.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.14.3->-r requirements.txt (line 3)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.14.3->-r requirements.txt (line 3)) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.37.2->-r requirements.txt (line 2)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.37.2->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.37.2->-r requirements.txt (line 2)) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.37.2->-r requirements.txt (line 2)) (2023.11.17)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio->sse-starlette->-r requirements.txt (line 18)) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->sse-starlette->-r requirements.txt (line 18)) (1.2.0)\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.7.6->-r requirements.txt (line 6)) (0.15)\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.7.6->-r requirements.txt (line 6)) (13.7.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.7.6->-r requirements.txt (line 6)) (1.6.5)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (1.0.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.1->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (0.10.6)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl>=0.7.6->-r requirements.txt (line 6)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl>=0.7.6->-r requirements.txt (line 6)) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl>=0.7.6->-r requirements.txt (line 6)) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pGnRy7w-U2m4",
    "outputId": "fb347c2a-383f-4746-fec2-5ef695197b5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7860\n",
      "Running on public URL: https://74825fa5abdc0c29ca.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
      "02/15/2024 15:30:30 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "[INFO|training_args.py:1828] 2024-02-15 15:30:30,135 >> PyTorch: setting up devices\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1741: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "02/15/2024 15:30:30 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1\n",
      "  distributed training: True, compute dtype: torch.float16\n",
      "02/15/2024 15:30:30 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=True,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=saves/Custom/lora/train_2024-02-15-15-27-50/runs/Feb15_15-30-30_efd77714c8ba,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=cosine,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=saves/Custom/lora/train_2024-02-15-15-27-50,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=saves/Custom/lora/train_2024-02-15-15-27-50,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=100,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.16k/1.16k [00:00<00:00, 3.76MB/s]\n",
      "vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.78M/2.78M [00:00<00:00, 5.14MB/s]\n",
      "merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.67M/1.67M [00:00<00:00, 3.84MB/s]\n",
      "tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.03M/7.03M [00:00<00:00, 21.1MB/s]\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-02-15 15:30:33,306 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-02-15 15:30:33,307 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-02-15 15:30:33,307 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-02-15 15:30:33,307 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-02-15 15:30:33,307 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-02-15 15:30:33,307 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/tokenizer.json\n",
      "[WARNING|logging.py:314] 2024-02-15 15:30:33,624 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 661/661 [00:00<00:00, 3.93MB/s]\n",
      "[INFO|configuration_utils.py:729] 2024-02-15 15:30:33,927 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-02-15 15:30:33,929 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"Qwen/Qwen1.5-0.5B\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2816,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.24G/1.24G [00:11<00:00, 106MB/s]\n",
      "[INFO|modeling_utils.py:3476] 2024-02-15 15:30:46,482 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/model.safetensors\n",
      "[INFO|modeling_utils.py:1426] 2024-02-15 15:30:46,513 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:826] 2024-02-15 15:30:46,523 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4350] 2024-02-15 15:30:48,582 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4358] 2024-02-15 15:30:48,582 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen1.5-0.5B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "generation_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 138/138 [00:00<00:00, 828kB/s]\n",
      "[INFO|configuration_utils.py:781] 2024-02-15 15:30:48,910 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/generation_config.json\n",
      "[INFO|configuration_utils.py:826] 2024-02-15 15:30:48,911 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"max_new_tokens\": 2048\n",
      "}\n",
      "\n",
      "02/15/2024 15:30:48 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.\n",
      "02/15/2024 15:30:48 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "02/15/2024 15:30:48 - INFO - llmtuner.model.utils - Found linear modules: up_proj,q_proj,o_proj,v_proj,gate_proj,k_proj,down_proj\n",
      "02/15/2024 15:30:49 - INFO - llmtuner.model.loader - trainable params: 3784704 || all params: 467772416 || trainable%: 0.8091\n",
      "02/15/2024 15:30:49 - INFO - llmtuner.data.loader - Loading dataset alpaca_data_en_52k.json...\n",
      "Using custom data configuration default-3891c44fa6b71dbf\n",
      "Loading Dataset Infos from /opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "Generating dataset json (/root/.cache/huggingface/datasets/json/default-3891c44fa6b71dbf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-3891c44fa6b71dbf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading took 0.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "Generating train split\n",
      "Generating train split: 52002 examples [00:00, 206208.01 examples/s]\n",
      "Unable to verify splits sizes.\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-3891c44fa6b71dbf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "Converting format of dataset:   0%|             | 0/8000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-3891c44fa6b71dbf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a21bc0bf03e77228.arrow\n",
      "Converting format of dataset: 100%|â–ˆ| 8000/8000 [00:00<00:00, 79185.81 examples/\n",
      "02/15/2024 15:30:50 - INFO - llmtuner.data.loader - Loading dataset glaive_toolcall_10k.json...\n",
      "02/15/2024 15:30:50 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/glaive_toolcall_10k.json.\n",
      "Using custom data configuration default-c275bf43f3aa67e9\n",
      "Loading Dataset Infos from /opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "Generating dataset json (/root/.cache/huggingface/datasets/json/default-c275bf43f3aa67e9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-c275bf43f3aa67e9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading took 0.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "Generating train split\n",
      "Generating train split: 10000 examples [00:00, 19262.37 examples/s]\n",
      "Unable to verify splits sizes.\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-c275bf43f3aa67e9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "Converting format of dataset:   0%|             | 0/8000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-c275bf43f3aa67e9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6bfd3a330f1e9206.arrow\n",
      "Converting format of dataset: 100%|â–ˆ| 8000/8000 [00:00<00:00, 11132.74 examples/\n",
      "Running tokenizer on dataset:   0%|            | 0/16000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-3891c44fa6b71dbf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-47f656e72cef6a80.arrow\n",
      "Running tokenizer on dataset: 100%|â–ˆ| 16000/16000 [00:27<00:00, 576.84 examples/\n",
      "input_ids:\n",
      "[33975, 25, 20678, 2326, 10414, 369, 19429, 9314, 624, 71703, 25, 220, 16, 5142, 266, 264, 23831, 9968, 323, 1281, 2704, 311, 2924, 11260, 315, 25322, 323, 23880, 13, 715, 17, 13, 32818, 15502, 311, 2506, 697, 2487, 4541, 323, 3746, 13, 715, 18, 13, 2126, 3322, 6084, 323, 10306, 264, 12966, 6084, 9700, 13, 151643]\n",
      "inputs:\n",
      "Human: Give three tips for staying healthy.\n",
      "Assistant: 1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
      "2. Exercise regularly to keep your body active and strong. \n",
      "3. Get enough sleep and maintain a consistent sleep schedule.<|endoftext|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 16, 5142, 266, 264, 23831, 9968, 323, 1281, 2704, 311, 2924, 11260, 315, 25322, 323, 23880, 13, 715, 17, 13, 32818, 15502, 311, 2506, 697, 2487, 4541, 323, 3746, 13, 715, 18, 13, 2126, 3322, 6084, 323, 10306, 264, 12966, 6084, 9700, 13, 151643]\n",
      "labels:\n",
      "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
      "2. Exercise regularly to keep your body active and strong. \n",
      "3. Get enough sleep and maintain a consistent sleep schedule.<|endoftext|>\n",
      "[INFO|training_args.py:1828] 2024-02-15 15:31:24,522 >> PyTorch: setting up devices\n",
      "[INFO|trainer.py:571] 2024-02-15 15:31:25,067 >> Using auto half precision backend\n",
      "[INFO|trainer.py:1721] 2024-02-15 15:31:25,285 >> ***** Running training *****\n",
      "[INFO|trainer.py:1722] 2024-02-15 15:31:25,286 >>   Num examples = 16,000\n",
      "[INFO|trainer.py:1723] 2024-02-15 15:31:25,286 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1724] 2024-02-15 15:31:25,286 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1727] 2024-02-15 15:31:25,286 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1728] 2024-02-15 15:31:25,286 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:1729] 2024-02-15 15:31:25,286 >>   Total optimization steps = 1,000\n",
      "[INFO|trainer.py:1730] 2024-02-15 15:31:25,289 >>   Number of trainable parameters = 3,784,704\n",
      "02/15/2024 15:31:43 - INFO - llmtuner.extras.callbacks - {'loss': 1.2332, 'learning_rate': 4.9997e-05, 'epoch': 0.01}\n",
      "{'loss': 1.2332, 'learning_rate': 4.999691581204152e-05, 'epoch': 0.01}\n",
      "[INFO|trainer.py:1962] 2024-02-15 15:32:01,387 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "02/15/2024 15:32:01 - INFO - llmtuner.extras.callbacks - {'loss': 0.0000, 'learning_rate': 0.0000e+00, 'epoch': 0.02}\n",
      "{'train_runtime': 36.0979, 'train_samples_per_second': 886.478, 'train_steps_per_second': 27.702, 'train_loss': 1.250326931476593, 'epoch': 0.02}\n",
      "[INFO|trainer.py:2936] 2024-02-15 15:32:01,388 >> Saving model checkpoint to saves/Custom/lora/train_2024-02-15-15-27-50\n",
      "[INFO|configuration_utils.py:729] 2024-02-15 15:32:01,954 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-02-15 15:32:01,958 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2816,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-15 15:32:02,166 >> tokenizer config file saved in saves/Custom/lora/train_2024-02-15-15-27-50/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-15 15:32:02,167 >> Special tokens file saved in saves/Custom/lora/train_2024-02-15-15-27-50/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2493] 2024-02-15 15:32:02,167 >> added tokens file saved in saves/Custom/lora/train_2024-02-15-15-27-50/added_tokens.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       0.02\n",
      "  train_loss               =     1.2503\n",
      "  train_runtime            = 0:00:36.09\n",
      "  train_samples_per_second =    886.478\n",
      "  train_steps_per_second   =     27.702\n",
      "[INFO|modelcard.py:452] 2024-02-15 15:32:02,539 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
      "[INFO|training_args.py:1828] 2024-02-15 15:32:55,736 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:1566] 2024-02-15 15:32:55,737 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "02/15/2024 15:32:55 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "[INFO|training_args.py:1828] 2024-02-15 15:32:55,748 >> PyTorch: setting up devices\n",
      "Exception in thread Thread-10 (run_exp):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/workspace/LLaMA-Factory/src/llmtuner/train/tuner.py\", line 25, in run_exp\n",
      "    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n",
      "  File \"/workspace/LLaMA-Factory/src/llmtuner/hparams/parser.py\", line 200, in get_train_args\n",
      "    raise ValueError(\"Output directory already exists and is not empty. Please set `overwrite_output_dir`.\")\n",
      "ValueError: Output directory already exists and is not empty. Please set `overwrite_output_dir`.\n",
      "[INFO|training_args.py:1828] 2024-02-15 15:33:29,524 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:1566] 2024-02-15 15:33:29,525 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "02/15/2024 15:33:29 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "[INFO|training_args.py:1828] 2024-02-15 15:33:29,536 >> PyTorch: setting up devices\n",
      "02/15/2024 15:33:29 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1\n",
      "  distributed training: True, compute dtype: torch.float16\n",
      "02/15/2024 15:33:29 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=True,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=saves/Custom/lora/Owen_fc_t1/runs/Feb15_15-33-29_efd77714c8ba,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=cosine,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=saves/Custom/lora/Owen_fc_t1,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=saves/Custom/lora/Owen_fc_t1,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=100,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-02-15 15:33:29,776 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-02-15 15:33:29,776 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-02-15 15:33:29,776 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-02-15 15:33:29,777 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-02-15 15:33:29,777 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-02-15 15:33:29,777 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/tokenizer.json\n",
      "[WARNING|logging.py:314] 2024-02-15 15:33:30,061 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:729] 2024-02-15 15:33:30,427 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-02-15 15:33:30,428 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"Qwen/Qwen1.5-0.5B\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2816,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "02/15/2024 15:33:30 - WARNING - llmtuner.model.patcher - FlashAttention2 is not installed.\n",
      "[INFO|modeling_utils.py:3476] 2024-02-15 15:33:30,431 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/model.safetensors\n",
      "[INFO|modeling_utils.py:1426] 2024-02-15 15:33:30,447 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:826] 2024-02-15 15:33:30,449 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4350] 2024-02-15 15:33:32,121 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4358] 2024-02-15 15:33:32,122 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen1.5-0.5B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:781] 2024-02-15 15:33:32,357 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/generation_config.json\n",
      "[INFO|configuration_utils.py:826] 2024-02-15 15:33:32,357 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"max_new_tokens\": 2048\n",
      "}\n",
      "\n",
      "02/15/2024 15:33:32 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.\n",
      "02/15/2024 15:33:32 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "02/15/2024 15:33:32 - INFO - llmtuner.model.utils - Found linear modules: up_proj,q_proj,o_proj,v_proj,gate_proj,k_proj,down_proj\n",
      "02/15/2024 15:33:32 - INFO - llmtuner.model.loader - trainable params: 3784704 || all params: 467772416 || trainable%: 0.8091\n",
      "02/15/2024 15:33:32 - INFO - llmtuner.data.loader - Loading dataset alpaca_data_en_52k.json...\n",
      "Using custom data configuration default-3891c44fa6b71dbf\n",
      "Loading Dataset Infos from /opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3891c44fa6b71dbf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-3891c44fa6b71dbf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3891c44fa6b71dbf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3891c44fa6b71dbf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a21bc0bf03e77228.arrow\n",
      "02/15/2024 15:33:33 - INFO - llmtuner.data.loader - Loading dataset glaive_toolcall_10k.json...\n",
      "02/15/2024 15:33:33 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/glaive_toolcall_10k.json.\n",
      "Using custom data configuration default-c275bf43f3aa67e9\n",
      "Loading Dataset Infos from /opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-c275bf43f3aa67e9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-c275bf43f3aa67e9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-c275bf43f3aa67e9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-c275bf43f3aa67e9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6bfd3a330f1e9206.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3891c44fa6b71dbf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-47f656e72cef6a80.arrow\n",
      "input_ids:\n",
      "[33975, 25, 20678, 2326, 10414, 369, 19429, 9314, 624, 71703, 25, 220, 16, 5142, 266, 264, 23831, 9968, 323, 1281, 2704, 311, 2924, 11260, 315, 25322, 323, 23880, 13, 715, 17, 13, 32818, 15502, 311, 2506, 697, 2487, 4541, 323, 3746, 13, 715, 18, 13, 2126, 3322, 6084, 323, 10306, 264, 12966, 6084, 9700, 13, 151643]\n",
      "inputs:\n",
      "Human: Give three tips for staying healthy.\n",
      "Assistant: 1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
      "2. Exercise regularly to keep your body active and strong. \n",
      "3. Get enough sleep and maintain a consistent sleep schedule.<|endoftext|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 16, 5142, 266, 264, 23831, 9968, 323, 1281, 2704, 311, 2924, 11260, 315, 25322, 323, 23880, 13, 715, 17, 13, 32818, 15502, 311, 2506, 697, 2487, 4541, 323, 3746, 13, 715, 18, 13, 2126, 3322, 6084, 323, 10306, 264, 12966, 6084, 9700, 13, 151643]\n",
      "labels:\n",
      "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
      "2. Exercise regularly to keep your body active and strong. \n",
      "3. Get enough sleep and maintain a consistent sleep schedule.<|endoftext|>\n",
      "[INFO|training_args.py:1828] 2024-02-15 15:33:38,205 >> PyTorch: setting up devices\n",
      "[INFO|trainer.py:571] 2024-02-15 15:33:38,528 >> Using auto half precision backend\n",
      "[INFO|trainer.py:1721] 2024-02-15 15:33:38,738 >> ***** Running training *****\n",
      "[INFO|trainer.py:1722] 2024-02-15 15:33:38,738 >>   Num examples = 16,000\n",
      "[INFO|trainer.py:1723] 2024-02-15 15:33:38,738 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1724] 2024-02-15 15:33:38,738 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1727] 2024-02-15 15:33:38,738 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1728] 2024-02-15 15:33:38,738 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:1729] 2024-02-15 15:33:38,738 >>   Total optimization steps = 1,000\n",
      "[INFO|trainer.py:1730] 2024-02-15 15:33:38,742 >>   Number of trainable parameters = 3,784,704\n",
      "02/15/2024 15:33:56 - INFO - llmtuner.extras.callbacks - {'loss': 1.2332, 'learning_rate': 4.9997e-05, 'epoch': 0.01}\n",
      "{'loss': 1.2332, 'learning_rate': 4.999691581204152e-05, 'epoch': 0.01}\n",
      "02/15/2024 15:34:19 - INFO - llmtuner.extras.callbacks - {'loss': 1.0154, 'learning_rate': 4.9988e-05, 'epoch': 0.02}\n",
      "{'loss': 1.0154, 'learning_rate': 4.998766400914329e-05, 'epoch': 0.02}\n",
      "02/15/2024 15:34:43 - INFO - llmtuner.extras.callbacks - {'loss': 1.0374, 'learning_rate': 4.9972e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0374, 'learning_rate': 4.9972246874049254e-05, 'epoch': 0.03}\n",
      "02/15/2024 15:35:04 - INFO - llmtuner.extras.callbacks - {'loss': 0.8964, 'learning_rate': 4.9951e-05, 'epoch': 0.04}\n",
      "{'loss': 0.8964, 'learning_rate': 4.995066821070679e-05, 'epoch': 0.04}\n",
      "02/15/2024 15:35:25 - INFO - llmtuner.extras.callbacks - {'loss': 1.0285, 'learning_rate': 4.9923e-05, 'epoch': 0.05}\n",
      "{'loss': 1.0285, 'learning_rate': 4.99229333433282e-05, 'epoch': 0.05}\n",
      "02/15/2024 15:35:44 - INFO - llmtuner.extras.callbacks - {'loss': 0.9904, 'learning_rate': 4.9889e-05, 'epoch': 0.06}\n",
      "{'loss': 0.9904, 'learning_rate': 4.9889049115077005e-05, 'epoch': 0.06}\n",
      "02/15/2024 15:36:07 - INFO - llmtuner.extras.callbacks - {'loss': 0.9899, 'learning_rate': 4.9849e-05, 'epoch': 0.07}\n",
      "{'loss': 0.9899, 'learning_rate': 4.98490238863795e-05, 'epoch': 0.07}\n",
      "02/15/2024 15:36:27 - INFO - llmtuner.extras.callbacks - {'loss': 1.0784, 'learning_rate': 4.9803e-05, 'epoch': 0.08}\n",
      "{'loss': 1.0784, 'learning_rate': 4.980286753286195e-05, 'epoch': 0.08}\n",
      "02/15/2024 15:36:52 - INFO - llmtuner.extras.callbacks - {'loss': 0.9965, 'learning_rate': 4.9751e-05, 'epoch': 0.09}\n",
      "{'loss': 0.9965, 'learning_rate': 4.975059144291394e-05, 'epoch': 0.09}\n",
      "02/15/2024 15:37:12 - INFO - llmtuner.extras.callbacks - {'loss': 0.9534, 'learning_rate': 4.9692e-05, 'epoch': 0.10}\n",
      "{'loss': 0.9534, 'learning_rate': 4.9692208514878444e-05, 'epoch': 0.1}\n",
      "02/15/2024 15:37:30 - INFO - llmtuner.extras.callbacks - {'loss': 0.8956, 'learning_rate': 4.9628e-05, 'epoch': 0.11}\n",
      "{'loss': 0.8956, 'learning_rate': 4.962773315386935e-05, 'epoch': 0.11}\n",
      "02/15/2024 15:37:53 - INFO - llmtuner.extras.callbacks - {'loss': 0.9290, 'learning_rate': 4.9557e-05, 'epoch': 0.12}\n",
      "{'loss': 0.929, 'learning_rate': 4.9557181268217227e-05, 'epoch': 0.12}\n",
      "02/15/2024 15:38:16 - INFO - llmtuner.extras.callbacks - {'loss': 0.9149, 'learning_rate': 4.9481e-05, 'epoch': 0.13}\n",
      "{'loss': 0.9149, 'learning_rate': 4.9480570265544144e-05, 'epoch': 0.13}\n",
      "02/15/2024 15:38:41 - INFO - llmtuner.extras.callbacks - {'loss': 0.9147, 'learning_rate': 4.9398e-05, 'epoch': 0.14}\n",
      "{'loss': 0.9147, 'learning_rate': 4.939791904846869e-05, 'epoch': 0.14}\n",
      "02/15/2024 15:38:59 - INFO - llmtuner.extras.callbacks - {'loss': 0.9484, 'learning_rate': 4.9309e-05, 'epoch': 0.15}\n",
      "{'loss': 0.9484, 'learning_rate': 4.9309248009941914e-05, 'epoch': 0.15}\n",
      "02/15/2024 15:39:23 - INFO - llmtuner.extras.callbacks - {'loss': 0.9087, 'learning_rate': 4.9215e-05, 'epoch': 0.16}\n",
      "{'loss': 0.9087, 'learning_rate': 4.9214579028215776e-05, 'epoch': 0.16}\n",
      "02/15/2024 15:39:43 - INFO - llmtuner.extras.callbacks - {'loss': 0.8351, 'learning_rate': 4.9114e-05, 'epoch': 0.17}\n",
      "{'loss': 0.8351, 'learning_rate': 4.9113935461444955e-05, 'epoch': 0.17}\n",
      "02/15/2024 15:40:08 - INFO - llmtuner.extras.callbacks - {'loss': 0.9510, 'learning_rate': 4.9007e-05, 'epoch': 0.18}\n",
      "{'loss': 0.951, 'learning_rate': 4.900734214192358e-05, 'epoch': 0.18}\n",
      "02/15/2024 15:40:28 - INFO - llmtuner.extras.callbacks - {'loss': 0.8931, 'learning_rate': 4.8895e-05, 'epoch': 0.19}\n",
      "{'loss': 0.8931, 'learning_rate': 4.8894825369958255e-05, 'epoch': 0.19}\n",
      "02/15/2024 15:40:51 - INFO - llmtuner.extras.callbacks - {'loss': 0.8961, 'learning_rate': 4.8776e-05, 'epoch': 0.20}\n",
      "{'loss': 0.8961, 'learning_rate': 4.877641290737884e-05, 'epoch': 0.2}\n",
      "[INFO|trainer.py:2936] 2024-02-15 15:40:51,598 >> Saving model checkpoint to saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-100\n",
      "[INFO|configuration_utils.py:729] 2024-02-15 15:40:51,984 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-02-15 15:40:51,985 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2816,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-15 15:40:52,033 >> tokenizer config file saved in saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-15 15:40:52,034 >> Special tokens file saved in saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-100/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2493] 2024-02-15 15:40:52,034 >> added tokens file saved in saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-100/added_tokens.json\n",
      "02/15/2024 15:41:13 - INFO - llmtuner.extras.callbacks - {'loss': 0.9636, 'learning_rate': 4.8652e-05, 'epoch': 0.21}\n",
      "{'loss': 0.9636, 'learning_rate': 4.8652133970688636e-05, 'epoch': 0.21}\n",
      "02/15/2024 15:41:36 - INFO - llmtuner.extras.callbacks - {'loss': 0.8255, 'learning_rate': 4.8522e-05, 'epoch': 0.22}\n",
      "{'loss': 0.8255, 'learning_rate': 4.852201922385564e-05, 'epoch': 0.22}\n",
      "02/15/2024 15:42:01 - INFO - llmtuner.extras.callbacks - {'loss': 0.9251, 'learning_rate': 4.8386e-05, 'epoch': 0.23}\n",
      "{'loss': 0.9251, 'learning_rate': 4.838610077074669e-05, 'epoch': 0.23}\n",
      "02/15/2024 15:42:21 - INFO - llmtuner.extras.callbacks - {'loss': 0.9072, 'learning_rate': 4.8244e-05, 'epoch': 0.24}\n",
      "{'loss': 0.9072, 'learning_rate': 4.8244412147206284e-05, 'epoch': 0.24}\n",
      "02/15/2024 15:42:44 - INFO - llmtuner.extras.callbacks - {'loss': 0.9218, 'learning_rate': 4.8097e-05, 'epoch': 0.25}\n",
      "{'loss': 0.9218, 'learning_rate': 4.8096988312782174e-05, 'epoch': 0.25}\n",
      "02/15/2024 15:43:04 - INFO - llmtuner.extras.callbacks - {'loss': 0.9576, 'learning_rate': 4.7944e-05, 'epoch': 0.26}\n",
      "{'loss': 0.9576, 'learning_rate': 4.794386564209953e-05, 'epoch': 0.26}\n",
      "02/15/2024 15:43:27 - INFO - llmtuner.extras.callbacks - {'loss': 0.8811, 'learning_rate': 4.7785e-05, 'epoch': 0.27}\n",
      "{'loss': 0.8811, 'learning_rate': 4.7785081915886134e-05, 'epoch': 0.27}\n",
      "02/15/2024 15:43:53 - INFO - llmtuner.extras.callbacks - {'loss': 0.9106, 'learning_rate': 4.7621e-05, 'epoch': 0.28}\n",
      "{'loss': 0.9106, 'learning_rate': 4.762067631165049e-05, 'epoch': 0.28}\n",
      "02/15/2024 15:44:15 - INFO - llmtuner.extras.callbacks - {'loss': 0.8288, 'learning_rate': 4.7451e-05, 'epoch': 0.29}\n",
      "{'loss': 0.8288, 'learning_rate': 4.745068939401539e-05, 'epoch': 0.29}\n",
      "02/15/2024 15:44:41 - INFO - llmtuner.extras.callbacks - {'loss': 0.9170, 'learning_rate': 4.7275e-05, 'epoch': 0.30}\n",
      "{'loss': 0.917, 'learning_rate': 4.72751631047092e-05, 'epoch': 0.3}\n",
      "02/15/2024 15:45:03 - INFO - llmtuner.extras.callbacks - {'loss': 0.9006, 'learning_rate': 4.7094e-05, 'epoch': 0.31}\n",
      "{'loss': 0.9006, 'learning_rate': 4.709414075221734e-05, 'epoch': 0.31}\n",
      "02/15/2024 15:45:26 - INFO - llmtuner.extras.callbacks - {'loss': 0.8013, 'learning_rate': 4.6908e-05, 'epoch': 0.32}\n",
      "{'loss': 0.8013, 'learning_rate': 4.690766700109659e-05, 'epoch': 0.32}\n",
      "02/15/2024 15:45:45 - INFO - llmtuner.extras.callbacks - {'loss': 0.8699, 'learning_rate': 4.6716e-05, 'epoch': 0.33}\n",
      "{'loss': 0.8699, 'learning_rate': 4.671578786095478e-05, 'epoch': 0.33}\n",
      "02/15/2024 15:46:04 - INFO - llmtuner.extras.callbacks - {'loss': 0.9166, 'learning_rate': 4.6519e-05, 'epoch': 0.34}\n",
      "{'loss': 0.9166, 'learning_rate': 4.65185506750986e-05, 'epoch': 0.34}\n",
      "02/15/2024 15:46:27 - INFO - llmtuner.extras.callbacks - {'loss': 0.8602, 'learning_rate': 4.6316e-05, 'epoch': 0.35}\n",
      "{'loss': 0.8602, 'learning_rate': 4.6316004108852305e-05, 'epoch': 0.35}\n",
      "02/15/2024 15:46:48 - INFO - llmtuner.extras.callbacks - {'loss': 0.9279, 'learning_rate': 4.6108e-05, 'epoch': 0.36}\n",
      "{'loss': 0.9279, 'learning_rate': 4.610819813755038e-05, 'epoch': 0.36}\n",
      "02/15/2024 15:47:09 - INFO - llmtuner.extras.callbacks - {'loss': 0.8374, 'learning_rate': 4.5895e-05, 'epoch': 0.37}\n",
      "{'loss': 0.8374, 'learning_rate': 4.5895184034206765e-05, 'epoch': 0.37}\n",
      "02/15/2024 15:47:31 - INFO - llmtuner.extras.callbacks - {'loss': 0.9461, 'learning_rate': 4.5677e-05, 'epoch': 0.38}\n",
      "{'loss': 0.9461, 'learning_rate': 4.567701435686404e-05, 'epoch': 0.38}\n",
      "02/15/2024 15:47:49 - INFO - llmtuner.extras.callbacks - {'loss': 0.9822, 'learning_rate': 4.5454e-05, 'epoch': 0.39}\n",
      "{'loss': 0.9822, 'learning_rate': 4.545374293562559e-05, 'epoch': 0.39}\n",
      "02/15/2024 15:48:09 - INFO - llmtuner.extras.callbacks - {'loss': 0.9187, 'learning_rate': 4.5225e-05, 'epoch': 0.40}\n",
      "{'loss': 0.9187, 'learning_rate': 4.522542485937369e-05, 'epoch': 0.4}\n",
      "[INFO|trainer.py:2936] 2024-02-15 15:48:09,904 >> Saving model checkpoint to saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-200\n",
      "[INFO|configuration_utils.py:729] 2024-02-15 15:48:10,275 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-02-15 15:48:10,277 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2816,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-15 15:48:10,329 >> tokenizer config file saved in saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-15 15:48:10,329 >> Special tokens file saved in saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-200/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2493] 2024-02-15 15:48:10,329 >> added tokens file saved in saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-200/added_tokens.json\n",
      "02/15/2024 15:48:34 - INFO - llmtuner.extras.callbacks - {'loss': 1.0287, 'learning_rate': 4.4992e-05, 'epoch': 0.41}\n",
      "{'loss': 1.0287, 'learning_rate': 4.499211646217727e-05, 'epoch': 0.41}\n",
      "02/15/2024 15:48:56 - INFO - llmtuner.extras.callbacks - {'loss': 0.8160, 'learning_rate': 4.4754e-05, 'epoch': 0.42}\n",
      "{'loss': 0.816, 'learning_rate': 4.4753875309392266e-05, 'epoch': 0.42}\n",
      "02/15/2024 15:49:19 - INFO - llmtuner.extras.callbacks - {'loss': 0.8687, 'learning_rate': 4.4511e-05, 'epoch': 0.43}\n",
      "{'loss': 0.8687, 'learning_rate': 4.451076018345825e-05, 'epoch': 0.43}\n",
      "02/15/2024 15:49:41 - INFO - llmtuner.extras.callbacks - {'loss': 0.9275, 'learning_rate': 4.4263e-05, 'epoch': 0.44}\n",
      "{'loss': 0.9275, 'learning_rate': 4.426283106939474e-05, 'epoch': 0.44}\n",
      "02/15/2024 15:50:03 - INFO - llmtuner.extras.callbacks - {'loss': 0.9474, 'learning_rate': 4.4010e-05, 'epoch': 0.45}\n",
      "{'loss': 0.9474, 'learning_rate': 4.401014914000078e-05, 'epoch': 0.45}\n",
      "02/15/2024 15:50:24 - INFO - llmtuner.extras.callbacks - {'loss': 0.8296, 'learning_rate': 4.3753e-05, 'epoch': 0.46}\n",
      "{'loss': 0.8296, 'learning_rate': 4.375277674076149e-05, 'epoch': 0.46}\n",
      "02/15/2024 15:50:48 - INFO - llmtuner.extras.callbacks - {'loss': 0.8831, 'learning_rate': 4.3491e-05, 'epoch': 0.47}\n",
      "{'loss': 0.8831, 'learning_rate': 4.349077737446525e-05, 'epoch': 0.47}\n",
      "02/15/2024 15:51:07 - INFO - llmtuner.extras.callbacks - {'loss': 1.0252, 'learning_rate': 4.3224e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0252, 'learning_rate': 4.3224215685535294e-05, 'epoch': 0.48}\n",
      "02/15/2024 15:51:31 - INFO - llmtuner.extras.callbacks - {'loss': 0.9663, 'learning_rate': 4.2953e-05, 'epoch': 0.49}\n",
      "{'loss': 0.9663, 'learning_rate': 4.295315744407972e-05, 'epoch': 0.49}\n",
      "02/15/2024 15:51:52 - INFO - llmtuner.extras.callbacks - {'loss': 0.8573, 'learning_rate': 4.2678e-05, 'epoch': 0.50}\n",
      "{'loss': 0.8573, 'learning_rate': 4.267766952966369e-05, 'epoch': 0.5}\n",
      "02/15/2024 15:52:13 - INFO - llmtuner.extras.callbacks - {'loss': 0.8912, 'learning_rate': 4.2398e-05, 'epoch': 0.51}\n",
      "{'loss': 0.8912, 'learning_rate': 4.2397819914807856e-05, 'epoch': 0.51}\n",
      "02/15/2024 15:52:34 - INFO - llmtuner.extras.callbacks - {'loss': 0.9105, 'learning_rate': 4.2114e-05, 'epoch': 0.52}\n",
      "{'loss': 0.9105, 'learning_rate': 4.211367764821722e-05, 'epoch': 0.52}\n",
      "02/15/2024 15:52:58 - INFO - llmtuner.extras.callbacks - {'loss': 0.8692, 'learning_rate': 4.1825e-05, 'epoch': 0.53}\n",
      "{'loss': 0.8692, 'learning_rate': 4.182531283774434e-05, 'epoch': 0.53}\n",
      "02/15/2024 15:53:19 - INFO - llmtuner.extras.callbacks - {'loss': 0.8286, 'learning_rate': 4.1533e-05, 'epoch': 0.54}\n",
      "{'loss': 0.8286, 'learning_rate': 4.1532796633091296e-05, 'epoch': 0.54}\n",
      "02/15/2024 15:53:40 - INFO - llmtuner.extras.callbacks - {'loss': 0.9152, 'learning_rate': 4.1236e-05, 'epoch': 0.55}\n",
      "{'loss': 0.9152, 'learning_rate': 4.123620120825459e-05, 'epoch': 0.55}\n",
      "02/15/2024 15:54:04 - INFO - llmtuner.extras.callbacks - {'loss': 0.9844, 'learning_rate': 4.0936e-05, 'epoch': 0.56}\n",
      "{'loss': 0.9844, 'learning_rate': 4.093559974371725e-05, 'epoch': 0.56}\n",
      "02/15/2024 15:54:22 - INFO - llmtuner.extras.callbacks - {'loss': 0.8819, 'learning_rate': 4.0631e-05, 'epoch': 0.57}\n",
      "{'loss': 0.8819, 'learning_rate': 4.063106640839264e-05, 'epoch': 0.57}\n",
      "02/15/2024 15:54:43 - INFO - llmtuner.extras.callbacks - {'loss': 0.9167, 'learning_rate': 4.0323e-05, 'epoch': 0.58}\n",
      "{'loss': 0.9167, 'learning_rate': 4.0322676341324415e-05, 'epoch': 0.58}\n",
      "02/15/2024 15:55:03 - INFO - llmtuner.extras.callbacks - {'loss': 0.8261, 'learning_rate': 4.0011e-05, 'epoch': 0.59}\n",
      "{'loss': 0.8261, 'learning_rate': 4.0010505633147106e-05, 'epoch': 0.59}\n",
      "02/15/2024 15:55:24 - INFO - llmtuner.extras.callbacks - {'loss': 0.9248, 'learning_rate': 3.9695e-05, 'epoch': 0.60}\n",
      "{'loss': 0.9248, 'learning_rate': 3.969463130731183e-05, 'epoch': 0.6}\n",
      "[INFO|trainer.py:2936] 2024-02-15 15:55:24,624 >> Saving model checkpoint to saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-300\n",
      "[INFO|configuration_utils.py:729] 2024-02-15 15:55:25,169 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-02-15 15:55:25,171 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2816,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-15 15:55:25,223 >> tokenizer config file saved in saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-300/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-15 15:55:25,224 >> Special tokens file saved in saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-300/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2493] 2024-02-15 15:55:25,224 >> added tokens file saved in saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-300/added_tokens.json\n",
      "02/15/2024 15:55:49 - INFO - llmtuner.extras.callbacks - {'loss': 0.9974, 'learning_rate': 3.9375e-05, 'epoch': 0.61}\n",
      "{'loss': 0.9974, 'learning_rate': 3.937513130108197e-05, 'epoch': 0.61}\n",
      "02/15/2024 15:56:12 - INFO - llmtuner.extras.callbacks - {'loss': 0.8477, 'learning_rate': 3.9052e-05, 'epoch': 0.62}\n",
      "{'loss': 0.8477, 'learning_rate': 3.905208444630327e-05, 'epoch': 0.62}\n",
      "02/15/2024 15:56:32 - INFO - llmtuner.extras.callbacks - {'loss': 0.9251, 'learning_rate': 3.8726e-05, 'epoch': 0.63}\n",
      "{'loss': 0.9251, 'learning_rate': 3.87255704499533e-05, 'epoch': 0.63}\n",
      "02/15/2024 15:56:55 - INFO - llmtuner.extras.callbacks - {'loss': 0.9061, 'learning_rate': 3.8396e-05, 'epoch': 0.64}\n",
      "{'loss': 0.9061, 'learning_rate': 3.8395669874474915e-05, 'epoch': 0.64}\n",
      "02/15/2024 15:57:18 - INFO - llmtuner.extras.callbacks - {'loss': 0.8933, 'learning_rate': 3.8062e-05, 'epoch': 0.65}\n",
      "{'loss': 0.8933, 'learning_rate': 3.8062464117898724e-05, 'epoch': 0.65}\n",
      "02/15/2024 15:57:43 - INFO - llmtuner.extras.callbacks - {'loss': 0.8962, 'learning_rate': 3.7726e-05, 'epoch': 0.66}\n",
      "{'loss': 0.8962, 'learning_rate': 3.7726035393759285e-05, 'epoch': 0.66}\n",
      "02/15/2024 15:58:04 - INFO - llmtuner.extras.callbacks - {'loss': 0.8396, 'learning_rate': 3.7386e-05, 'epoch': 0.67}\n",
      "{'loss': 0.8396, 'learning_rate': 3.7386466710810194e-05, 'epoch': 0.67}\n",
      "02/15/2024 15:58:27 - INFO - llmtuner.extras.callbacks - {'loss': 0.7853, 'learning_rate': 3.7044e-05, 'epoch': 0.68}\n",
      "{'loss': 0.7853, 'learning_rate': 3.704384185254288e-05, 'epoch': 0.68}\n",
      "02/15/2024 15:58:45 - INFO - llmtuner.extras.callbacks - {'loss': 0.9338, 'learning_rate': 3.6698e-05, 'epoch': 0.69}\n",
      "{'loss': 0.9338, 'learning_rate': 3.6698245356514335e-05, 'epoch': 0.69}\n",
      "02/15/2024 15:59:05 - INFO - llmtuner.extras.callbacks - {'loss': 0.8782, 'learning_rate': 3.6350e-05, 'epoch': 0.70}\n",
      "{'loss': 0.8782, 'learning_rate': 3.634976249348867e-05, 'epoch': 0.7}\n",
      "02/15/2024 15:59:28 - INFO - llmtuner.extras.callbacks - {'loss': 0.8629, 'learning_rate': 3.5998e-05, 'epoch': 0.71}\n",
      "{'loss': 0.8629, 'learning_rate': 3.599847924639788e-05, 'epoch': 0.71}\n",
      "02/15/2024 15:59:52 - INFO - llmtuner.extras.callbacks - {'loss': 0.9394, 'learning_rate': 3.5644e-05, 'epoch': 0.72}\n",
      "{'loss': 0.9394, 'learning_rate': 3.564448228912682e-05, 'epoch': 0.72}\n",
      "02/15/2024 16:00:17 - INFO - llmtuner.extras.callbacks - {'loss': 0.9409, 'learning_rate': 3.5288e-05, 'epoch': 0.73}\n",
      "{'loss': 0.9409, 'learning_rate': 3.528785896512772e-05, 'epoch': 0.73}\n",
      "02/15/2024 16:00:39 - INFO - llmtuner.extras.callbacks - {'loss': 0.8980, 'learning_rate': 3.4929e-05, 'epoch': 0.74}\n",
      "{'loss': 0.898, 'learning_rate': 3.4928697265869515e-05, 'epoch': 0.74}\n",
      "02/15/2024 16:01:02 - INFO - llmtuner.extras.callbacks - {'loss': 0.9446, 'learning_rate': 3.4567e-05, 'epoch': 0.75}\n",
      "{'loss': 0.9446, 'learning_rate': 3.456708580912725e-05, 'epoch': 0.75}\n",
      "02/15/2024 16:01:22 - INFO - llmtuner.extras.callbacks - {'loss': 0.8980, 'learning_rate': 3.4203e-05, 'epoch': 0.76}\n",
      "{'loss': 0.898, 'learning_rate': 3.4203113817116957e-05, 'epoch': 0.76}\n",
      "02/15/2024 16:01:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.9005, 'learning_rate': 3.3837e-05, 'epoch': 0.77}\n",
      "{'loss': 0.9005, 'learning_rate': 3.383687109448143e-05, 'epoch': 0.77}\n",
      "02/15/2024 16:02:06 - INFO - llmtuner.extras.callbacks - {'loss': 0.8822, 'learning_rate': 3.3468e-05, 'epoch': 0.78}\n",
      "{'loss': 0.8822, 'learning_rate': 3.346844800613229e-05, 'epoch': 0.78}\n",
      "02/15/2024 16:02:29 - INFO - llmtuner.extras.callbacks - {'loss': 0.8773, 'learning_rate': 3.3098e-05, 'epoch': 0.79}\n",
      "{'loss': 0.8773, 'learning_rate': 3.309793545495374e-05, 'epoch': 0.79}\n",
      "02/15/2024 16:02:49 - INFO - llmtuner.extras.callbacks - {'loss': 0.8985, 'learning_rate': 3.2725e-05, 'epoch': 0.80}\n",
      "{'loss': 0.8985, 'learning_rate': 3.272542485937369e-05, 'epoch': 0.8}\n",
      "[INFO|trainer.py:2936] 2024-02-15 16:02:49,822 >> Saving model checkpoint to saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-400\n",
      "[INFO|configuration_utils.py:729] 2024-02-15 16:02:50,246 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-02-15 16:02:50,248 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2816,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-15 16:02:50,306 >> tokenizer config file saved in saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-15 16:02:50,306 >> Special tokens file saved in saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-400/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2493] 2024-02-15 16:02:50,307 >> added tokens file saved in saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-400/added_tokens.json\n",
      "02/15/2024 16:03:10 - INFO - llmtuner.extras.callbacks - {'loss': 0.8737, 'learning_rate': 3.2351e-05, 'epoch': 0.81}\n",
      "{'loss': 0.8737, 'learning_rate': 3.23510081308076e-05, 'epoch': 0.81}\n",
      "02/15/2024 16:03:30 - INFO - llmtuner.extras.callbacks - {'loss': 0.7628, 'learning_rate': 3.1975e-05, 'epoch': 0.82}\n",
      "{'loss': 0.7628, 'learning_rate': 3.1974777650980735e-05, 'epoch': 0.82}\n",
      "02/15/2024 16:03:50 - INFO - llmtuner.extras.callbacks - {'loss': 0.7923, 'learning_rate': 3.1597e-05, 'epoch': 0.83}\n",
      "{'loss': 0.7923, 'learning_rate': 3.1596826249134324e-05, 'epoch': 0.83}\n",
      "02/15/2024 16:04:10 - INFO - llmtuner.extras.callbacks - {'loss': 0.8029, 'learning_rate': 3.1217e-05, 'epoch': 0.84}\n",
      "{'loss': 0.8029, 'learning_rate': 3.121724717912138e-05, 'epoch': 0.84}\n",
      "02/15/2024 16:04:27 - INFO - llmtuner.extras.callbacks - {'loss': 0.8299, 'learning_rate': 3.0836e-05, 'epoch': 0.85}\n",
      "{'loss': 0.8299, 'learning_rate': 3.083613409639764e-05, 'epoch': 0.85}\n",
      "02/15/2024 16:04:49 - INFO - llmtuner.extras.callbacks - {'loss': 0.8587, 'learning_rate': 3.0454e-05, 'epoch': 0.86}\n",
      "{'loss': 0.8587, 'learning_rate': 3.045358103491357e-05, 'epoch': 0.86}\n",
      "02/15/2024 16:05:10 - INFO - llmtuner.extras.callbacks - {'loss': 0.8729, 'learning_rate': 3.0070e-05, 'epoch': 0.87}\n",
      "{'loss': 0.8729, 'learning_rate': 3.0069682383912813e-05, 'epoch': 0.87}\n",
      "02/15/2024 16:05:29 - INFO - llmtuner.extras.callbacks - {'loss': 0.7692, 'learning_rate': 2.9685e-05, 'epoch': 0.88}\n",
      "{'loss': 0.7692, 'learning_rate': 2.9684532864643122e-05, 'epoch': 0.88}\n",
      "02/15/2024 16:05:46 - INFO - llmtuner.extras.callbacks - {'loss': 0.8966, 'learning_rate': 2.9298e-05, 'epoch': 0.89}\n",
      "{'loss': 0.8966, 'learning_rate': 2.929822750698524e-05, 'epoch': 0.89}\n",
      "02/15/2024 16:06:09 - INFO - llmtuner.extras.callbacks - {'loss': 0.8922, 'learning_rate': 2.8911e-05, 'epoch': 0.90}\n",
      "{'loss': 0.8922, 'learning_rate': 2.8910861626005776e-05, 'epoch': 0.9}\n",
      "02/15/2024 16:06:31 - INFO - llmtuner.extras.callbacks - {'loss': 0.8821, 'learning_rate': 2.8523e-05, 'epoch': 0.91}\n",
      "{'loss': 0.8821, 'learning_rate': 2.8522530798439567e-05, 'epoch': 0.91}\n",
      "02/15/2024 16:06:55 - INFO - llmtuner.extras.callbacks - {'loss': 0.8680, 'learning_rate': 2.8133e-05, 'epoch': 0.92}\n",
      "{'loss': 0.868, 'learning_rate': 2.8133330839107608e-05, 'epoch': 0.92}\n",
      "02/15/2024 16:07:18 - INFO - llmtuner.extras.callbacks - {'loss': 0.8622, 'learning_rate': 2.7743e-05, 'epoch': 0.93}\n",
      "{'loss': 0.8622, 'learning_rate': 2.774335777727613e-05, 'epoch': 0.93}\n",
      "02/15/2024 16:07:41 - INFO - llmtuner.extras.callbacks - {'loss': 0.8506, 'learning_rate': 2.7353e-05, 'epoch': 0.94}\n",
      "{'loss': 0.8506, 'learning_rate': 2.7352707832962865e-05, 'epoch': 0.94}\n",
      "02/15/2024 16:08:01 - INFO - llmtuner.extras.callbacks - {'loss': 0.8185, 'learning_rate': 2.6961e-05, 'epoch': 0.95}\n",
      "{'loss': 0.8185, 'learning_rate': 2.6961477393196126e-05, 'epoch': 0.95}\n",
      "02/15/2024 16:08:23 - INFO - llmtuner.extras.callbacks - {'loss': 0.9513, 'learning_rate': 2.6570e-05, 'epoch': 0.96}\n",
      "{'loss': 0.9513, 'learning_rate': 2.656976298823284e-05, 'epoch': 0.96}\n",
      "02/15/2024 16:08:41 - INFO - llmtuner.extras.callbacks - {'loss': 0.8697, 'learning_rate': 2.6178e-05, 'epoch': 0.97}\n",
      "{'loss': 0.8697, 'learning_rate': 2.6177661267741065e-05, 'epoch': 0.97}\n",
      "02/15/2024 16:09:03 - INFO - llmtuner.extras.callbacks - {'loss': 0.8154, 'learning_rate': 2.5785e-05, 'epoch': 0.98}\n",
      "{'loss': 0.8154, 'learning_rate': 2.578526897695321e-05, 'epoch': 0.98}\n",
      "02/15/2024 16:09:26 - INFO - llmtuner.extras.callbacks - {'loss': 0.8625, 'learning_rate': 2.5393e-05, 'epoch': 0.99}\n",
      "{'loss': 0.8625, 'learning_rate': 2.539268293279552e-05, 'epoch': 0.99}\n",
      "02/15/2024 16:09:49 - INFO - llmtuner.extras.callbacks - {'loss': 1.0086, 'learning_rate': 2.5000e-05, 'epoch': 1.00}\n",
      "{'loss': 1.0086, 'learning_rate': 2.5e-05, 'epoch': 1.0}\n",
      "[INFO|trainer.py:2936] 2024-02-15 16:09:49,304 >> Saving model checkpoint to saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-500\n",
      "[INFO|configuration_utils.py:729] 2024-02-15 16:09:49,691 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-02-15 16:09:49,693 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2816,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-15 16:09:49,743 >> tokenizer config file saved in saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-15 16:09:49,743 >> Special tokens file saved in saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2493] 2024-02-15 16:09:49,744 >> added tokens file saved in saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-500/added_tokens.json\n",
      "02/15/2024 16:10:10 - INFO - llmtuner.extras.callbacks - {'loss': 0.8776, 'learning_rate': 2.4607e-05, 'epoch': 1.01}\n",
      "{'loss': 0.8776, 'learning_rate': 2.460731706720449e-05, 'epoch': 1.01}\n",
      "02/15/2024 16:10:31 - INFO - llmtuner.extras.callbacks - {'loss': 0.9265, 'learning_rate': 2.4215e-05, 'epoch': 1.02}\n",
      "{'loss': 0.9265, 'learning_rate': 2.4214731023046793e-05, 'epoch': 1.02}\n",
      "02/15/2024 16:10:55 - INFO - llmtuner.extras.callbacks - {'loss': 0.7812, 'learning_rate': 2.3822e-05, 'epoch': 1.03}\n",
      "{'loss': 0.7812, 'learning_rate': 2.3822338732258937e-05, 'epoch': 1.03}\n",
      "02/15/2024 16:11:14 - INFO - llmtuner.extras.callbacks - {'loss': 0.8707, 'learning_rate': 2.3430e-05, 'epoch': 1.04}\n",
      "{'loss': 0.8707, 'learning_rate': 2.3430237011767167e-05, 'epoch': 1.04}\n",
      "02/15/2024 16:11:36 - INFO - llmtuner.extras.callbacks - {'loss': 0.8820, 'learning_rate': 2.3039e-05, 'epoch': 1.05}\n",
      "{'loss': 0.882, 'learning_rate': 2.303852260680388e-05, 'epoch': 1.05}\n",
      "02/15/2024 16:11:59 - INFO - llmtuner.extras.callbacks - {'loss': 0.7885, 'learning_rate': 2.2647e-05, 'epoch': 1.06}\n",
      "{'loss': 0.7885, 'learning_rate': 2.2647292167037144e-05, 'epoch': 1.06}\n",
      "02/15/2024 16:12:21 - INFO - llmtuner.extras.callbacks - {'loss': 0.9275, 'learning_rate': 2.2257e-05, 'epoch': 1.07}\n",
      "{'loss': 0.9275, 'learning_rate': 2.225664222272387e-05, 'epoch': 1.07}\n",
      "02/15/2024 16:12:44 - INFO - llmtuner.extras.callbacks - {'loss': 0.8853, 'learning_rate': 2.1867e-05, 'epoch': 1.08}\n",
      "{'loss': 0.8853, 'learning_rate': 2.186666916089239e-05, 'epoch': 1.08}\n",
      "02/15/2024 16:13:03 - INFO - llmtuner.extras.callbacks - {'loss': 0.9246, 'learning_rate': 2.1477e-05, 'epoch': 1.09}\n",
      "{'loss': 0.9246, 'learning_rate': 2.1477469201560435e-05, 'epoch': 1.09}\n",
      "02/15/2024 16:13:23 - INFO - llmtuner.extras.callbacks - {'loss': 0.8488, 'learning_rate': 2.1089e-05, 'epoch': 1.10}\n",
      "{'loss': 0.8488, 'learning_rate': 2.1089138373994223e-05, 'epoch': 1.1}\n",
      "02/15/2024 16:13:43 - INFO - llmtuner.extras.callbacks - {'loss': 0.8163, 'learning_rate': 2.0702e-05, 'epoch': 1.11}\n",
      "{'loss': 0.8163, 'learning_rate': 2.070177249301476e-05, 'epoch': 1.11}\n",
      "02/15/2024 16:14:06 - INFO - llmtuner.extras.callbacks - {'loss': 0.8603, 'learning_rate': 2.0315e-05, 'epoch': 1.12}\n",
      "{'loss': 0.8603, 'learning_rate': 2.031546713535688e-05, 'epoch': 1.12}\n",
      "02/15/2024 16:14:26 - INFO - llmtuner.extras.callbacks - {'loss': 0.7762, 'learning_rate': 1.9930e-05, 'epoch': 1.13}\n",
      "{'loss': 0.7762, 'learning_rate': 1.9930317616087196e-05, 'epoch': 1.13}\n",
      "02/15/2024 16:14:46 - INFO - llmtuner.extras.callbacks - {'loss': 0.7816, 'learning_rate': 1.9546e-05, 'epoch': 1.14}\n",
      "{'loss': 0.7816, 'learning_rate': 1.9546418965086442e-05, 'epoch': 1.14}\n",
      "02/15/2024 16:15:03 - INFO - llmtuner.extras.callbacks - {'loss': 0.9537, 'learning_rate': 1.9164e-05, 'epoch': 1.15}\n",
      "{'loss': 0.9537, 'learning_rate': 1.9163865903602374e-05, 'epoch': 1.15}\n",
      "02/15/2024 16:15:23 - INFO - llmtuner.extras.callbacks - {'loss': 0.8317, 'learning_rate': 1.8783e-05, 'epoch': 1.16}\n",
      "{'loss': 0.8317, 'learning_rate': 1.8782752820878634e-05, 'epoch': 1.16}\n",
      "02/15/2024 16:15:46 - INFO - llmtuner.extras.callbacks - {'loss': 0.8070, 'learning_rate': 1.8403e-05, 'epoch': 1.17}\n",
      "{'loss': 0.807, 'learning_rate': 1.8403173750865685e-05, 'epoch': 1.17}\n",
      "02/15/2024 16:16:09 - INFO - llmtuner.extras.callbacks - {'loss': 0.8607, 'learning_rate': 1.8025e-05, 'epoch': 1.18}\n",
      "{'loss': 0.8607, 'learning_rate': 1.802522234901927e-05, 'epoch': 1.18}\n",
      "02/15/2024 16:16:32 - INFO - llmtuner.extras.callbacks - {'loss': 0.8888, 'learning_rate': 1.7649e-05, 'epoch': 1.19}\n",
      "{'loss': 0.8888, 'learning_rate': 1.7648991869192405e-05, 'epoch': 1.19}\n",
      "02/15/2024 16:16:55 - INFO - llmtuner.extras.callbacks - {'loss': 0.9462, 'learning_rate': 1.7275e-05, 'epoch': 1.20}\n",
      "{'loss': 0.9462, 'learning_rate': 1.7274575140626318e-05, 'epoch': 1.2}\n",
      "[INFO|trainer.py:2936] 2024-02-15 16:16:55,576 >> Saving model checkpoint to saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-600\n",
      "[INFO|configuration_utils.py:729] 2024-02-15 16:16:55,978 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-02-15 16:16:55,980 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2816,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-15 16:16:56,028 >> tokenizer config file saved in saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-15 16:16:56,028 >> Special tokens file saved in saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-600/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2493] 2024-02-15 16:16:56,028 >> added tokens file saved in saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-600/added_tokens.json\n",
      "02/15/2024 16:17:20 - INFO - llmtuner.extras.callbacks - {'loss': 0.8288, 'learning_rate': 1.6902e-05, 'epoch': 1.21}\n",
      "{'loss': 0.8288, 'learning_rate': 1.690206454504627e-05, 'epoch': 1.21}\n",
      "02/15/2024 16:17:38 - INFO - llmtuner.extras.callbacks - {'loss': 0.8517, 'learning_rate': 1.6532e-05, 'epoch': 1.22}\n",
      "{'loss': 0.8517, 'learning_rate': 1.6531551993867717e-05, 'epoch': 1.22}\n",
      "02/15/2024 16:17:56 - INFO - llmtuner.extras.callbacks - {'loss': 0.9904, 'learning_rate': 1.6163e-05, 'epoch': 1.23}\n",
      "{'loss': 0.9904, 'learning_rate': 1.6163128905518578e-05, 'epoch': 1.23}\n",
      "02/15/2024 16:18:17 - INFO - llmtuner.extras.callbacks - {'loss': 0.8300, 'learning_rate': 1.5797e-05, 'epoch': 1.24}\n",
      "{'loss': 0.83, 'learning_rate': 1.5796886182883053e-05, 'epoch': 1.24}\n",
      "02/15/2024 16:18:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.8425, 'learning_rate': 1.5433e-05, 'epoch': 1.25}\n",
      "{'loss': 0.8425, 'learning_rate': 1.5432914190872757e-05, 'epoch': 1.25}\n",
      "02/15/2024 16:19:02 - INFO - llmtuner.extras.callbacks - {'loss': 0.8771, 'learning_rate': 1.5071e-05, 'epoch': 1.26}\n",
      "{'loss': 0.8771, 'learning_rate': 1.5071302734130489e-05, 'epoch': 1.26}\n",
      "02/15/2024 16:19:23 - INFO - llmtuner.extras.callbacks - {'loss': 0.9105, 'learning_rate': 1.4712e-05, 'epoch': 1.27}\n",
      "{'loss': 0.9105, 'learning_rate': 1.4712141034872282e-05, 'epoch': 1.27}\n",
      "02/15/2024 16:19:48 - INFO - llmtuner.extras.callbacks - {'loss': 0.8156, 'learning_rate': 1.4356e-05, 'epoch': 1.28}\n",
      "{'loss': 0.8156, 'learning_rate': 1.4355517710873184e-05, 'epoch': 1.28}\n",
      "02/15/2024 16:20:09 - INFO - llmtuner.extras.callbacks - {'loss': 0.8448, 'learning_rate': 1.4002e-05, 'epoch': 1.29}\n",
      "{'loss': 0.8448, 'learning_rate': 1.4001520753602121e-05, 'epoch': 1.29}\n",
      "02/15/2024 16:20:31 - INFO - llmtuner.extras.callbacks - {'loss': 0.8743, 'learning_rate': 1.3650e-05, 'epoch': 1.30}\n",
      "{'loss': 0.8743, 'learning_rate': 1.3650237506511331e-05, 'epoch': 1.3}\n",
      "02/15/2024 16:20:55 - INFO - llmtuner.extras.callbacks - {'loss': 0.8890, 'learning_rate': 1.3302e-05, 'epoch': 1.31}\n",
      "{'loss': 0.889, 'learning_rate': 1.330175464348567e-05, 'epoch': 1.31}\n",
      "02/15/2024 16:21:17 - INFO - llmtuner.extras.callbacks - {'loss': 0.7970, 'learning_rate': 1.2956e-05, 'epoch': 1.32}\n",
      "{'loss': 0.797, 'learning_rate': 1.2956158147457115e-05, 'epoch': 1.32}\n",
      "02/15/2024 16:21:40 - INFO - llmtuner.extras.callbacks - {'loss': 0.8929, 'learning_rate': 1.2614e-05, 'epoch': 1.33}\n",
      "{'loss': 0.8929, 'learning_rate': 1.261353328918981e-05, 'epoch': 1.33}\n",
      "02/15/2024 16:22:03 - INFO - llmtuner.extras.callbacks - {'loss': 0.8695, 'learning_rate': 1.2274e-05, 'epoch': 1.34}\n",
      "{'loss': 0.8695, 'learning_rate': 1.2273964606240718e-05, 'epoch': 1.34}\n",
      "02/15/2024 16:22:27 - INFO - llmtuner.extras.callbacks - {'loss': 0.9639, 'learning_rate': 1.1938e-05, 'epoch': 1.35}\n",
      "{'loss': 0.9639, 'learning_rate': 1.1937535882101281e-05, 'epoch': 1.35}\n",
      "02/15/2024 16:22:47 - INFO - llmtuner.extras.callbacks - {'loss': 0.8936, 'learning_rate': 1.1604e-05, 'epoch': 1.36}\n",
      "{'loss': 0.8936, 'learning_rate': 1.1604330125525079e-05, 'epoch': 1.36}\n",
      "02/15/2024 16:23:04 - INFO - llmtuner.extras.callbacks - {'loss': 0.8805, 'learning_rate': 1.1274e-05, 'epoch': 1.37}\n",
      "{'loss': 0.8805, 'learning_rate': 1.1274429550046704e-05, 'epoch': 1.37}\n",
      "02/15/2024 16:23:25 - INFO - llmtuner.extras.callbacks - {'loss': 0.8835, 'learning_rate': 1.0948e-05, 'epoch': 1.38}\n",
      "{'loss': 0.8835, 'learning_rate': 1.0947915553696742e-05, 'epoch': 1.38}\n",
      "02/15/2024 16:23:48 - INFO - llmtuner.extras.callbacks - {'loss': 0.8194, 'learning_rate': 1.0625e-05, 'epoch': 1.39}\n",
      "{'loss': 0.8194, 'learning_rate': 1.0624868698918045e-05, 'epoch': 1.39}\n",
      "02/15/2024 16:24:11 - INFO - llmtuner.extras.callbacks - {'loss': 0.8309, 'learning_rate': 1.0305e-05, 'epoch': 1.40}\n",
      "{'loss': 0.8309, 'learning_rate': 1.0305368692688174e-05, 'epoch': 1.4}\n",
      "[INFO|trainer.py:2936] 2024-02-15 16:24:11,542 >> Saving model checkpoint to saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-700\n",
      "[INFO|configuration_utils.py:729] 2024-02-15 16:24:12,167 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-02-15 16:24:12,169 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2816,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-15 16:24:12,229 >> tokenizer config file saved in saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-700/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-15 16:24:12,229 >> Special tokens file saved in saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-700/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2493] 2024-02-15 16:24:12,230 >> added tokens file saved in saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-700/added_tokens.json\n",
      "02/15/2024 16:24:34 - INFO - llmtuner.extras.callbacks - {'loss': 0.7933, 'learning_rate': 9.9895e-06, 'epoch': 1.41}\n",
      "{'loss': 0.7933, 'learning_rate': 9.989494366852904e-06, 'epoch': 1.41}\n",
      "02/15/2024 16:24:58 - INFO - llmtuner.extras.callbacks - {'loss': 0.8685, 'learning_rate': 9.6773e-06, 'epoch': 1.42}\n",
      "{'loss': 0.8685, 'learning_rate': 9.677323658675594e-06, 'epoch': 1.42}\n",
      "02/15/2024 16:25:21 - INFO - llmtuner.extras.callbacks - {'loss': 0.8786, 'learning_rate': 9.3689e-06, 'epoch': 1.43}\n",
      "{'loss': 0.8786, 'learning_rate': 9.368933591607378e-06, 'epoch': 1.43}\n",
      "02/15/2024 16:25:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.9410, 'learning_rate': 9.0644e-06, 'epoch': 1.44}\n",
      "{'loss': 0.941, 'learning_rate': 9.064400256282757e-06, 'epoch': 1.44}\n",
      "02/15/2024 16:26:05 - INFO - llmtuner.extras.callbacks - {'loss': 0.8095, 'learning_rate': 8.7638e-06, 'epoch': 1.45}\n",
      "{'loss': 0.8095, 'learning_rate': 8.763798791745411e-06, 'epoch': 1.45}\n",
      "02/15/2024 16:26:30 - INFO - llmtuner.extras.callbacks - {'loss': 0.8712, 'learning_rate': 8.4672e-06, 'epoch': 1.46}\n",
      "{'loss': 0.8712, 'learning_rate': 8.467203366908707e-06, 'epoch': 1.46}\n",
      "02/15/2024 16:26:53 - INFO - llmtuner.extras.callbacks - {'loss': 0.7994, 'learning_rate': 8.1747e-06, 'epoch': 1.47}\n",
      "{'loss': 0.7994, 'learning_rate': 8.174687162255672e-06, 'epoch': 1.47}\n",
      "02/15/2024 16:27:14 - INFO - llmtuner.extras.callbacks - {'loss': 0.8219, 'learning_rate': 7.8863e-06, 'epoch': 1.48}\n",
      "{'loss': 0.8219, 'learning_rate': 7.886322351782783e-06, 'epoch': 1.48}\n",
      "02/15/2024 16:27:34 - INFO - llmtuner.extras.callbacks - {'loss': 0.8903, 'learning_rate': 7.6022e-06, 'epoch': 1.49}\n",
      "{'loss': 0.8903, 'learning_rate': 7.602180085192143e-06, 'epoch': 1.49}\n",
      "02/15/2024 16:27:54 - INFO - llmtuner.extras.callbacks - {'loss': 0.8059, 'learning_rate': 7.3223e-06, 'epoch': 1.50}\n",
      "{'loss': 0.8059, 'learning_rate': 7.3223304703363135e-06, 'epoch': 1.5}\n",
      "02/15/2024 16:28:14 - INFO - llmtuner.extras.callbacks - {'loss': 0.7909, 'learning_rate': 7.0468e-06, 'epoch': 1.51}\n",
      "{'loss': 0.7909, 'learning_rate': 7.046842555920283e-06, 'epoch': 1.51}\n",
      "02/15/2024 16:28:32 - INFO - llmtuner.extras.callbacks - {'loss': 0.8582, 'learning_rate': 6.7758e-06, 'epoch': 1.52}\n",
      "{'loss': 0.8582, 'learning_rate': 6.775784314464717e-06, 'epoch': 1.52}\n",
      "02/15/2024 16:28:53 - INFO - llmtuner.extras.callbacks - {'loss': 0.8166, 'learning_rate': 6.5092e-06, 'epoch': 1.53}\n",
      "{'loss': 0.8166, 'learning_rate': 6.509222625534755e-06, 'epoch': 1.53}\n",
      "02/15/2024 16:29:15 - INFO - llmtuner.extras.callbacks - {'loss': 0.9448, 'learning_rate': 6.2472e-06, 'epoch': 1.54}\n",
      "{'loss': 0.9448, 'learning_rate': 6.247223259238511e-06, 'epoch': 1.54}\n",
      "02/15/2024 16:29:38 - INFO - llmtuner.extras.callbacks - {'loss': 0.8635, 'learning_rate': 5.9899e-06, 'epoch': 1.55}\n",
      "{'loss': 0.8635, 'learning_rate': 5.989850859999227e-06, 'epoch': 1.55}\n",
      "02/15/2024 16:30:00 - INFO - llmtuner.extras.callbacks - {'loss': 0.8977, 'learning_rate': 5.7372e-06, 'epoch': 1.56}\n",
      "{'loss': 0.8977, 'learning_rate': 5.737168930605272e-06, 'epoch': 1.56}\n",
      "02/15/2024 16:30:21 - INFO - llmtuner.extras.callbacks - {'loss': 0.8543, 'learning_rate': 5.4892e-06, 'epoch': 1.57}\n",
      "{'loss': 0.8543, 'learning_rate': 5.489239816541755e-06, 'epoch': 1.57}\n",
      "02/15/2024 16:30:45 - INFO - llmtuner.extras.callbacks - {'loss': 0.8446, 'learning_rate': 5.2461e-06, 'epoch': 1.58}\n",
      "{'loss': 0.8446, 'learning_rate': 5.24612469060774e-06, 'epoch': 1.58}\n",
      "02/15/2024 16:31:05 - INFO - llmtuner.extras.callbacks - {'loss': 0.8651, 'learning_rate': 5.0079e-06, 'epoch': 1.59}\n",
      "{'loss': 0.8651, 'learning_rate': 5.007883537822736e-06, 'epoch': 1.59}\n",
      "02/15/2024 16:31:26 - INFO - llmtuner.extras.callbacks - {'loss': 0.8736, 'learning_rate': 4.7746e-06, 'epoch': 1.60}\n",
      "{'loss': 0.8736, 'learning_rate': 4.7745751406263165e-06, 'epoch': 1.6}\n",
      "[INFO|trainer.py:2936] 2024-02-15 16:31:26,056 >> Saving model checkpoint to saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-800\n",
      "[INFO|configuration_utils.py:729] 2024-02-15 16:31:26,436 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-02-15 16:31:26,438 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2816,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-15 16:31:26,482 >> tokenizer config file saved in saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-800/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-15 16:31:26,483 >> Special tokens file saved in saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-800/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2493] 2024-02-15 16:31:26,483 >> added tokens file saved in saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-800/added_tokens.json\n",
      "02/15/2024 16:31:49 - INFO - llmtuner.extras.callbacks - {'loss': 0.7561, 'learning_rate': 4.5463e-06, 'epoch': 1.61}\n",
      "{'loss': 0.7561, 'learning_rate': 4.54625706437441e-06, 'epoch': 1.61}\n",
      "02/15/2024 16:32:09 - INFO - llmtuner.extras.callbacks - {'loss': 0.8185, 'learning_rate': 4.3230e-06, 'epoch': 1.62}\n",
      "{'loss': 0.8185, 'learning_rate': 4.322985643135952e-06, 'epoch': 1.62}\n",
      "02/15/2024 16:32:32 - INFO - llmtuner.extras.callbacks - {'loss': 0.8215, 'learning_rate': 4.1048e-06, 'epoch': 1.63}\n",
      "{'loss': 0.8215, 'learning_rate': 4.104815965793249e-06, 'epoch': 1.63}\n",
      "02/15/2024 16:32:53 - INFO - llmtuner.extras.callbacks - {'loss': 0.8366, 'learning_rate': 3.8918e-06, 'epoch': 1.64}\n",
      "{'loss': 0.8366, 'learning_rate': 3.891801862449629e-06, 'epoch': 1.64}\n",
      "02/15/2024 16:33:15 - INFO - llmtuner.extras.callbacks - {'loss': 0.7073, 'learning_rate': 3.6840e-06, 'epoch': 1.65}\n",
      "{'loss': 0.7073, 'learning_rate': 3.6839958911476957e-06, 'epoch': 1.65}\n",
      "02/15/2024 16:33:36 - INFO - llmtuner.extras.callbacks - {'loss': 0.8626, 'learning_rate': 3.4814e-06, 'epoch': 1.66}\n",
      "{'loss': 0.8626, 'learning_rate': 3.4814493249014116e-06, 'epoch': 1.66}\n",
      "02/15/2024 16:33:57 - INFO - llmtuner.extras.callbacks - {'loss': 0.8947, 'learning_rate': 3.2842e-06, 'epoch': 1.67}\n",
      "{'loss': 0.8947, 'learning_rate': 3.284212139045223e-06, 'epoch': 1.67}\n",
      "02/15/2024 16:34:19 - INFO - llmtuner.extras.callbacks - {'loss': 0.7661, 'learning_rate': 3.0923e-06, 'epoch': 1.68}\n",
      "{'loss': 0.7661, 'learning_rate': 3.092332998903416e-06, 'epoch': 1.68}\n",
      "02/15/2024 16:34:40 - INFO - llmtuner.extras.callbacks - {'loss': 0.7731, 'learning_rate': 2.9059e-06, 'epoch': 1.69}\n",
      "{'loss': 0.7731, 'learning_rate': 2.9058592477826636e-06, 'epoch': 1.69}\n",
      "02/15/2024 16:35:02 - INFO - llmtuner.extras.callbacks - {'loss': 0.9148, 'learning_rate': 2.7248e-06, 'epoch': 1.70}\n",
      "{'loss': 0.9148, 'learning_rate': 2.7248368952908053e-06, 'epoch': 1.7}\n",
      "02/15/2024 16:35:23 - INFO - llmtuner.extras.callbacks - {'loss': 0.8122, 'learning_rate': 2.5493e-06, 'epoch': 1.71}\n",
      "{'loss': 0.8122, 'learning_rate': 2.5493106059846116e-06, 'epoch': 1.71}\n",
      "02/15/2024 16:35:44 - INFO - llmtuner.extras.callbacks - {'loss': 0.7455, 'learning_rate': 2.3793e-06, 'epoch': 1.72}\n",
      "{'loss': 0.7455, 'learning_rate': 2.379323688349516e-06, 'epoch': 1.72}\n",
      "02/15/2024 16:36:07 - INFO - llmtuner.extras.callbacks - {'loss': 0.9325, 'learning_rate': 2.2149e-06, 'epoch': 1.73}\n",
      "{'loss': 0.9325, 'learning_rate': 2.2149180841138676e-06, 'epoch': 1.73}\n",
      "02/15/2024 16:36:33 - INFO - llmtuner.extras.callbacks - {'loss': 0.9294, 'learning_rate': 2.0561e-06, 'epoch': 1.74}\n",
      "{'loss': 0.9294, 'learning_rate': 2.0561343579004715e-06, 'epoch': 1.74}\n",
      "02/15/2024 16:36:52 - INFO - llmtuner.extras.callbacks - {'loss': 0.7995, 'learning_rate': 1.9030e-06, 'epoch': 1.75}\n",
      "{'loss': 0.7995, 'learning_rate': 1.9030116872178316e-06, 'epoch': 1.75}\n",
      "02/15/2024 16:37:14 - INFO - llmtuner.extras.callbacks - {'loss': 0.8902, 'learning_rate': 1.7556e-06, 'epoch': 1.76}\n",
      "{'loss': 0.8902, 'learning_rate': 1.7555878527937164e-06, 'epoch': 1.76}\n",
      "02/15/2024 16:37:39 - INFO - llmtuner.extras.callbacks - {'loss': 0.9461, 'learning_rate': 1.6139e-06, 'epoch': 1.77}\n",
      "{'loss': 0.9461, 'learning_rate': 1.6138992292533183e-06, 'epoch': 1.77}\n",
      "02/15/2024 16:38:00 - INFO - llmtuner.extras.callbacks - {'loss': 0.8671, 'learning_rate': 1.4780e-06, 'epoch': 1.78}\n",
      "{'loss': 0.8671, 'learning_rate': 1.4779807761443636e-06, 'epoch': 1.78}\n",
      "02/15/2024 16:38:23 - INFO - llmtuner.extras.callbacks - {'loss': 0.9075, 'learning_rate': 1.3479e-06, 'epoch': 1.79}\n",
      "{'loss': 0.9075, 'learning_rate': 1.3478660293113676e-06, 'epoch': 1.79}\n",
      "02/15/2024 16:38:47 - INFO - llmtuner.extras.callbacks - {'loss': 0.9130, 'learning_rate': 1.2236e-06, 'epoch': 1.80}\n",
      "{'loss': 0.913, 'learning_rate': 1.2235870926211619e-06, 'epoch': 1.8}\n",
      "[INFO|trainer.py:2936] 2024-02-15 16:38:47,042 >> Saving model checkpoint to saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-900\n",
      "[INFO|configuration_utils.py:729] 2024-02-15 16:38:47,659 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-02-15 16:38:47,661 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2816,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-15 16:38:47,715 >> tokenizer config file saved in saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-900/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-15 16:38:47,715 >> Special tokens file saved in saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-900/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2493] 2024-02-15 16:38:47,715 >> added tokens file saved in saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-900/added_tokens.json\n",
      "02/15/2024 16:39:08 - INFO - llmtuner.extras.callbacks - {'loss': 0.7545, 'learning_rate': 1.1052e-06, 'epoch': 1.81}\n",
      "{'loss': 0.7545, 'learning_rate': 1.105174630041747e-06, 'epoch': 1.81}\n",
      "02/15/2024 16:39:32 - INFO - llmtuner.extras.callbacks - {'loss': 0.7837, 'learning_rate': 9.9266e-07, 'epoch': 1.82}\n",
      "{'loss': 0.7837, 'learning_rate': 9.926578580764234e-07, 'epoch': 1.82}\n",
      "02/15/2024 16:39:54 - INFO - llmtuner.extras.callbacks - {'loss': 0.8166, 'learning_rate': 8.8606e-07, 'epoch': 1.83}\n",
      "{'loss': 0.8166, 'learning_rate': 8.860645385550481e-07, 'epoch': 1.83}\n",
      "02/15/2024 16:40:17 - INFO - llmtuner.extras.callbacks - {'loss': 0.8108, 'learning_rate': 7.8542e-07, 'epoch': 1.84}\n",
      "{'loss': 0.8108, 'learning_rate': 7.854209717842231e-07, 'epoch': 1.84}\n",
      "02/15/2024 16:40:35 - INFO - llmtuner.extras.callbacks - {'loss': 0.9358, 'learning_rate': 6.9075e-07, 'epoch': 1.85}\n",
      "{'loss': 0.9358, 'learning_rate': 6.907519900580861e-07, 'epoch': 1.85}\n",
      "02/15/2024 16:40:56 - INFO - llmtuner.extras.callbacks - {'loss': 0.8081, 'learning_rate': 6.0208e-07, 'epoch': 1.86}\n",
      "{'loss': 0.8081, 'learning_rate': 6.020809515313142e-07, 'epoch': 1.86}\n",
      "02/15/2024 16:41:19 - INFO - llmtuner.extras.callbacks - {'loss': 0.8932, 'learning_rate': 5.1943e-07, 'epoch': 1.87}\n",
      "{'loss': 0.8932, 'learning_rate': 5.194297344558536e-07, 'epoch': 1.87}\n",
      "02/15/2024 16:41:44 - INFO - llmtuner.extras.callbacks - {'loss': 0.9166, 'learning_rate': 4.4282e-07, 'epoch': 1.88}\n",
      "{'loss': 0.9166, 'learning_rate': 4.4281873178278475e-07, 'epoch': 1.88}\n",
      "02/15/2024 16:42:06 - INFO - llmtuner.extras.callbacks - {'loss': 0.8529, 'learning_rate': 3.7227e-07, 'epoch': 1.89}\n",
      "{'loss': 0.8529, 'learning_rate': 3.7226684613065333e-07, 'epoch': 1.89}\n",
      "02/15/2024 16:42:29 - INFO - llmtuner.extras.callbacks - {'loss': 0.8554, 'learning_rate': 3.0779e-07, 'epoch': 1.90}\n",
      "{'loss': 0.8554, 'learning_rate': 3.077914851215585e-07, 'epoch': 1.9}\n",
      "02/15/2024 16:42:54 - INFO - llmtuner.extras.callbacks - {'loss': 0.8293, 'learning_rate': 2.4941e-07, 'epoch': 1.91}\n",
      "{'loss': 0.8293, 'learning_rate': 2.494085570860616e-07, 'epoch': 1.91}\n",
      "02/15/2024 16:43:15 - INFO - llmtuner.extras.callbacks - {'loss': 0.9114, 'learning_rate': 1.9713e-07, 'epoch': 1.92}\n",
      "{'loss': 0.9114, 'learning_rate': 1.9713246713805588e-07, 'epoch': 1.92}\n",
      "02/15/2024 16:43:38 - INFO - llmtuner.extras.callbacks - {'loss': 0.8525, 'learning_rate': 1.5098e-07, 'epoch': 1.93}\n",
      "{'loss': 0.8525, 'learning_rate': 1.509761136205101e-07, 'epoch': 1.93}\n",
      "02/15/2024 16:43:59 - INFO - llmtuner.extras.callbacks - {'loss': 0.8101, 'learning_rate': 1.1095e-07, 'epoch': 1.94}\n",
      "{'loss': 0.8101, 'learning_rate': 1.109508849230001e-07, 'epoch': 1.94}\n",
      "02/15/2024 16:44:23 - INFO - llmtuner.extras.callbacks - {'loss': 0.9540, 'learning_rate': 7.7067e-08, 'epoch': 1.95}\n",
      "{'loss': 0.954, 'learning_rate': 7.706665667180091e-08, 'epoch': 1.95}\n",
      "02/15/2024 16:44:40 - INFO - llmtuner.extras.callbacks - {'loss': 0.9111, 'learning_rate': 4.9332e-08, 'epoch': 1.96}\n",
      "{'loss': 0.9111, 'learning_rate': 4.9331789293211026e-08, 'epoch': 1.96}\n",
      "02/15/2024 16:45:03 - INFO - llmtuner.extras.callbacks - {'loss': 0.9583, 'learning_rate': 2.7753e-08, 'epoch': 1.97}\n",
      "{'loss': 0.9583, 'learning_rate': 2.7753125950752413e-08, 'epoch': 1.97}\n",
      "02/15/2024 16:45:25 - INFO - llmtuner.extras.callbacks - {'loss': 0.8885, 'learning_rate': 1.2336e-08, 'epoch': 1.98}\n",
      "{'loss': 0.8885, 'learning_rate': 1.233599085671e-08, 'epoch': 1.98}\n",
      "02/15/2024 16:45:45 - INFO - llmtuner.extras.callbacks - {'loss': 0.7927, 'learning_rate': 3.0842e-09, 'epoch': 1.99}\n",
      "{'loss': 0.7927, 'learning_rate': 3.0841879584853073e-09, 'epoch': 1.99}\n",
      "02/15/2024 16:46:08 - INFO - llmtuner.extras.callbacks - {'loss': 0.8209, 'learning_rate': 0.0000e+00, 'epoch': 2.00}\n",
      "{'loss': 0.8209, 'learning_rate': 0.0, 'epoch': 2.0}\n",
      "[INFO|trainer.py:2936] 2024-02-15 16:46:08,263 >> Saving model checkpoint to saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-1000\n",
      "[INFO|configuration_utils.py:729] 2024-02-15 16:46:08,674 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-02-15 16:46:08,676 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2816,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-15 16:46:08,738 >> tokenizer config file saved in saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-15 16:46:08,738 >> Special tokens file saved in saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-1000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2493] 2024-02-15 16:46:08,738 >> added tokens file saved in saves/Custom/lora/Owen_fc_t1/tmp-checkpoint-1000/added_tokens.json\n",
      "[INFO|trainer.py:1962] 2024-02-15 16:46:09,166 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "02/15/2024 16:46:09 - INFO - llmtuner.extras.callbacks - {'loss': 0.0000, 'learning_rate': 0.0000e+00, 'epoch': 2.00}\n",
      "{'train_runtime': 4350.4239, 'train_samples_per_second': 7.356, 'train_steps_per_second': 0.23, 'train_loss': 0.8812481429576874, 'epoch': 2.0}\n",
      "[INFO|trainer.py:2936] 2024-02-15 16:46:09,167 >> Saving model checkpoint to saves/Custom/lora/Owen_fc_t1\n",
      "[INFO|configuration_utils.py:729] 2024-02-15 16:46:09,487 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-02-15 16:46:09,488 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2816,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-15 16:46:09,515 >> tokenizer config file saved in saves/Custom/lora/Owen_fc_t1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-15 16:46:09,515 >> Special tokens file saved in saves/Custom/lora/Owen_fc_t1/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2493] 2024-02-15 16:46:09,515 >> added tokens file saved in saves/Custom/lora/Owen_fc_t1/added_tokens.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     0.8812\n",
      "  train_runtime            = 1:12:30.42\n",
      "  train_samples_per_second =      7.356\n",
      "  train_steps_per_second   =       0.23\n",
      "[INFO|modelcard.py:452] 2024-02-15 16:46:09,851 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-02-15 16:50:55,004 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-02-15 16:50:55,004 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-02-15 16:50:55,005 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-02-15 16:50:55,005 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-02-15 16:50:55,005 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-02-15 16:50:55,005 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/tokenizer.json\n",
      "[WARNING|logging.py:314] 2024-02-15 16:50:55,312 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:729] 2024-02-15 16:50:55,454 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-02-15 16:50:55,455 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"Qwen/Qwen1.5-0.5B\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2816,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "02/15/2024 16:50:55 - WARNING - llmtuner.model.patcher - FlashAttention2 is not installed.\n",
      "[INFO|modeling_utils.py:3476] 2024-02-15 16:50:55,458 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/model.safetensors\n",
      "[INFO|modeling_utils.py:1426] 2024-02-15 16:50:55,473 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:826] 2024-02-15 16:50:55,476 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4350] 2024-02-15 16:50:56,399 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4358] 2024-02-15 16:50:56,399 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen1.5-0.5B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:781] 2024-02-15 16:50:56,542 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/generation_config.json\n",
      "[INFO|configuration_utils.py:826] 2024-02-15 16:50:56,542 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"max_new_tokens\": 2048\n",
      "}\n",
      "\n",
      "02/15/2024 16:50:56 - INFO - llmtuner.model.adapter - Adapter is not found at evaluation, load the base model.\n",
      "02/15/2024 16:50:56 - INFO - llmtuner.model.loader - trainable params: 0 || all params: 463987712 || trainable%: 0.0000\n",
      "02/15/2024 16:50:56 - INFO - llmtuner.model.loader - This IS expected that the trainable params is 0 if you are using model for inference only.\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-02-15 16:51:10,820 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-02-15 16:51:10,820 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-02-15 16:51:10,820 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-02-15 16:51:10,820 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-02-15 16:51:10,821 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-02-15 16:51:10,821 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/tokenizer.json\n",
      "[WARNING|logging.py:314] 2024-02-15 16:51:11,126 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:729] 2024-02-15 16:51:11,265 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-02-15 16:51:11,267 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"Qwen/Qwen1.5-0.5B\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2816,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "02/15/2024 16:51:11 - WARNING - llmtuner.model.patcher - FlashAttention2 is not installed.\n",
      "[INFO|modeling_utils.py:3476] 2024-02-15 16:51:11,270 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/model.safetensors\n",
      "[INFO|modeling_utils.py:1426] 2024-02-15 16:51:11,284 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:826] 2024-02-15 16:51:11,286 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4350] 2024-02-15 16:51:11,806 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4358] 2024-02-15 16:51:11,806 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen1.5-0.5B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:781] 2024-02-15 16:51:11,953 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/generation_config.json\n",
      "[INFO|configuration_utils.py:826] 2024-02-15 16:51:11,953 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"max_new_tokens\": 2048\n",
      "}\n",
      "\n",
      "02/15/2024 16:51:11 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "02/15/2024 16:51:19 - INFO - llmtuner.model.adapter - Merged 1 adapter(s).\n",
      "02/15/2024 16:51:19 - INFO - llmtuner.model.adapter - Loaded adapter(s): saves/Custom/lora/Owen_fc_t1\n",
      "02/15/2024 16:51:19 - INFO - llmtuner.model.loader - trainable params: 0 || all params: 463987712 || trainable%: 0.0000\n",
      "02/15/2024 16:51:19 - INFO - llmtuner.model.loader - This IS expected that the trainable params is 0 if you are using model for inference only.\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-02-15 16:56:19,172 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-02-15 16:56:19,173 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-02-15 16:56:19,173 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-02-15 16:56:19,173 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-02-15 16:56:19,173 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-02-15 16:56:19,173 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/tokenizer.json\n",
      "[WARNING|logging.py:314] 2024-02-15 16:56:19,466 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:729] 2024-02-15 16:56:19,607 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-02-15 16:56:19,608 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"Qwen/Qwen1.5-0.5B\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2816,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3476] 2024-02-15 16:56:19,611 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/model.safetensors\n",
      "[INFO|modeling_utils.py:1426] 2024-02-15 16:56:19,627 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:826] 2024-02-15 16:56:19,629 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4350] 2024-02-15 16:56:20,337 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4358] 2024-02-15 16:56:20,338 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen1.5-0.5B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:781] 2024-02-15 16:56:20,700 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/fedce23ef6393499effdf4958f9b3256f299cc7d/generation_config.json\n",
      "[INFO|configuration_utils.py:826] 2024-02-15 16:56:20,700 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"max_new_tokens\": 2048\n",
      "}\n",
      "\n",
      "02/15/2024 16:56:20 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "02/15/2024 16:56:25 - INFO - llmtuner.model.adapter - Merged 1 adapter(s).\n",
      "02/15/2024 16:56:25 - INFO - llmtuner.model.adapter - Loaded adapter(s): saves/Custom/lora/Owen_fc_t1\n",
      "02/15/2024 16:56:25 - INFO - llmtuner.model.loader - trainable params: 0 || all params: 463987712 || trainable%: 0.0000\n",
      "02/15/2024 16:56:25 - INFO - llmtuner.model.loader - This IS expected that the trainable params is 0 if you are using model for inference only.\n",
      "[INFO|configuration_utils.py:473] 2024-02-15 16:56:25,972 >> Configuration saved in qwen-1.5-0.5B-tool/config.json\n",
      "[INFO|configuration_utils.py:594] 2024-02-15 16:56:25,972 >> Configuration saved in qwen-1.5-0.5B-tool/generation_config.json\n",
      "[INFO|modeling_utils.py:2493] 2024-02-15 16:56:26,647 >> Model weights saved in qwen-1.5-0.5B-tool/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-15 16:56:26,649 >> tokenizer config file saved in qwen-1.5-0.5B-tool/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-15 16:56:26,649 >> Special tokens file saved in qwen-1.5-0.5B-tool/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2493] 2024-02-15 16:56:26,649 >> added tokens file saved in qwen-1.5-0.5B-tool/added_tokens.json\n"
     ]
    }
   ],
   "source": [
    "!python src/train_web.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWNui_tmg8em"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
