{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81dab0a3-4ff0-4e9d-a48c-828eeb6971e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers/\n",
      "  Cloning https://github.com/huggingface/transformers/ to /tmp/pip-req-build-g4fplu_e\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers/ /tmp/pip-req-build-g4fplu_e\n",
      "\n",
      "  Resolved https://github.com/huggingface/transformers/ to commit 979f4774f619e43a7c121c49b3c9cdc0b48d687a\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (3.15.4)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.45.0.dev0)\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.45.0.dev0)\n",
      "  Downloading regex-2024.7.24-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (2.32.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.45.0.dev0)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.45.0.dev0)\n",
      "  Downloading safetensors-0.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (2024.7.4)\n",
      "Downloading huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\n",
      "Downloading regex-2024.7.24-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (776 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.5/776.5 kB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.45.0.dev0-py3-none-any.whl size=9617333 sha256=c5e5146478ad6fb7c935056b09063877b4a696130a346641f815ba5b3c82ecc0\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-m3ibsyc1/wheels/63/80/ea/4c4887871a4fd5e8f51fbc00ea09a1ebcf4582736a0b1a585e\n",
      "Successfully built transformers\n",
      "Installing collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.24.6 regex-2024.7.24 safetensors-0.4.4 tokenizers-0.19.1 transformers-4.45.0.dev0\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d64fd8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cupy-cuda12x\n",
      "  Downloading cupy_cuda12x-13.3.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cupy-cuda12x) (1.26.4)\n",
      "Collecting fastrlock>=0.5 (from cupy-cuda12x)\n",
      "  Using cached fastrlock-0.8.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (9.3 kB)\n",
      "Downloading cupy_cuda12x-13.3.0-cp310-cp310-manylinux2014_x86_64.whl (90.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 MB\u001b[0m \u001b[31m170.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached fastrlock-0.8.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (51 kB)\n",
      "Installing collected packages: fastrlock, cupy-cuda12x\n",
      "Successfully installed cupy-cuda12x-13.3.0 fastrlock-0.8.2\n"
     ]
    }
   ],
   "source": [
    "!pip install cupy-cuda12x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dda1894-9d79-4796-942f-289bd7ffee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from transformers.models.gemma2 import Gemma2ForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9db195db-83c8-4bfd-96f4-940d13aa366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers.models.gemma2 import Gemma2ForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8285db46-bfd7-40bd-9eab-c7df0ae156a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3b28f6979a4858bbcee5d2c1804939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63c591cc-a08f-4813-b4f8-9aad55a54d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"google/gemma-2-2b\",\n",
    "    trust_remote_code = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03a25982-9fed-4c1f-a910-f4f19f69eb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d434c802084d159cc3e1afe6501075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Gemma2ForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-2b\",\n",
    "    torch_dtype = torch.float16 ,\n",
    "    trust_remote_code = True,\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb78c039-3c1e-4ff8-bbf0-79644b372141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b826513",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf gemma2-f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef39d27-9f1d-4c08-959b-f7b18e898436",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir gemma2-f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81be1b50-6c87-4d05-9a04-c9041f5d1693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight torch.Size([256000, 2304])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.lm_head.named_parameters():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae7981ff-18af-48e4-b474-4759a0e65852",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([256000, 2304])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.q_proj.weight torch.Size([2048, 2304])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([2304, 2048])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([2304, 9216])\n",
      "model.layers.0.input_layernorm.weight torch.Size([2304])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([2304])\n",
      "model.layers.0.pre_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.0.post_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([2048, 2304])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([2304, 2048])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([2304, 9216])\n",
      "model.layers.1.input_layernorm.weight torch.Size([2304])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([2304])\n",
      "model.layers.1.pre_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.1.post_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([2048, 2304])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([2304, 2048])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([2304, 9216])\n",
      "model.layers.2.input_layernorm.weight torch.Size([2304])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([2304])\n",
      "model.layers.2.pre_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.2.post_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([2048, 2304])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([2304, 2048])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([2304, 9216])\n",
      "model.layers.3.input_layernorm.weight torch.Size([2304])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([2304])\n",
      "model.layers.3.pre_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.3.post_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([2048, 2304])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([2304, 2048])\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([2304, 9216])\n",
      "model.layers.4.input_layernorm.weight torch.Size([2304])\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([2304])\n",
      "model.layers.4.pre_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.4.post_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([2048, 2304])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([2304, 2048])\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([2304, 9216])\n",
      "model.layers.5.input_layernorm.weight torch.Size([2304])\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([2304])\n",
      "model.layers.5.pre_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.5.post_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([2048, 2304])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([2304, 2048])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([2304, 9216])\n",
      "model.layers.6.input_layernorm.weight torch.Size([2304])\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([2304])\n",
      "model.layers.6.pre_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.6.post_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([2048, 2304])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([2304, 2048])\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([2304, 9216])\n",
      "model.layers.7.input_layernorm.weight torch.Size([2304])\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([2304])\n",
      "model.layers.7.pre_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.7.post_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([2048, 2304])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([2304, 2048])\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([2304, 9216])\n",
      "model.layers.8.input_layernorm.weight torch.Size([2304])\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([2304])\n",
      "model.layers.8.pre_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.8.post_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([2048, 2304])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([2304, 2048])\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([2304, 9216])\n",
      "model.layers.9.input_layernorm.weight torch.Size([2304])\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([2304])\n",
      "model.layers.9.pre_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.9.post_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([2048, 2304])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([2304, 2048])\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([2304, 9216])\n",
      "model.layers.10.input_layernorm.weight torch.Size([2304])\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([2304])\n",
      "model.layers.10.pre_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.10.post_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([2048, 2304])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([2304, 2048])\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([2304, 9216])\n",
      "model.layers.11.input_layernorm.weight torch.Size([2304])\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([2304])\n",
      "model.layers.11.pre_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.11.post_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([2048, 2304])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([2304, 2048])\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([2304, 9216])\n",
      "model.layers.12.input_layernorm.weight torch.Size([2304])\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([2304])\n",
      "model.layers.12.pre_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.12.post_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([2048, 2304])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([2304, 2048])\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([2304, 9216])\n",
      "model.layers.13.input_layernorm.weight torch.Size([2304])\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([2304])\n",
      "model.layers.13.pre_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.13.post_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([2048, 2304])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([2304, 2048])\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([2304, 9216])\n",
      "model.layers.14.input_layernorm.weight torch.Size([2304])\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([2304])\n",
      "model.layers.14.pre_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.14.post_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([2048, 2304])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([2304, 2048])\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([2304, 9216])\n",
      "model.layers.15.input_layernorm.weight torch.Size([2304])\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([2304])\n",
      "model.layers.15.pre_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.15.post_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([2048, 2304])\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([2304, 2048])\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([2304, 9216])\n",
      "model.layers.16.input_layernorm.weight torch.Size([2304])\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([2304])\n",
      "model.layers.16.pre_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.16.post_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([2048, 2304])\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([2304, 2048])\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([2304, 9216])\n",
      "model.layers.17.input_layernorm.weight torch.Size([2304])\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([2304])\n",
      "model.layers.17.pre_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.17.post_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([2048, 2304])\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([2304, 2048])\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([2304, 9216])\n",
      "model.layers.18.input_layernorm.weight torch.Size([2304])\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([2304])\n",
      "model.layers.18.pre_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.18.post_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([2048, 2304])\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([2304, 2048])\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([2304, 9216])\n",
      "model.layers.19.input_layernorm.weight torch.Size([2304])\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([2304])\n",
      "model.layers.19.pre_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.19.post_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([2048, 2304])\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([2304, 2048])\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([2304, 9216])\n",
      "model.layers.20.input_layernorm.weight torch.Size([2304])\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([2304])\n",
      "model.layers.20.pre_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.20.post_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([2048, 2304])\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([2304, 2048])\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([2304, 9216])\n",
      "model.layers.21.input_layernorm.weight torch.Size([2304])\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([2304])\n",
      "model.layers.21.pre_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.21.post_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([2048, 2304])\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([2304, 2048])\n",
      "model.layers.22.mlp.gate_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([2304, 9216])\n",
      "model.layers.22.input_layernorm.weight torch.Size([2304])\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([2304])\n",
      "model.layers.22.pre_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.22.post_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([2048, 2304])\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([2304, 2048])\n",
      "model.layers.23.mlp.gate_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([2304, 9216])\n",
      "model.layers.23.input_layernorm.weight torch.Size([2304])\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([2304])\n",
      "model.layers.23.pre_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.23.post_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.24.self_attn.q_proj.weight torch.Size([2048, 2304])\n",
      "model.layers.24.self_attn.k_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.24.self_attn.v_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.24.self_attn.o_proj.weight torch.Size([2304, 2048])\n",
      "model.layers.24.mlp.gate_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.24.mlp.up_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.24.mlp.down_proj.weight torch.Size([2304, 9216])\n",
      "model.layers.24.input_layernorm.weight torch.Size([2304])\n",
      "model.layers.24.post_attention_layernorm.weight torch.Size([2304])\n",
      "model.layers.24.pre_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.24.post_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.25.self_attn.q_proj.weight torch.Size([2048, 2304])\n",
      "model.layers.25.self_attn.k_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.25.self_attn.v_proj.weight torch.Size([1024, 2304])\n",
      "model.layers.25.self_attn.o_proj.weight torch.Size([2304, 2048])\n",
      "model.layers.25.mlp.gate_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.25.mlp.up_proj.weight torch.Size([9216, 2304])\n",
      "model.layers.25.mlp.down_proj.weight torch.Size([2304, 9216])\n",
      "model.layers.25.input_layernorm.weight torch.Size([2304])\n",
      "model.layers.25.post_attention_layernorm.weight torch.Size([2304])\n",
      "model.layers.25.pre_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.layers.25.post_feedforward_layernorm.weight torch.Size([2304])\n",
      "model.norm.weight torch.Size([2304])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.size())\n",
    "    weights = param.data.numpy()\n",
    "    cp.save(f'gemma2-f16/{name}_weights.npy', weights)\n",
    "    if hasattr(param, 'bias'):\n",
    "        bias = param.bias.data.numpy()\n",
    "        cp.save(f'gemma2-f16/{name}_bias.npy', bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32be162c-aa83-48f3-8142-b68569b1344b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight torch.Size([256000, 2304])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.lm_head.named_parameters():\n",
    "    print(name, param.size())\n",
    "    weights = param.data.numpy()\n",
    "    cp.save(f'gemma2-f16/lm_head.weight_weights.npy', weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58e529f1-ea9d-4540-81ca-46bdc21acfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generateText(input_text, max_new_tokens=20):  \n",
    "#     input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "#     # Potential optimization with torch.inference_mode()\n",
    "#     with torch.inference_mode():\n",
    "#       outputs = model.generate(\n",
    "#           input_ids, \n",
    "#           max_new_tokens=max_new_tokens,\n",
    "#           do_sample=True, \n",
    "#           temperature=0.7,\n",
    "#       )\n",
    "\n",
    "#     generated_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "#     # Format the code output\n",
    "#     formatted_code = generated_tenp.strip() + \"\\n`\"\n",
    "#     return formatted_code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb71312f-c0c6-4d83-9b56-648b40eee7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generateText('once upon a time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "058b714d-4dc7-4750-9044-a7b72b13c94f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 256000,\n",
       " 'max_position_embeddings': 8192,\n",
       " 'hidden_size': 2304,\n",
       " 'intermediate_size': 9216,\n",
       " 'num_hidden_layers': 26,\n",
       " 'num_attention_heads': 8,\n",
       " 'head_dim': 256,\n",
       " 'num_key_value_heads': 4,\n",
       " 'hidden_activation': 'gelu_pytorch_tanh',\n",
       " 'initializer_range': 0.02,\n",
       " 'rms_norm_eps': 1e-06,\n",
       " 'use_cache': True,\n",
       " 'rope_theta': 10000.0,\n",
       " 'attention_bias': False,\n",
       " 'attention_dropout': 0.0,\n",
       " 'attn_logit_softcapping': 50.0,\n",
       " 'return_dict': True,\n",
       " 'output_hidden_states': False,\n",
       " 'output_attentions': False,\n",
       " 'torchscript': False,\n",
       " 'torch_dtype': 'float32',\n",
       " 'use_bfloat16': False,\n",
       " 'tf_legacy_loss': False,\n",
       " 'pruned_heads': {},\n",
       " 'tie_word_embeddings': True,\n",
       " 'chunk_size_feed_forward': 0,\n",
       " 'is_encoder_decoder': False,\n",
       " 'is_decoder': False,\n",
       " 'cross_attention_hidden_size': None,\n",
       " 'add_cross_attention': False,\n",
       " 'tie_encoder_decoder': False,\n",
       " 'max_length': 20,\n",
       " 'min_length': 0,\n",
       " 'do_sample': False,\n",
       " 'early_stopping': False,\n",
       " 'num_beams': 1,\n",
       " 'num_beam_groups': 1,\n",
       " 'diversity_penalty': 0.0,\n",
       " 'temperature': 1.0,\n",
       " 'top_k': 50,\n",
       " 'top_p': 1.0,\n",
       " 'typical_p': 1.0,\n",
       " 'repetition_penalty': 1.0,\n",
       " 'length_penalty': 1.0,\n",
       " 'no_repeat_ngram_size': 0,\n",
       " 'encoder_no_repeat_ngram_size': 0,\n",
       " 'bad_words_ids': None,\n",
       " 'num_return_sequences': 1,\n",
       " 'output_scores': False,\n",
       " 'return_dict_in_generate': False,\n",
       " 'forced_bos_token_id': None,\n",
       " 'forced_eos_token_id': None,\n",
       " 'remove_invalid_values': False,\n",
       " 'exponential_decay_length_penalty': None,\n",
       " 'suppress_tokens': None,\n",
       " 'begin_suppress_tokens': None,\n",
       " 'architectures': ['Gemma2ForCausalLM'],\n",
       " 'finetuning_task': None,\n",
       " 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'},\n",
       " 'label2id': {'LABEL_0': 0, 'LABEL_1': 1},\n",
       " 'tokenizer_class': None,\n",
       " 'prefix': None,\n",
       " 'bos_token_id': 2,\n",
       " 'pad_token_id': 0,\n",
       " 'eos_token_id': 1,\n",
       " 'sep_token_id': None,\n",
       " 'decoder_start_token_id': None,\n",
       " 'task_specific_params': None,\n",
       " 'problem_type': None,\n",
       " '_name_or_path': 'google/gemma-2-2b',\n",
       " 'transformers_version': '4.45.0.dev0',\n",
       " 'cache_implementation': 'hybrid',\n",
       " 'hidden_act': 'gelu_pytorch_tanh',\n",
       " 'model_type': 'gemma2',\n",
       " 'final_logit_softcapping': 30.0,\n",
       " 'query_pre_attn_scalar': 256,\n",
       " 'sliding_window': 4096}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "716830b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09fe61c9-ca28-4eab-a533-bebc3e541635",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from transformers.models.gemma2 import Gemma2ForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"google/gemma-2-2b\",\n",
    "    trust_remote_code = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae7c2325-6f4e-4c49-b277-abc256a23ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'vocab_size': 256000,\n",
    " 'max_position_embeddings': 8192,\n",
    " 'hidden_size': 2304,\n",
    " 'intermediate_size': 9216,\n",
    " 'num_hidden_layers': 26,\n",
    " 'num_attention_heads': 8,\n",
    " 'head_dim': 256,\n",
    " 'num_key_value_heads': 4,\n",
    " 'hidden_activation': 'gelu_pytorch_tanh',\n",
    " 'initializer_range': 0.02,\n",
    " 'rms_norm_eps': 1e-06,\n",
    " 'use_cache': False,\n",
    " 'rope_theta': 10000.0,\n",
    " 'attention_bias': False,\n",
    " 'attention_dropout': 0.0,\n",
    " 'attn_logit_softcapping': 50.0,\n",
    " 'return_dict': True,\n",
    " 'output_hidden_states': False,\n",
    " 'output_attentions': False,\n",
    " 'torchscript': False,\n",
    " 'torch_dtype': 'float32',\n",
    " 'use_bfloat16': False,\n",
    " 'tf_legacy_loss': False,\n",
    " 'pruned_heads': {},\n",
    " 'tie_word_embeddings': True,\n",
    " 'chunk_size_feed_forward': 0,\n",
    " 'is_encoder_decoder': False,\n",
    " 'is_decoder': False,\n",
    " 'cross_attention_hidden_size': None,\n",
    " 'add_cross_attention': False,\n",
    " 'tie_encoder_decoder': False,\n",
    " 'max_length': 20,\n",
    " 'min_length': 0,\n",
    " 'do_sample': False,\n",
    " 'early_stopping': False,\n",
    " 'num_beams': 1,\n",
    " 'num_beam_groups': 1,\n",
    " 'diversity_penalty': 0.0,\n",
    " 'temperature': 1.0,\n",
    " 'top_k': 50,\n",
    " 'top_p': 1.0,\n",
    " 'typical_p': 1.0,\n",
    " 'repetition_penalty': 1.0,\n",
    " 'length_penalty': 1.0,\n",
    " 'no_repeat_ngram_size': 0,\n",
    " 'encoder_no_repeat_ngram_size': 0,\n",
    " 'bad_words_ids': None,\n",
    " 'num_return_sequences': 1,\n",
    " 'output_scores': False,\n",
    " 'return_dict_in_generate': False,\n",
    " 'forced_bos_token_id': None,\n",
    " 'forced_eos_token_id': None,\n",
    " 'remove_invalid_values': False,\n",
    " 'exponential_decay_length_penalty': None,\n",
    " 'suppress_tokens': None,\n",
    " 'begin_suppress_tokens': None,\n",
    " 'architectures': ['Gemma2ForCausalLM'],\n",
    " 'finetuning_task': None,\n",
    " 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'},\n",
    " 'label2id': {'LABEL_0': 0, 'LABEL_1': 1},\n",
    " 'tokenizer_class': None,\n",
    " 'prefix': None,\n",
    " 'bos_token_id': 2,\n",
    " 'pad_token_id': 0,\n",
    " 'eos_token_id': 1,\n",
    " 'sep_token_id': None,\n",
    " 'decoder_start_token_id': None,\n",
    " 'task_specific_params': None,\n",
    " 'problem_type': None,\n",
    " '_name_or_path': 'google/gemma-2-2b',\n",
    " 'transformers_version': '4.45.0.dev0',\n",
    " 'cache_implementation': 'hybrid',\n",
    " 'hidden_act': 'gelu_pytorch_tanh',\n",
    " 'model_type': 'gemma2',\n",
    " 'final_logit_softcapping': 30.0,\n",
    " 'query_pre_attn_scalar': 256,\n",
    " 'sliding_window': 4096}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbf55ae0-6cae-4729-b6c5-6c4dd626d349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from os import environ\n",
    "# environ['OMP_NUM_THREADS'] = '32'\n",
    "# environ['MKL_NUM_THREADS'] ='32'\n",
    "# environ['NUMEXPR_NUM_THREADS'] = '32'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dc96803-7fd0-4735-a445-6bb681f00fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d74c0a3c-1fff-4c04-ae12-74447d6cdbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = [0] * config['hidden_size']\n",
    "embedding = cp.random.randn(config['vocab_size'], config['hidden_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a613bd9-b105-4b53-be0f-4feafa2a1146",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = config['vocab_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47ce278b-d9b5-4aec-af45-d24c4c9230c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = cp.load('gemma2-f16/model.embed_tokens.weight_weights.npy', mmap_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb3453d0-cf25-495c-a840-460fef74ccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Gemma2RotaryEmbedding:\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000):\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        self.inv_freq = 1.0 / (self.base ** (cp.arange(0, self.dim, 2).astype(cp.float32) / self.dim))\n",
    "\n",
    "    def __call__(self, x, position_ids, seq_len=None):\n",
    "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
    "        inv_freq_expanded = cp.expand_dims(self.inv_freq, axis=0).astype(cp.float32)\n",
    "        inv_freq_expanded = cp.expand_dims(inv_freq_expanded, axis=-1).repeat(position_ids.shape[0], axis=0)\n",
    "        position_ids_expanded = cp.expand_dims(position_ids, axis=1).astype(cp.float32)\n",
    "        \n",
    "        freqs = cp.matmul(inv_freq_expanded, position_ids_expanded).transpose(0, 2, 1)\n",
    "        emb = cp.concatenate((freqs, freqs), axis=-1)\n",
    "        cos = cp.cos(emb)\n",
    "        sin = cp.sin(emb)\n",
    "        \n",
    "        return cos.astype(x.dtype), sin.astype(x.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab764c6b-6bed-41bc-9d6a-e137ada5def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import cupy as cp\n",
    "\n",
    "def rotate_half_np(x):\n",
    "    \"\"\"\n",
    "    Rotates half the hidden dims of the input, implemented in NumPy.\n",
    "    Args:\n",
    "        x (numpy.ndarray): Input array.\n",
    "    Returns:\n",
    "        numpy.ndarray: Rotated array.\n",
    "    \"\"\"\n",
    "    # Split the array into two halves along the last dimension\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "\n",
    "    # Reverse the second half and concatenate with the first half\n",
    "    # NumPy's negative indexing reverses the array\n",
    "    return cp.concatenate((-x2, x1), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17672738-e32b-4f1e-82d0-217dc14a5a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    cos = cp.expand_dims( cos, unsqueeze_dim)\n",
    "    sin = cp.expand_dims( sin, unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half_np(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half_np(k) * sin)\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e3930c5-c7e2-4399-9713-473dc4ae249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import math\n",
    "\n",
    "def gelu_np(input):\n",
    "    return 0.5 * input * (1.0 + cp.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * cp.power(input, 3.0))))\n",
    "\n",
    "# Example usage\n",
    "input_np = cp.random.randn(10) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe847046-a84d-4e51-9e59-b54f700b7478",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example activation function mapping (adjust as needed)\n",
    "ACT2FN_np = {\n",
    "    'relu': cp.vectorize(lambda x: max(0, x)),\n",
    "    # Add other activation functions as needed\n",
    "    'gelu_pytorch_tanh': gelu_np\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9423457a-0e5c-421d-a961-1533fde41410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "\n",
    "# class Linear_np:\n",
    "#     def __init__(self, in_features, out_features, bias=True):\n",
    "#         # Initialize weights and bias\n",
    "#         self.weights = cp.random.randn(out_features, in_features)\n",
    "#         self.bias = cp.random.randn(out_features)\n",
    "\n",
    "        \n",
    "#     def load(self, weights, bias=None):\n",
    "#         self.weights = weights\n",
    "#         self.bias = bias\n",
    "        \n",
    "    \n",
    "#     def __call__(self, x):\n",
    "#         # Perform the linear operation (y = xA^T + b)\n",
    "#         out =  cp.dot(x, self.weights.T) \n",
    "#         if self.bias is not None:\n",
    "#             out = out + self.bias\n",
    "#         return out\n",
    "import cupy as cp\n",
    "\n",
    "class Linear_np:\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        # Initialize weights and bias\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def load(self, weights, bias=None):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Preallocate the output array to avoid creating new arrays repeatedly\n",
    "        out = cp.empty((*x.shape[:-1], self.weights.shape[0]), dtype=x.dtype)\n",
    "        \n",
    "        # Perform the linear operation (y = xA^T + b)\n",
    "        cp.dot(x, self.weights.T, out=out)\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            out += self.bias  # This uses broadcasting, which is efficient in numpy\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe1a1631-80a2-42ce-805f-e80563ae291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Gemma2MLP_np:\n",
    "    def __init__(self, config, layer_index):\n",
    "        self.config = config\n",
    "        self.layer_index = layer_index\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.gate_proj_weights = Linear_np(self.hidden_size, self.intermediate_size)\n",
    "        self.up_proj_weights = Linear_np(self.hidden_size, self.intermediate_size)\n",
    "        self.down_proj_weights = Linear_np(self.intermediate_size, self.hidden_size)\n",
    "\n",
    "        gate_proj_weights= cp.load(f'gemma2-f16/model.layers.{layer_index}.mlp.gate_proj.weight_weights.npy', mmap_mode=None)\n",
    "\n",
    "        down_proj_weights= cp.load(f'gemma2-f16/model.layers.{layer_index}.mlp.down_proj.weight_weights.npy', mmap_mode=None)\n",
    "\n",
    "        up_proj_weights= cp.load(f'gemma2-f16/model.layers.{layer_index}.mlp.up_proj.weight_weights.npy', mmap_mode=None)\n",
    "\n",
    "        self.gate_proj_weights.load(gate_proj_weights)\n",
    "        self.down_proj_weights.load(down_proj_weights)\n",
    "        self.up_proj_weights.load(up_proj_weights)\n",
    "\n",
    "        self.act_fn = ACT2FN_np[config.hidden_act]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        gate_proj_output = self.gate_proj_weights(x)\n",
    "        up_proj_output = self.up_proj_weights(x)\n",
    "        activated_output = self.act_fn(gate_proj_output)\n",
    "        multiplied_output = activated_output * up_proj_output\n",
    "        down_proj_output = self.down_proj_weights(multiplied_output)\n",
    "        return down_proj_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af584bda-6b4a-4b8b-bb56-0b2ee547a3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 6, 4, 5)\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "\n",
    "def repeat_kv_np(hidden_states, n_rep):\n",
    "    \"\"\"\n",
    "    Replicates the behavior of torch.repeat_interleave for a specific use-case. \n",
    "    The hidden states go from (batch, num_key_value_heads, seqlen, head_dim) \n",
    "    to (batch, num_attention_heads, seqlen, head_dim) in NumPy.\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    \n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    \n",
    "    # Expand and then repeat the array along the third axis\n",
    "    hidden_states_expanded = cp.expand_dims(hidden_states, axis=2)\n",
    "    hidden_states_repeated = cp.tile(hidden_states_expanded, (1, 1, n_rep, 1, 1))\n",
    "\n",
    "    # Reshape the array to the desired shape\n",
    "    return hidden_states_repeated.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "# Example usage\n",
    "hidden_states = cp.random.randn(2, 3, 4, 5)  # Example input tensor with shape (batch, num_key_value_heads, slen, head_dim)\n",
    "n_rep = 2  # Example repetition factor\n",
    "output = repeat_kv_np(hidden_states, n_rep)\n",
    "print(output.shape)  # The shape should be (2, 6, 4, 5) in this example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4abfce41-9d95-4e97-ab83-6b59b9d87644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bobbyhadz.com\n",
      "Python\n",
      "{'id': 1, 'website': 'bobbyhadz.com', 'topic': 'Python', 'author': 'Borislav Hadzhiev'}\n",
      "{'id': 1, 'website': 'bobbyhadz.com', 'topic': 'Python'}\n"
     ]
    }
   ],
   "source": [
    "class AttributeDict(dict):\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "\n",
    "my_dict = {'id': 1, 'website': 'bobbyhadz.com', 'topic': 'Python'}\n",
    "\n",
    "new_dict = AttributeDict(my_dict)\n",
    "\n",
    "print(new_dict.website)  # 👉️ bobbyhadz.com\n",
    "print(new_dict.topic)  # 👉️ Python\n",
    "\n",
    "new_dict.author = 'Borislav Hadzhiev'\n",
    "\n",
    "# 👇️ {'id': 1, 'website': 'bobbyhadz.com', 'topic': 'Python', 'author': 'Borislav Hadzhiev'}\n",
    "print(new_dict)\n",
    "\n",
    "del new_dict.author\n",
    "\n",
    "# 👇️ {'id': 1, 'website': 'bobbyhadz.com', 'topic': 'Python'}\n",
    "print(new_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4afaf9c8-2db8-49f5-8b31-c69d748283c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 = AttributeDict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c483c579-0a55-4c45-9707-7f5c5e5e50e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config1.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc2dcc49-812f-47e2-ab0d-f7cc4283b19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import cupy as cp\n",
    "\n",
    "class Gemma2RMSNorm_np:\n",
    "    def __init__(self, dim: int,layer_idx =-1, eps: float = 1e-6, loc='input_layernorm'):\n",
    "        self.eps = eps\n",
    "        if layer_idx == -1:\n",
    "        \n",
    "            gamma = cp.load(f'gemma2-f16/model.norm.weight_weights.npy', mmap_mode=None)            \n",
    "        else:\n",
    "            gamma = cp.load(f'gemma2-f16/model.layers.{layer_idx}.{loc}.weight_weights.npy', mmap_mode=None)\n",
    "\n",
    "        self.weight = gamma +1.0\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * cp.reciprocal(cp.sqrt(cp.mean(cp.square(x), axis=-1, keepdims=True) + self.eps))\n",
    "    # def _norm(self, x):\n",
    "    #     # Cast x to float32 for the computation to avoid overflow\n",
    "    #     x = x.astype(cp.float32)\n",
    "        \n",
    "    #     # Compute the mean square and add eps to prevent division by zero\n",
    "    #     mean_square = cp.mean(cp.square(x), axis=-1, keepdims=True)\n",
    "        \n",
    "    #     # Calculate the reciprocal of the root mean square\n",
    "    #     norm_factor = cp.reciprocal(cp.sqrt(cp.maximum(mean_square, self.eps)))\n",
    "        \n",
    "    #     # Apply the normalization factor\n",
    "    #     normalized_x = x * norm_factor\n",
    "        \n",
    "    #     # Cast back to float32 if needed\n",
    "    #     return normalized_x.astype(cp.float32)\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        output = self._norm(x)\n",
    "        output = output * self.weight\n",
    "        return output\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f\"{self.weight.shape}, eps={self.eps}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ebba924-83a4-4eef-947c-95932c497b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "def softmax1(x, axis=-1):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x along the specified axis.\"\"\"\n",
    "    e_x = cp.exp(x - cp.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / cp.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x along the specified axis.\"\"\"\n",
    "    e_x = cp.exp(x )\n",
    "    return e_x / cp.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "# Example usage\n",
    "attn_weights_np = cp.random.randn(2, 4, 10, 10).astype(cp.float32)  # Example attention weights\n",
    "softmax_attn_weights_np = softmax(attn_weights_np, axis=-1)\n",
    "\n",
    "# If you need to convert back to a specific dtype, like in PyTorch code, you can cast it:\n",
    "# e.g., softmax_attn_weights_np = softmax_attn_weights_cp.astype(original_dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "714c6515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2,3][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dde819d7-b6da-421e-afe1-22e32e3bac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "from typing import Tuple\n",
    "\n",
    "class KVCache:\n",
    "    def __init__(self):\n",
    "        self.key_cache = []\n",
    "        self.value_cache = []\n",
    "\n",
    "    def num_items(self) -> int:\n",
    "        if len(self.key_cache) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.key_cache[0].shape[-2]\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        keys: cp.ndarray,\n",
    "        values: cp.ndarray,\n",
    "        layer_idx: int,\n",
    "    ) -> Tuple[cp.ndarray, cp.ndarray]:\n",
    "        if len(self.key_cache) <= layer_idx:\n",
    "            self.key_cache.append(keys)\n",
    "            self.value_cache.append(values)\n",
    "        else:\n",
    "            self.key_cache[layer_idx] = cp.concatenate(\n",
    "                [self.key_cache[layer_idx], keys], axis=-2\n",
    "            )\n",
    "            self.value_cache[layer_idx] = cp.concatenate(\n",
    "                [self.value_cache[layer_idx], values], axis=-2\n",
    "            )\n",
    "            \n",
    "        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58bea9e5-0c21-401c-85e6-4ae883d372b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gemma2Attention():\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config, layer_idx = None):\n",
    "        \n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        \n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = config.head_dim\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "        self.rope_theta = config.rope_theta\n",
    "        self.is_causal = True\n",
    "        self.scaling = config.query_pre_attn_scalar**-0.5\n",
    "        \n",
    "        if (self.hidden_size % self.num_heads) != 0:\n",
    "            raise ValueError(\n",
    "                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n",
    "                f\" and `num_heads`: {self.num_heads}).\"\n",
    "            )\n",
    "    \n",
    "\n",
    "        self._init_rope()\n",
    "\n",
    "        self.q_proj = Linear_np(self.hidden_size, self.num_heads * self.head_dim, bias=True)\n",
    "        self.k_proj = Linear_np(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n",
    "        self.v_proj = Linear_np(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n",
    "        self.o_proj = Linear_np(self.num_heads * self.head_dim, self.hidden_size, bias=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        q_proj =cp.load(f'gemma2-f16/model.layers.{layer_idx}.self_attn.q_proj.weight_weights.npy', mmap_mode=None)\n",
    "        k_proj =cp.load(f'gemma2-f16/model.layers.{layer_idx}.self_attn.k_proj.weight_weights.npy', mmap_mode=None)\n",
    "        v_proj =cp.load(f'gemma2-f16/model.layers.{layer_idx}.self_attn.v_proj.weight_weights.npy', mmap_mode=None)\n",
    "        o_proj = cp.load(f'gemma2-f16/model.layers.{layer_idx}.self_attn.o_proj.weight_weights.npy', mmap_mode=None)\n",
    "\n",
    "        self.q_proj.load(q_proj)\n",
    "        self.k_proj.load(k_proj)\n",
    "        self.v_proj.load(v_proj)\n",
    "        self.o_proj.load(o_proj)\n",
    "\n",
    "\n",
    "    def _init_rope(self):\n",
    "        self.rotary_emb = Gemma2RotaryEmbedding(\n",
    "            int(self.head_dim),\n",
    "            max_position_embeddings=self.max_position_embeddings,\n",
    "            base=self.rope_theta,\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def _split_heads(self, fused_qkv):\n",
    "        batch_size, seq_length, _ = fused_qkv.shape\n",
    "        reshaped = fused_qkv.reshape(batch_size, seq_length, self.num_heads, 3, self.head_dim)\n",
    "        query = reshaped[..., 0, :]\n",
    "        key = reshaped[..., 1, :]\n",
    "        value = reshaped[..., 2, :]\n",
    "        return query, key, value\n",
    "        \n",
    "    def __call__(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask = None,\n",
    "        position_ids = None,\n",
    "        output_attentions = False,\n",
    "        use_cache = False,\n",
    "        kv_cache = None\n",
    "        \n",
    "    ): \n",
    "        bsz, q_len, _ = hidden_states.shape\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        query_states = query_states.reshape(bsz, q_len, self.num_heads, self.head_dim)\n",
    "        key_states = key_states.reshape(bsz, q_len, self.num_key_value_heads, self.head_dim)\n",
    "        value_states = value_states.reshape(bsz, q_len, self.num_key_value_heads, self.head_dim)\n",
    "\n",
    "        query_states = query_states.transpose(0, 2, 1, 3)\n",
    "        key_states = key_states.transpose(0, 2, 1, 3)\n",
    "        value_states = value_states.transpose(0, 2, 1, 3)\n",
    "        \n",
    "        kv_seq_len = key_states.shape[-2]\n",
    "        # if past_key_value is not None:\n",
    "        #     kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n",
    "        cos, sin = self.rotary_emb(value_states, position_ids)\n",
    "\n",
    "        # # Partial rotary embedding\n",
    "        # query_rot, query_pass = (\n",
    "        #     query_states[..., : self.rotary_emb.dim],\n",
    "        #     query_states[..., self.rotary_emb.dim :],\n",
    "        # )\n",
    "        # key_rot, key_pass = (\n",
    "        #     key_states[..., : self.rotary_emb.dim],\n",
    "        #     key_states[..., self.rotary_emb.dim :],\n",
    "        # )\n",
    "        # # [batch_size, seq_length, num_heads, head_dim // config.partial_rotary_factor]\n",
    "        # query_rot, key_rot = apply_rotary_pos_emb(query_rot, key_rot, cos, sin, position_ids)\n",
    "\n",
    "        # # [batch_size, seq_length, num_heads, head_dim]\n",
    "        # query_states = cp.concatenate((query_rot, query_pass), axis=-1)\n",
    "        \n",
    "        # key_states = cp.concatenate((key_rot, key_pass), axis=-1)\n",
    "\n",
    "        \n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        #print(self.layer_idx, \"before\", kv_cache.num_items())\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            key_states, value_states = kv_cache.update(key_states, value_states, self.layer_idx)\n",
    "\n",
    "        #print(self.layer_idx,\"after\", kv_cache.num_items())\n",
    "        \n",
    "        \n",
    "        key_states = repeat_kv_np(key_states, self.num_key_value_groups)\n",
    "        value_states = repeat_kv_np(value_states, self.num_key_value_groups)\n",
    "\n",
    "        \n",
    "        # Queries and keys upcast to fp32 is required by Phi-2 to avoid overflow\n",
    "        attn_weights = cp.matmul(\n",
    "            query_states, key_states.transpose(0, 1, 3, 2)\n",
    "        ) / cp.sqrt(self.head_dim)\n",
    "\n",
    "        if q_len > 2:\n",
    "\n",
    "            tril = cp.tril(cp.ones((q_len, q_len), dtype=cp.float32))\n",
    "            \n",
    "            \n",
    "            mask = tril[None, None, :, :]  # Add a new dimension at the beginning to match batch size\n",
    "            \n",
    "            attn_weights = cp.where(mask == 0, float('-inf'), attn_weights)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = attn_weights + attention_mask\n",
    "\n",
    "        # upcast attention to fp32\n",
    "        \n",
    "        attn_weights = softmax(attn_weights, axis=-1)\n",
    "        \n",
    "        \n",
    "\n",
    "        attn_output = cp.matmul(attn_weights, value_states)\n",
    "        \n",
    "        \n",
    "\n",
    "        attn_output = attn_output.transpose(0, 2, 1, 3)\n",
    "        \n",
    "        attn_output = attn_output.reshape(bsz, q_len, self.num_heads * self.head_dim)\n",
    "\n",
    "        \n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights, kv_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d13e28aa-58ad-45e6-bca4-44e1393658a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gemma2DecoderLayer():\n",
    "    def __init__(self, config, layer_idx: int):\n",
    "        self.self_attn = Gemma2Attention(config, layer_idx=layer_idx)\n",
    "        self.mlp = Gemma2MLP_np(config, layer_index=layer_idx)\n",
    "        self.input_layernorm = Gemma2RMSNorm_np(config.hidden_size, layer_idx=layer_idx, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = Gemma2RMSNorm_np(config.hidden_size, layer_idx=layer_idx, eps=config.rms_norm_eps, loc='post_attention_layernorm')\n",
    "        self.pre_feedforward_layernorm = Gemma2RMSNorm_np(config.hidden_size, layer_idx=layer_idx, eps=config.rms_norm_eps, loc='pre_feedforward_layernorm')\n",
    "        self.post_feedforward_layernorm = Gemma2RMSNorm_np(config.hidden_size, layer_idx=layer_idx, eps=config.rms_norm_eps, loc='post_feedforward_layernorm')\n",
    "    def __call__(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask = None,\n",
    "        position_ids  = None,\n",
    "        output_attentions  = False,\n",
    "        use_cache  = False,\n",
    "        kv_cache = None\n",
    "    ) :\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`):\n",
    "                input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "            position_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n",
    "                Indices of positions of each input sequence tokens in the position embeddings. Selected in the range\n",
    "                `[0, config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            use_cache (`bool`, *optional*):\n",
    "                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
    "                (see `past_key_values`).\n",
    "            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n",
    "        \"\"\"\n",
    "\n",
    "        residual = hidden_states\n",
    "\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        # Self Attention\n",
    "        hidden_states, self_attn_weights, kv_cache = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            output_attentions=output_attentions,\n",
    "            use_cache=use_cache,\n",
    "            kv_cache=kv_cache\n",
    "        )\n",
    "        \n",
    "        \n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.pre_feedforward_layernorm(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = self.post_feedforward_layernorm(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        \n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        outputs += (self_attn_weights,)\n",
    "\n",
    "        outputs += (kv_cache,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d6b5c4-5fed-4241-948a-82a7ac20ee3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0914e746-064d-4ef9-ba02-0935ece98dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gemma2Model():\n",
    "\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \n",
    "        self.config = config\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = cp.load('gemma2-f16/model.embed_tokens.weight_weights.npy', mmap_mode=None)\n",
    "        \n",
    "        self.layers = [Gemma2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "        \n",
    "        self.final_layernorm = Gemma2RMSNorm_np(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self._use_flash_attention_2 = False\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "        # Initialize weights and apply final processing\n",
    "        \n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embed_tokens = value\n",
    "\n",
    "    \n",
    "    def __call__(\n",
    "        self,\n",
    "        input_ids = None,\n",
    "        attention_mask = None,\n",
    "        position_ids = None,\n",
    "        inputs_embeds= None,\n",
    "        use_cache= None,\n",
    "        output_attentions = True,\n",
    "        output_hidden_states= True,\n",
    "        return_dict = None,\n",
    "        kv_cache = None,\n",
    "    ) :\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "\n",
    "        return_dict = False\n",
    "\n",
    "        # retrieve input_ids and inputs_embeds\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            batch_size, seq_length = input_ids.shape[:2]\n",
    "        elif inputs_embeds is not None:\n",
    "            batch_size, seq_length = inputs_embeds.shape[:2]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        past_key_values_length = 0\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "        if position_ids is None:\n",
    "            \n",
    "            position_ids = cp.arange(\n",
    "                0, seq_length, dtype=cp.int64\n",
    "            )\n",
    "            \n",
    "            if use_cache and  kv_cache is not None:\n",
    "            \n",
    "                position_ids = cp.arange(\n",
    "                        kv_cache.num_items(), seq_length + kv_cache.num_items(), dtype=cp.int64\n",
    "                    )\n",
    "            \n",
    "\n",
    "            position_ids = cp.expand_dims(position_ids, 0)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens[input_ids]\n",
    "\n",
    "\n",
    "        \n",
    "        attention_mask = None\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        normalizer = self.config.hidden_size**0.5\n",
    "        hidden_states = hidden_states * normalizer\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        next_decoder_cache = None\n",
    "\n",
    "        \n",
    "        for decoder_layer in self.layers:\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "\n",
    "            layer_outputs = decoder_layer(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                output_attentions=output_attentions,\n",
    "                use_cache=use_cache,\n",
    "                kv_cache=kv_cache\n",
    "            )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            \n",
    "\n",
    "            if use_cache:\n",
    "                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "        hidden_states = self.final_layernorm(hidden_states)\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        \n",
    "        last_hidden_state=hidden_states\n",
    "        \n",
    "        hidden_states=all_hidden_states\n",
    "        attentions=all_self_attns\n",
    "        return (\n",
    "            last_hidden_state,\n",
    "            kv_cache,\n",
    "            hidden_states,\n",
    "            attentions\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ad8f9f6-18b2-444a-8811-760507e3e63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gemma2ForCausalLM_np():\n",
    "    \n",
    "\n",
    "    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.__init__ with Llama->Phi,bias=False->bias=True\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        \n",
    "        self.model = Gemma2Model(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = Linear_np(config.hidden_size, config.vocab_size, bias=True)\n",
    "        \n",
    "        weights =cp.load(f'gemma2-f16/lm_head.weight_weights.npy', mmap_mode=None)\n",
    "        \n",
    "        \n",
    "        self.lm_head.load(weights)\n",
    "\n",
    "        \n",
    "\n",
    "    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.get_input_embeddings\n",
    "    def get_input_embeddings(self):\n",
    "        return self.model.embed_tokens\n",
    "\n",
    "    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_input_embeddings\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.model.embed_tokens = value\n",
    "\n",
    "    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.get_output_embeddings\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_output_embeddings\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_decoder\n",
    "    def set_decoder(self, decoder):\n",
    "        self.model = decoder\n",
    "\n",
    "    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.get_decoder\n",
    "    def get_decoder(self):\n",
    "        return self.model\n",
    "\n",
    "    \n",
    "    def __call__(\n",
    "        self,\n",
    "        input_ids = None,\n",
    "        attention_mask = None,\n",
    "        position_ids = None,\n",
    "        inputs_embeds = None,\n",
    "        labels = None,\n",
    "        use_cache = None,\n",
    "        output_attentions = True,\n",
    "        output_hidden_states= True,\n",
    "        return_dict = True,\n",
    "        kv_cache=None\n",
    "    ) :\n",
    "       \n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = False #return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            kv_cache=kv_cache\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        #logits = softmax(logits, axis=-1)\n",
    "        logits = logits\n",
    "        if self.config.final_logit_softcapping is not None:\n",
    "            logits = logits / self.config.final_logit_softcapping\n",
    "            logits = cp.tanh(logits)\n",
    "            logits = logits * self.config.final_logit_softcapping\n",
    "\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        logits=logits\n",
    "        kv_cache=outputs[1]\n",
    "        hidden_states=outputs[2]\n",
    "        attentions=outputs[3]\n",
    "\n",
    "        return (\n",
    "            loss,\n",
    "            logits,\n",
    "            kv_cache,\n",
    "            hidden_states,\n",
    "            attentions,\n",
    "        )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b25c228e-5e04-4840-80ca-47fc57f8b949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def sample(probabilities, n):\n",
    "    probabilities = softmax(probabilities)\n",
    "    try:\n",
    "    # Sample index from probabilities, they must sum to 1\n",
    "        r = random.random()\n",
    "        probabilities = probabilities\n",
    "        cdf = 0.0\n",
    "        for i in range(n):\n",
    "            cdf += probabilities[i]\n",
    "            if r < cdf:\n",
    "                return i\n",
    "        return n - 1  # In case of rounding errors\n",
    "    except:\n",
    "        print(probabilities)\n",
    "\n",
    "# Example usage\n",
    "probabilities = cp.array([0.1, 0.2, 0.3, 0.4])  # Example list of probabilities\n",
    "n = len(probabilities)  # Number of probabilities\n",
    "index = sample(probabilities, n)\n",
    "print(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "056abef9-4b38-46a0-b941-ce7b76084eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config1.num_hidden_layers=26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60d44343-c955-48bf-89e2-283e7a20b717",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Gemma2ForCausalLM_np(config1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46716ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import torch\n",
    "\n",
    "def min_p_sampling(logits, p_base: float = 0.1) -> cp.ndarray:\n",
    "    logits = softmax(logits)\n",
    "    p_max = cp.max(logits, axis=-1, keepdims=True)\n",
    "    p_scaled = p_max * p_base\n",
    "    mask = logits >= p_scaled\n",
    "    logits = logits * mask.astype(cp.float32)\n",
    "    logits = logits / logits.sum(axis=-1, keepdims=True)\n",
    "    \n",
    "    # Use numpy's version of multinomial sampling\n",
    "    next_token =  torch.multinomial(torch.tensor(logits), num_samples=1)\n",
    "    \n",
    "    return next_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3a49204-86dc-4996-9390-075241166e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_tokens=20, streamer=None, kv_cache=None):\n",
    "    inp = tokenizer.encode(prompt, add_special_tokens=False )\n",
    "    \n",
    "\n",
    "    past_key_values = None\n",
    "    res = prompt\n",
    "    print(prompt, end=\"\")\n",
    "\n",
    "    for i in range(max_tokens):\n",
    "        if config1.use_cache or kv_cache is not None:\n",
    "            if i ==0 :\n",
    "                input_ids = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True).input_ids.numpy()\n",
    "            else:\n",
    "                input_ids = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).input_ids.numpy()\n",
    "        else:\n",
    "            input_ids = tokenizer(res, return_tensors=\"pt\", add_special_tokens=True).input_ids.numpy()\n",
    "        \n",
    "       \n",
    "        input_ids = cp.asarray(input_ids)\n",
    "        loss, logits, kv_cache, hidden_states, attentions = model(input_ids,\n",
    "                                                            use_cache = config1.use_cache,\n",
    "                                                            kv_cache = kv_cache\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "        val = min_p_sampling(logits[-1][-1])\n",
    "        #print(logits.shape)\n",
    "        \n",
    "        #val = sample(logits[-1][-1], vocab_size)\n",
    "        # val = cp.argmax(softmax(logits[-1][-1]), axis=-1)\n",
    "        # val = cp.asnumpy(val)\n",
    "\n",
    "\n",
    "        prompt = tokenizer.decode(val)\n",
    "        res += prompt\n",
    "        print(prompt, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5bf61b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import cupy as cp\n",
    "\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    \n",
    "    e_x = cp.exp(x )\n",
    "    y = e_x / cp.sum(e_x, axis=axis, keepdims=True)\n",
    "    return y\n",
    "\n",
    "# Example usage\n",
    "attn_weights_np = cp.random.randn(2, 4, 10, 10).astype(cp.float32)  # Example attention weights\n",
    "softmax_attn_weights_np = softmax(attn_weights_np, axis=-1)\n",
    "\n",
    "# If you need to convert back to a specific dtype, like in PyTorch code, you can cast it:\n",
    "# e.g., softmax_attn_weights_np = softmax_attn_weights_cp.astype(original_dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dcfcc4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.1012283  0.5376837  0.21530297 0.14578506]\n",
      "  [0.20651905 0.4059129  0.21603528 0.17153284]\n",
      "  [0.21736191 0.0113549  0.5613269  0.20995632]]\n",
      "\n",
      " [[0.60012865 0.05623438 0.0351813  0.30845568]\n",
      "  [0.15601993 0.17637    0.10369991 0.56391025]\n",
      "  [0.4496863  0.27053612 0.07809796 0.2016796 ]]]\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "\n",
    "# Define the custom CUDA kernel for softmax\n",
    "softmax_kernel = cp.RawKernel(r'''\n",
    "extern \"C\" __global__\n",
    "void softmax_kernel(const float* x, float* y, int axis_dim, int inner_dim, int outer_dim) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int total_size = outer_dim * axis_dim * inner_dim;\n",
    "\n",
    "    if (tid >= total_size) return;\n",
    "\n",
    "    int batch_idx = tid / (axis_dim * inner_dim);\n",
    "    int inner_idx = tid % inner_dim;\n",
    "    int axis_idx = (tid / inner_dim) % axis_dim;\n",
    "\n",
    "    // Compute the start index of the current softmax calculation\n",
    "    int offset = batch_idx * axis_dim * inner_dim + inner_idx;\n",
    "\n",
    "    // Find the max value for numerical stability\n",
    "    float max_val = x[offset];\n",
    "    for (int i = 1; i < axis_dim; ++i) {\n",
    "        max_val = max(max_val, x[offset + i * inner_dim]);\n",
    "    }\n",
    "\n",
    "    // Compute the sum of the exponentials\n",
    "    float sum = 0.0f;\n",
    "    for (int i = 0; i < axis_dim; ++i) {\n",
    "        sum += expf(x[offset + i * inner_dim] - max_val);\n",
    "    }\n",
    "\n",
    "    // Normalize the exponentials to get the softmax probabilities\n",
    "    y[tid] = expf(x[tid] - max_val) / sum;\n",
    "}\n",
    "''', 'softmax_kernel')\n",
    "\n",
    "# Define the softmax function using the custom kernel\n",
    "def softmax(x, axis=-1):\n",
    "    x = cp.asarray(x, dtype=cp.float32)\n",
    "    \n",
    "    # Determine the shape and dimensions for the softmax operation\n",
    "    outer_dim = int(np.prod(x.shape[:axis])) if axis != 0 else 1\n",
    "    axis_dim = x.shape[axis]\n",
    "    inner_dim = int(np.prod(x.shape[axis + 1:])) if axis != -1 else 1\n",
    "    \n",
    "    # Prepare the output array\n",
    "    y = cp.empty_like(x)\n",
    "    \n",
    "    # Define grid and block sizes for the CUDA kernel\n",
    "    block_size = 256\n",
    "    grid_size = (outer_dim * axis_dim * inner_dim + block_size - 1) // block_size\n",
    "    \n",
    "    # Launch the custom softmax kernel\n",
    "    softmax_kernel((grid_size,), (block_size,), (x, y, axis_dim, inner_dim, outer_dim))\n",
    "    \n",
    "    return y\n",
    "\n",
    "# Example usage\n",
    "x = cp.random.randn(2, 3, 4).astype(cp.float32)\n",
    "y = softmax(x, axis=-1)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56bd1e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", I had a dream to open a restaurant in a beautiful country house in the woods. It was an idea that was born when I was a child, and I've been carrying it with me all my life. I've been searching for a location that I thought would be suitable for me to build my restaurant, and I finally found the perfect place.\n",
      "\n",
      "In the middle of the woods, on a quiet street, I"
     ]
    }
   ],
   "source": [
    "\n",
    "config1.use_cache = True\n",
    "generate('Once upon a time', max_tokens=200,kv_cache=KVCache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25359f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a64d3e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def bubble_sort(arr):"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    for i in range(len(arr)):\n",
      "        for j in range(len(arr) - 1):\n",
      "            if arr[j] > arr[j + 1]:\n",
      "                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n",
      "\n",
      "def quick_sort(arr):\n",
      "    if len(arr) == 1:\n",
      "        return\n",
      "    if len(arr) == 2:\n"
     ]
    }
   ],
   "source": [
    "config1.use_cache = True\n",
    "generate('def bubble_sort(arr):', max_tokens=100,kv_cache=KVCache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0346cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
