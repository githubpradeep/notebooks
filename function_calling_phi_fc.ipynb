{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/githubpradeep/notebooks/blob/main/function_calling_phi_fc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e-52KJKvy4hM"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "# We have to check which Torch version for Xformers (2.3 -> 0.0.27)\n",
        "from torch import __version__; from packaging.version import Version as V\n",
        "xformers = \"xformers==0.0.27\" if V(__version__) < V(\"2.4.0\") else \"xformers\"\n",
        "!pip install --no-deps {xformers} trl peft accelerate bitsandbytes triton"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  langchain==0.1.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ymuaBeLTzlmI",
        "outputId": "2f7ac315-2c82-4f0b-9aac-6022065a9044"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.1.9\n",
            "  Downloading langchain-0.1.9-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.9) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.9) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.9) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.9) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.1.9)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.9) (1.33)\n",
            "Collecting langchain-community<0.1,>=0.0.21 (from langchain==0.1.9)\n",
            "  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting langchain-core<0.2,>=0.1.26 (from langchain==0.1.9)\n",
            "  Downloading langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.9) (0.1.108)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.9) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.9) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.9) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.9) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.9) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.9) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.9) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.9) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.9) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.9) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.9)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.9)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.9) (3.0.0)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2,>=0.1.26->langchain==0.1.9)\n",
            "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain==0.1.9) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain==0.1.9) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.9) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.9) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.9) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.9) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.9) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.9) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.9) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.9) (3.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain==0.1.9) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain==0.1.9) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain==0.1.9) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain==0.1.9) (0.14.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.9)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain==0.1.9) (1.2.2)\n",
            "Downloading langchain-0.1.9-py3-none-any.whl (816 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m817.0/817.0 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, mypy-extensions, typing-inspect, marshmallow, dataclasses-json, langchain-core, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.1\n",
            "    Uninstalling packaging-24.1:\n",
            "      Successfully uninstalled packaging-24.1\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.2.37\n",
            "    Uninstalling langchain-core-0.2.37:\n",
            "      Successfully uninstalled langchain-core-0.2.37\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.2.15\n",
            "    Uninstalling langchain-0.2.15:\n",
            "      Successfully uninstalled langchain-0.2.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "langchain-text-splitters 0.2.2 requires langchain-core<0.3.0,>=0.2.10, but you have langchain-core 0.1.52 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-0.1.9 langchain-community-0.0.38 langchain-core-0.1.52 marshmallow-3.22.0 mypy-extensions-1.0.0 packaging-23.2 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zOUU09wxy4hN"
      },
      "outputs": [],
      "source": [
        "template = '''\n",
        "Role: |\n",
        "  You are a function calling AI agent with self-recursion.\n",
        "  You can call only one function at a time and analyse data you get from function response.\n",
        "  You are provided with function signatures within <tools></tools> XML tags.\n",
        "  The current date is: {date}.\n",
        "Objective: |\n",
        "  You may use agentic frameworks for reasoning and planning to help with user query.\n",
        "  Please call a function and wait for function results to be provided to you in the next iteration.\n",
        "  Don't make assumptions about what values to plug into function arguments.\n",
        "  Once you have called a function, results will be fed back to you within <tool_response></tool_response> XML tags.\n",
        "  Don't make assumptions about tool results if <tool_response> XML tags are not present since function hasn't been executed yet.\n",
        "  Analyze the data once you get the results and call another function.\n",
        "  At each iteration please continue adding the your analysis to previous summary.\n",
        "  Your final response should directly answer the user query with an anlysis or summary of the results of function calls.\n",
        "Tools: |\n",
        "  Here are the available tools:\n",
        "  <tools> {tools} </tools>\n",
        "  If the provided function signatures doesn't have the function you must call, you may write executable python code in markdown syntax and call code_interpreter() function as follows:\n",
        "  <tool_call>\n",
        "  {{'arguments': {{'code_markdown': <python-code>, 'name': 'code_interpreter'}}}}\n",
        "  </tool_call>\n",
        "  Make sure that the json object above with code markdown block is parseable with json.loads() and the XML block with XML ElementTree.\n",
        "Examples: |\n",
        "  Here are some example usage of functions:\n",
        "  {examples}\n",
        "Schema: |\n",
        "  Use the following pydantic model json schema for each tool call you will make:\n",
        "  {schema}\n",
        "Instructions: |\n",
        "  At the very first turn you don't have <tool_results> so you shouldn't not make up the results.\n",
        "  Please keep a running summary with analysis of previous function results and summaries from previous iterations.\n",
        "  Do not stop calling functions until the task has been accomplished or you've reached max iteration of 10.\n",
        "  Calling multiple functions at once can overload the system and increase cost so call one function at a time please.\n",
        "  If you plan to continue with analysis, always call another function.\n",
        "  For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
        "  <tool_call>\n",
        "  {{'arguments': <args-dict>, 'name': <function-name>}}\n",
        "  </tool_call>'''\n",
        "\n",
        "from pydantic import BaseModel\n",
        "from typing import List, Dict, Literal, Optional\n",
        "import yaml\n",
        "import json\n",
        "import datetime\n",
        "from pydantic import BaseModel\n",
        "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
        "\n",
        "class FunctionCall(BaseModel):\n",
        "    arguments: dict\n",
        "    \"\"\"\n",
        "    The arguments to call the function with, as generated by the model in JSON\n",
        "    format. Note that the model does not always generate valid JSON, and may\n",
        "    hallucinate parameters not defined by your function schema. Validate the\n",
        "    arguments in your code before calling your function.\n",
        "    \"\"\"\n",
        "\n",
        "    name: str\n",
        "    \"\"\"The name of the function to call.\"\"\"\n",
        "\n",
        "class FunctionDefinition(BaseModel):\n",
        "    name: str\n",
        "    description: Optional[str] = None\n",
        "    parameters: Optional[Dict[str, object]] = None\n",
        "\n",
        "class FunctionSignature(BaseModel):\n",
        "    function: FunctionDefinition\n",
        "    type: Literal[\"function\"]\n",
        "\n",
        "def format_prompt(prompt_schema, variables) -> str:\n",
        "        formatted_prompt = \"\"\n",
        "        for field, value in prompt_schema.items():\n",
        "            if field == \"Examples\" and variables.get(\"examples\") is None:\n",
        "                continue\n",
        "            formatted_value = value.format(**variables)\n",
        "            if field == \"Instructions\":\n",
        "                formatted_prompt += f\"{formatted_value}\"\n",
        "            else:\n",
        "                formatted_value = formatted_value.replace(\"\\n\", \" \")\n",
        "                formatted_prompt += f\"{formatted_value}\"\n",
        "        return formatted_prompt\n",
        "\n",
        "def generate_prompt(user_prompt, tools, num_fewshot=None):\n",
        "\n",
        "        prompt_schema =  yaml.safe_load(template)\n",
        "\n",
        "\n",
        "        schema_json = json.loads(FunctionCall.schema_json())\n",
        "        #schema = schema_json.get(\"properties\", {})\n",
        "\n",
        "        variables = {\n",
        "            \"date\": datetime.date.today(),\n",
        "            \"tools\": tools,\n",
        "            \"schema\": schema_json\n",
        "        }\n",
        "        sys_prompt = format_prompt(prompt_schema, variables)\n",
        "\n",
        "        prompt = [\n",
        "                {'content': sys_prompt, 'role': 'system'}\n",
        "            ]\n",
        "        prompt.extend(user_prompt)\n",
        "        return prompt\n",
        "\n",
        "import ast\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import datetime\n",
        "import xml.etree.ElementTree as ET\n",
        "import re\n",
        "def get_assistant_message(completion, eos_token):\n",
        "    \"\"\"define and match pattern to find the assistant message\"\"\"\n",
        "    completion = completion.strip()\n",
        "    assistant_pattern = re.compile(r'<\\|assistant\\|>((?:(?!<\\|assistant\\|>).)*)$', re.DOTALL)\n",
        "\n",
        "    assistant_match = assistant_pattern.search(completion)\n",
        "    if assistant_match:\n",
        "        assistant_content = assistant_match.group(1).strip()\n",
        "        return assistant_content.replace(eos_token, \"\")\n",
        "    else:\n",
        "        assistant_content = None\n",
        "        return assistant_content\n",
        "\n",
        "def validate_and_extract_tool_calls(assistant_content):\n",
        "    validation_result = False\n",
        "    tool_calls = []\n",
        "    error_message = None\n",
        "    try:\n",
        "        # wrap content in root element\n",
        "        xml_root_element = f\"<root>{assistant_content}</root>\"\n",
        "        root = ET.fromstring(xml_root_element)\n",
        "        # extract JSON data\n",
        "        for element in root.findall(\".//tool_call\"):\n",
        "            json_text = element.text.strip()\n",
        "            json_data = None\n",
        "            try:\n",
        "                # Prioritize json.loads for better error handling\n",
        "                json_data = json.loads(json_text)\n",
        "            except json.JSONDecodeError as json_err:\n",
        "                try:\n",
        "                    # Fallback to ast.literal_eval if json.loads fails\n",
        "                    json_data = ast.literal_eval(json_text)\n",
        "                except (SyntaxError, ValueError) as eval_err:\n",
        "                    continue\n",
        "            if json_data is not None:\n",
        "                tool_calls.append(json_data)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "    # Return default values if no valid data is extracted\n",
        "    return tool_calls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5i6pNa6Xy4hO",
        "outputId": "9cf227c9-236a-4ffc-d235-42a42f70d683"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.2.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.4.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2024.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"Neuranest/Phi-3.5-mini-instruct-hfc\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "tokenizer = get_chat_template(\n",
        "            tokenizer,\n",
        "            chat_template = \"phi-3\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
        "            mapping = {\"role\" : \"role\", \"content\" : \"content\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
        "            #mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KDXp3KUmy4hO"
      },
      "outputs": [],
      "source": [
        "def generate(prompt):\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        prompt,#dataset[5]['conversations'][:2],\n",
        "        tokenize = True,\n",
        "        add_generation_prompt = True, # Must add for generation\n",
        "        return_tensors = \"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    from transformers import TextStreamer\n",
        "    text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "    outputs = model.generate(input_ids = inputs, #streamer = text_streamer,\n",
        "            use_cache = True,\n",
        "            max_new_tokens=1500,\n",
        "\n",
        "    eos_token_id=32007\n",
        "    )\n",
        "\n",
        "#    completion = tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=False, clean_up_tokenization_space=True)\n",
        "    completion = tokenizer.decode(outputs[0], skip_special_tokens=False, clean_up_tokenization_space=True)\n",
        "\n",
        "    return completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iFn82W61y4hO"
      },
      "outputs": [],
      "source": [
        "def execute_function_call(tool_call):\n",
        "    function_name = tool_call.get(\"name\")\n",
        "    function_to_call = globals()[function_name]\n",
        "\n",
        "    function_args = tool_call.get(\"arguments\", {})\n",
        "\n",
        "    print(f\"Invoking function call {function_name} ...{function_args.values()}\")\n",
        "    function_response = function_to_call(*function_args.values())\n",
        "    results_dict = f'{{\"name\": \"{function_name}\", \"content\": {function_response}}}'\n",
        "    return results_dict\n",
        "\n",
        "def generate_function_call(query, tools,  max_depth=5):\n",
        "        try:\n",
        "            depth = 0\n",
        "            user_message = f\"{query}\\nThis is the first turn and you don't have <tool_results> to analyze yet\"\n",
        "            chat = [{\"role\": \"user\", \"content\": user_message}]\n",
        "            prompt = generate_prompt(chat, tools)\n",
        "            completion = generate(prompt)\n",
        "\n",
        "\n",
        "            def recursive_loop(prompt, completion, depth):\n",
        "                nonlocal max_depth\n",
        "                assistant_message = get_assistant_message(completion, '<|end|>')\n",
        "                tool_calls = validate_and_extract_tool_calls(assistant_message)\n",
        "                prompt.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "                print(f\"Assistant Message:\\n{assistant_message}\")\n",
        "                tool_message = f\"Agent iteration {depth} to assist with user query: {query}\\n\"\n",
        "                if tool_calls:\n",
        "\n",
        "\n",
        "                    for tool_call in tool_calls:\n",
        "\n",
        "                        try:\n",
        "                            function_response = execute_function_call(tool_call)\n",
        "                            tool_message += f\"<tool_response>\\n{function_response}\\n</tool_response>\\n\"\n",
        "                            print(f\"Here's the response from the function call: {tool_call.get('name')}\\n{function_response}\")\n",
        "                        except Exception as e:\n",
        "                            print(f\"Could not execute function: {e}\")\n",
        "                            tool_message += f\"<tool_response>\\nThere was an error when executing the function: {tool_call.get('name')}\\nHere's the error traceback: {e}\\nPlease call this function again with correct arguments within XML tags <tool_call></tool_call>\\n</tool_response>\\n\"\n",
        "                    prompt.append({\"role\": \"tool\", \"content\": tool_message})\n",
        "\n",
        "                    depth += 1\n",
        "                    if depth >= max_depth:\n",
        "                        print(f\"Maximum recursion depth reached ({max_depth}). Stopping recursion.\")\n",
        "                        return\n",
        "\n",
        "                    completion =  generate(prompt)\n",
        "                    recursive_loop(prompt, completion, depth)\n",
        "                else:\n",
        "                    completion =  generate(prompt)\n",
        "\n",
        "            recursive_loop(prompt, completion, depth)\n",
        "\n",
        "        except Exception as e:\n",
        "\n",
        "            raise e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y0wioqBy4hO",
        "outputId": "012207f2-cf43-41da-e1f0-80a7b3c1ffb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant Message:\n",
            "<tool_call>\n",
            "{'arguments': {'location': 'San Francisco'}, 'name': 'get_current_weather'}\n",
            "</tool_call>\n",
            "<tool_call>\n",
            "{'arguments': {'location': 'Tokyo'}, 'name': 'get_current_weather'}\n",
            "</tool_call>\n",
            "<tool_call>\n",
            "{'arguments': {'location': 'Paris'}, 'name': 'get_current_weather'}\n",
            "</tool_call>\n",
            "Invoking function call get_current_weather ...dict_values(['San Francisco'])\n",
            "Here's the response from the function call: get_current_weather\n",
            "{\"name\": \"get_current_weather\", \"content\": {\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": \"fahrenheit\"}}\n",
            "Invoking function call get_current_weather ...dict_values(['Tokyo'])\n",
            "Here's the response from the function call: get_current_weather\n",
            "{\"name\": \"get_current_weather\", \"content\": {\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": \"celsius\"}}\n",
            "Invoking function call get_current_weather ...dict_values(['Paris'])\n",
            "Here's the response from the function call: get_current_weather\n",
            "{\"name\": \"get_current_weather\", \"content\": {\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": \"celsius\"}}\n",
            "Assistant Message:\n",
            "Here are the current weather conditions for the locations you asked about:\n",
            "\n",
            "1. San Francisco: The temperature is 72 degrees Fahrenheit.\n",
            "2. Tokyo: The temperature is 10 degrees Celsius.\n",
            "3. Paris: The temperature is 22 degrees Celsius.\n"
          ]
        }
      ],
      "source": [
        "# Example dummy function hard coded to return the same weather\n",
        "# In production, this could be your backend API or an external API\n",
        "def get_current_weather(location, unit=\"fahrenheit\"):\n",
        "    \"\"\"Get the current weather in a given location\"\"\"\n",
        "    if \"tokyo\" in location.lower():\n",
        "        return json.dumps(\n",
        "            {\"location\": location, \"temperature\": \"10\", \"unit\": \"celsius\"}\n",
        "        )\n",
        "    elif \"san francisco\" in location.lower():\n",
        "        return json.dumps(\n",
        "            {\"location\": location, \"temperature\": \"72\", \"unit\": \"fahrenheit\"}\n",
        "        )\n",
        "    else:\n",
        "        return json.dumps(\n",
        "            {\"location\": location, \"temperature\": \"22\", \"unit\": \"celsius\"}\n",
        "        )\n",
        "tools = json.dumps([convert_to_openai_tool(get_current_weather)])\n",
        "\n",
        "\n",
        "\n",
        "generate_function_call(\"What's the weather like in San Francisco, Tokyo, Paris? use chain of thought\", tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-p9iLB0Oy4hO"
      },
      "outputs": [],
      "source": [
        "\n",
        "import yfinance as yf\n",
        "\n",
        "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
        "\n",
        "\n",
        "def get_current_stock_price(symbol: str) -> float:\n",
        "  \"\"\"\n",
        "  Get the current stock price for a given symbol.\n",
        "\n",
        "  Args:\n",
        "    symbol (str): The stock symbol.\n",
        "\n",
        "  Returns:\n",
        "    float: The current stock price, or None if an error occurs.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    print(symbol)\n",
        "    stock = yf.Ticker(symbol)\n",
        "    # Use \"regularMarketPrice\" for regular market hours, or \"currentPrice\" for pre/post market\n",
        "    current_price = stock.info.get(\"regularMarketPrice\", stock.info.get(\"currentPrice\"))\n",
        "    return current_price if current_price else None\n",
        "  except Exception as e:\n",
        "    print(f\"Error fetching current price for {symbol}: {e}\")\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXZ-V6C0y4hO",
        "outputId": "edfa9e0e-553b-46ec-c751-d99a5726190b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant Message:\n",
            "<tool_call>\n",
            "{'arguments': {'symbol': 'TSLA'}, 'name': 'get_current_stock_price'}\n",
            "</tool_call>\n",
            "<tool_call>\n",
            "{'arguments': {'symbol': 'GOOG'}, 'name': 'get_current_stock_price'}\n",
            "</tool_call>\n",
            "Invoking function call get_current_stock_price ...dict_values(['TSLA'])\n",
            "TSLA\n",
            "Here's the response from the function call: get_current_stock_price\n",
            "{\"name\": \"get_current_stock_price\", \"content\": 214.11}\n",
            "Invoking function call get_current_stock_price ...dict_values(['GOOG'])\n",
            "GOOG\n",
            "Here's the response from the function call: get_current_stock_price\n",
            "{\"name\": \"get_current_stock_price\", \"content\": 165.11}\n",
            "Assistant Message:\n",
            "The current stock price for Tesla (TSLA) is $214.11 and for Google (GOOG) it is $165.11.\n"
          ]
        }
      ],
      "source": [
        "tools = json.dumps([convert_to_openai_tool(get_current_stock_price)])\n",
        "\n",
        "generate_function_call(\"I need the current stock price of Tesla (TSLA) and Google (GOOG)\", tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gajwvyCYy4hO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pb9Q6CDQy4hP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}