{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18e13e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'sk-'\n",
    "os.environ['COHERE_API_KEY'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aec1d269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    ")\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "# Load\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=512, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f399d93f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'page_content=\\'LLM Powered Autonomous Agents | Lil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\' metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'title\\': \"LLM Powered Autonomous Agents | Lil\\'Log\", \\'description\\': \\'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\', \\'language\\': \\'en\\'}'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(doc_splits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecf70895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chunk is part of the \"Maximum Inner Product Search (MIPS)\" section, which discusses various algorithms for efficient memory retrieval in large language model architectures. It specifically highlights ScaNN as an innovative method for anisotropic vector quantization, focusing on optimizing the similarity of inner products during the search process.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def instructions(context_variables):\n",
    "    whole_document = context_variables.get(\"whole_document\")\n",
    "    chunk = context_variables.get(\"chunk\")\n",
    "    return f'''<document> \n",
    "{whole_document}\n",
    "</document> \n",
    "Here is the chunk we want to situate within the whole document \n",
    "<chunk> \n",
    "{chunk} \n",
    "</chunk> \n",
    "'''\n",
    "\n",
    "from swarm import Swarm, Agent\n",
    "\n",
    "client = Swarm()\n",
    "\n",
    "chunk_agent = Agent(\n",
    "    name=\"Chunk Agent\",\n",
    "    instructions=instructions,\n",
    "    model = 'gpt-4o-mini'\n",
    ")\n",
    "\n",
    "context_variables = {\n",
    "    'whole_document': docs,\n",
    "    'chunk': doc_splits[10].page_content\n",
    "}\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": '''Please give a short succinct context to situate this chunk within the overall document for the purposes of\n",
    "improving search retrieval of the chunk. Answer only with the succinct context and nothing else. '''}]\n",
    "response = client.run(agent=chunk_agent, messages=messages, context_variables=context_variables,)\n",
    "\n",
    "print(response.messages[-1][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96fc69e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36d99720",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "def process_chunk(i, doc, docs, client, chunk_agent):\n",
    "    \n",
    "    print(i)\n",
    "    context_variables = {\n",
    "        'whole_document': docs,\n",
    "        'chunk': doc\n",
    "    }\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": '''Please give a short succinct context to situate this chunk within the overall document for the purposes of\n",
    "    improving search retrieval of the chunk. Answer only with the succinct context and nothing else. '''}]\n",
    "    \n",
    "    response = client.run(agent=chunk_agent, messages=messages, context_variables=context_variables,)\n",
    "    chunk = response.messages[-1][\"content\"]\n",
    "    doc.page_content = chunk + '\\n' + doc.page_content\n",
    "    return doc\n",
    "\n",
    "# List to store the processed docs\n",
    "processed_docs = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = [executor.submit(process_chunk, i, doc, docs, client, chunk_agent) for i, doc in enumerate(doc_splits)]\n",
    "    \n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        processed_docs.append(future.result())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e71145a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The chunk discusses ScaNN (Scalable Nearest Neighbors), a component of memory retrieval in the context of LLM-powered autonomous agents. It highlights ScaNN's innovation in anisotropic vector quantization, which enhances the efficiency and accuracy of information retrieval within the agent system. This information is part of a broader examination of various memory types and retrieval algorithms used to augment the capabilities of language models in autonomous agents.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization cen\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_splits[10].page_content[:800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76446722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to vectorstore\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=embedding,\n",
    ")\n",
    "\n",
    "vectorstore_retreiver = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d25389b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "keyword_retriever = BM25Retriever.from_documents(\n",
    "    doc_splits\n",
    ")\n",
    "\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[vectorstore_retreiver,\n",
    "                                                   keyword_retriever],\n",
    "                                       weights=[0.3, 0.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6690b7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "\n",
    "\n",
    "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=ensemble_retriever\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12c08a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(q):\n",
    "    print(\"retrieve\", q)\n",
    "    d = compression_retriever.invoke(q)\n",
    "    print(len(d))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f11ebeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieve types of agent memory\n",
      "3\n",
      "The types of agent memory in LLM-powered autonomous agents include:\n",
      "\n",
      "1. **Sensory Memory**: This is the first stage of memory that retains impressions of sensory information (such as visual and auditory) for a short period, typically lasting only a few seconds. Subcategories include:\n",
      "   - Iconic Memory (visual)\n",
      "   - Echoic Memory (auditory)\n",
      "   - Haptic Memory (touch)\n",
      "\n",
      "2. **Short-Term Memory (STM)**: Also known as working memory, this stores information that individuals are currently aware of and need for complex cognitive tasks such as learning and reasoning. It is believed to have a capacity of about 7 items and lasts for approximately 20-30 seconds.\n",
      "\n",
      "3. **Long-Term Memory (LTM)**: This type of memory can store information for a long time, ranging from days to decades, with essentially unlimited capacity. LTM is further divided into:\n",
      "   - **Explicit/Declarative Memory**: Memory of facts and events that can be consciously recalled, including:\n",
      "     - Episodic Memory (events and experiences)\n",
      "     - Semantic Memory (facts and concepts)\n",
      "   - **Implicit/Procedural Memory**: Unconscious memory that involves skills and routines, like riding a bike or typing.\n",
      "\n",
      "These memory types contribute significantly to the functionalities of agents, enhancing their performance in various tasks.\n"
     ]
    }
   ],
   "source": [
    "from swarm import Swarm, Agent\n",
    "\n",
    "client = Swarm()\n",
    "\n",
    "rag_agent = Agent(\n",
    "    name=\"RAG Agent\",\n",
    "    instructions=\"Answers the user's question based on data from the retrieve only\",\n",
    "    model = 'gpt-4o-mini'\n",
    ")\n",
    "rag_agent.functions.append(retrieve)\n",
    "\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"What are the types of agent memory?\"}]\n",
    "response = client.run(agent=rag_agent, messages=messages)\n",
    "\n",
    "print(response.messages[-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "840e310a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieve Scann\n",
      "3\n",
      "ScaNN (Scalable Nearest Neighbors) is a component related to memory retrieval in large language model (LLM)-powered autonomous agents. It features an innovation in anisotropic vector quantization, which enhances the efficiency and accuracy of information retrieval within such systems. The primary purpose of ScaNN is to quantize data points in a way that makes the inner product between a query and a point as similar as possible to the original distance, rather than merely selecting the closest quantization centroid points.\n",
      "\n",
      "This technology is part of a broader exploration of various memory types and algorithms used to improve the capabilities of language models in autonomous agents.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is Scann?\"}]\n",
    "response = client.run(agent=rag_agent, messages=messages)\n",
    "\n",
    "print(response.messages[-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "493e5a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieve Maximum Inner Product Search\n",
      "3\n",
      "Maximum Inner Product Search (MIPS) is a technique used in the context of optimizing retrieval from external memory sources, particularly in systems powered by large language models (LLMs). MIPS aids in quickly finding embedding representations within a vector store, which serves as long-term memory for the LLM. \n",
      "\n",
      "In MIPS, a standard practice involves storing the embedding representations of relevant information in a vector store database that supports fast retrieval. For speeding up the retrieval process, approximate nearest neighbors (ANN) algorithms are commonly employed, allowing the system to return approximate top-k nearest neighbors while sacrificing some accuracy for significant speed improvements.\n",
      "\n",
      "Here are a few common choices of ANN algorithms utilized for efficient MIPS:\n",
      "\n",
      "1. **Locality-Sensitive Hashing (LSH)**: It hashes similar items to the same buckets with high likelihood, facilitating fast retrieval.\n",
      "  \n",
      "2. **ANNOY (Approximate Nearest Neighbors Oh Yeah)**: This algorithm utilizes random projection trees, where each tree represents a hyperplane splitting the input space, helping in scalable nearest neighbor searches.\n",
      "\n",
      "3. **HNSW (Hierarchical Navigable Small World)**: This algorithm creates layers of small-world networks, allowing efficient searches by navigating through layers to reach data points, enhancing search speed and quality.\n",
      "\n",
      "4. **FAISS (Facebook AI Similarity Search)**: FAISS assumes clustering in high-dimensional spaces and applies vector quantization, facilitating efficient searching within clusters of data.\n",
      "\n",
      "These methods contribute to the ability of systems, such as autonomous agents using LLMs, to manage memory effectively and enhance the overall performance and speed of information retrieval.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": 'Maximum Inner Product Search'}]\n",
    "response = client.run(agent=rag_agent, messages=messages)\n",
    "\n",
    "print(response.messages[-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bfb822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
