{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OrFxTXOgBven"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vqDX5Bv_BwfT"
   },
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cBcIqN_SB0C9",
    "outputId": "d5b76955-a67b-4b69-a20d-034ec66031f2"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Loading the tokenizer and model from Hugging Face's model hub.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype = torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5rAeOibHB0VL",
    "outputId": "dc7a80fb-55f0-4a7d-fcf3-e6f2707bed3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7 -4  0  4  7]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def quantize_4bit(array):\n",
    "    # Normalize the array to the range [-1, 1]\n",
    "    max_abs_val = np.max(np.abs(array))\n",
    "    normalized = array / max_abs_val\n",
    "\n",
    "    # Scale to the range [-8, 7], round to nearest integer, and clamp\n",
    "    quantized = np.round(normalized * 7)\n",
    "    quantized = np.clip(quantized, -8, 7).astype(np.int8)\n",
    "\n",
    "    return quantized\n",
    "\n",
    "# Example usage\n",
    "array = np.array([-3.0, -1.5, 0.0, 1.5, 3.0])\n",
    "quantized_array = quantize_4bit(array)\n",
    "print(quantized_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fh5aHS9fCD-M",
    "outputId": "20bdd206-c9a0-48cd-a510-6585f58f16d3"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\"addmm_impl_cpu_\" not implemented for 'Half'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;124m'''\u001b[39m\u001b[38;5;124monce upon a time\u001b[39m\u001b[38;5;124m'''\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(text)\n",
      "File \u001b[0;32m~/.conda/envs/cv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cv/lib/python3.11/site-packages/transformers/generation/utils.py:1731\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1715\u001b[0m         input_ids,\n\u001b[1;32m   1716\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1727\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1728\u001b[0m     )\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1730\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1731\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1732\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1733\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1734\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1735\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.conda/envs/cv/lib/python3.11/site-packages/transformers/generation/utils.py:2592\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2589\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2591\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2592\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2593\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2595\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2596\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2597\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2600\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/cv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/cv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1183\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1180\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1183\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/cv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/cv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1070\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1061\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1062\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1067\u001b[0m         use_cache,\n\u001b[1;32m   1068\u001b[0m     )\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1070\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1079\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.conda/envs/cv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/cv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:798\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    795\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    797\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 798\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    807\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    809\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/cv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/cv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:386\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(value_states, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m     query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[1;32m    388\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n",
      "File \u001b[0;32m~/.conda/envs/cv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/cv/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"addmm_impl_cpu_\" not implemented for 'Half'"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer('''once upon a time''', return_tensors=\"pt\", return_attention_mask=False)\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=200)\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1_csDPFoCs9X"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf tinyllama\n",
    "!mkdir tinyllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "NlGdDpONFE_b"
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "QKLW4EoYFJFW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([32000, 2048])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([2048, 5632])\n",
      "model.layers.0.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([2048, 5632])\n",
      "model.layers.1.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([2048, 5632])\n",
      "model.layers.2.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([2048, 5632])\n",
      "model.layers.3.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([2048, 5632])\n",
      "model.layers.4.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([2048, 5632])\n",
      "model.layers.5.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([2048, 5632])\n",
      "model.layers.6.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([2048, 5632])\n",
      "model.layers.7.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([2048, 5632])\n",
      "model.layers.8.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([2048, 5632])\n",
      "model.layers.9.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([2048, 5632])\n",
      "model.layers.10.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([2048, 5632])\n",
      "model.layers.11.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([2048, 5632])\n",
      "model.layers.12.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([2048, 5632])\n",
      "model.layers.13.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([2048, 5632])\n",
      "model.layers.14.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([2048, 5632])\n",
      "model.layers.15.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([5632, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.16.mlp.up_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([2048, 5632])\n",
      "model.layers.16.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([2048, 5632])\n",
      "model.layers.17.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([2048, 5632])\n",
      "model.layers.18.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([2048, 5632])\n",
      "model.layers.19.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([2048, 5632])\n",
      "model.layers.20.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([5632, 2048])\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([2048, 5632])\n",
      "model.layers.21.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.norm.weight torch.Size([2048])\n",
      "lm_head.weight torch.Size([32000, 2048])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.size())\n",
    "    if not os.path.isfile(f'tinyllama/{name}_weights.npy'):\n",
    "        weights = param.data.numpy()\n",
    "        #weights = quantize_4bit(weights)\n",
    "\n",
    "        np.save(f'tinyllama/{name}_weights.npy', weights)\n",
    "\n",
    "    if hasattr(param, 'bias'):\n",
    "        if not os.path.isfile(f'tinyllama/{name}_bias.npy'):\n",
    "            bias = param.bias.data.numpy()\n",
    "            #bias = quantize_4bit(bias)\n",
    "            np.save(f'tinyllama/{name}_bias.npy', bias)\n",
    "    params = None\n",
    "    weights = None\n",
    "    bias = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tBsoQZx5FQbc"
   },
   "outputs": [],
   "source": [
    "config = {'vocab_size': 32000,\n",
    " 'max_position_embeddings': 2048,\n",
    " 'hidden_size': 2048,\n",
    " 'intermediate_size': 5632,\n",
    " 'num_hidden_layers': 22,\n",
    " 'num_attention_heads': 32,\n",
    " 'num_key_value_heads': 4,\n",
    " 'hidden_act': 'silu',\n",
    " 'initializer_range': 0.02,\n",
    " 'rms_norm_eps': 1e-05,\n",
    " 'pretraining_tp': 1,\n",
    " 'use_cache': True,\n",
    " 'rope_theta': 10000.0,\n",
    " 'rope_scaling': None,\n",
    " 'attention_bias': False,\n",
    " 'return_dict': True,\n",
    " 'output_hidden_states': False,\n",
    " 'output_attentions': False,\n",
    " 'torchscript': False,\n",
    " 'torch_dtype': 'bfloat16',\n",
    " 'use_bfloat16': False,\n",
    " 'tf_legacy_loss': False,\n",
    " 'pruned_heads': {},\n",
    " 'tie_word_embeddings': False,\n",
    " 'is_encoder_decoder': False,\n",
    " 'is_decoder': False,\n",
    " 'cross_attention_hidden_size': None,\n",
    " 'add_cross_attention': False,\n",
    " 'tie_encoder_decoder': False,\n",
    " 'max_length': 20,\n",
    " 'min_length': 0,\n",
    " 'do_sample': False,\n",
    " 'early_stopping': False,\n",
    " 'num_beams': 1,\n",
    " 'num_beam_groups': 1,\n",
    " 'diversity_penalty': 0.0,\n",
    " 'temperature': 1.0,\n",
    " 'top_k': 50,\n",
    " 'top_p': 1.0,\n",
    " 'typical_p': 1.0,\n",
    " 'repetition_penalty': 1.0,\n",
    " 'length_penalty': 1.0,\n",
    " 'no_repeat_ngram_size': 0,\n",
    " 'encoder_no_repeat_ngram_size': 0,\n",
    " 'bad_words_ids': None,\n",
    " 'num_return_sequences': 1,\n",
    " 'chunk_size_feed_forward': 0,\n",
    " 'output_scores': False,\n",
    " 'return_dict_in_generate': False,\n",
    " 'forced_bos_token_id': None,\n",
    " 'forced_eos_token_id': None,\n",
    " 'remove_invalid_values': False,\n",
    " 'exponential_decay_length_penalty': None,\n",
    " 'suppress_tokens': None,\n",
    " 'begin_suppress_tokens': None,\n",
    " 'architectures': ['LlamaForCausalLM'],\n",
    " 'finetuning_task': None,\n",
    " 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'},\n",
    " 'label2id': {'LABEL_0': 0, 'LABEL_1': 1},\n",
    " 'tokenizer_class': None,\n",
    " 'prefix': None,\n",
    " 'bos_token_id': 1,\n",
    " 'pad_token_id': None,\n",
    " 'eos_token_id': 2,\n",
    " 'sep_token_id': None,\n",
    " 'decoder_start_token_id': None,\n",
    " 'task_specific_params': None,\n",
    " 'problem_type': None,\n",
    " '_name_or_path': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
    " 'transformers_version': '4.35.2',\n",
    " 'model_type': 'llama'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from os import environ\n",
    "environ['OMP_NUM_THREADS'] = '16'\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UqwVU8EcGuAv",
    "outputId": "e23c0a06-6859-479b-e312-9d12cf2fbb7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bobbyhadz.com\n",
      "Python\n",
      "{'id': 1, 'website': 'bobbyhadz.com', 'topic': 'Python', 'author': 'Borislav Hadzhiev'}\n",
      "{'id': 1, 'website': 'bobbyhadz.com', 'topic': 'Python'}\n"
     ]
    }
   ],
   "source": [
    "class AttributeDict(dict):\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "\n",
    "my_dict = {'id': 1, 'website': 'bobbyhadz.com', 'topic': 'Python'}\n",
    "\n",
    "new_dict = AttributeDict(my_dict)\n",
    "\n",
    "print(new_dict.website)  #  bobbyhadz.com\n",
    "print(new_dict.topic)  #  Python\n",
    "\n",
    "new_dict.author = 'Borislav Hadzhiev'\n",
    "\n",
    "#  {'id': 1, 'website': 'bobbyhadz.com', 'topic': 'Python', 'author': 'Borislav Hadzhiev'}\n",
    "print(new_dict)\n",
    "\n",
    "del new_dict.author\n",
    "\n",
    "#  {'id': 1, 'website': 'bobbyhadz.com', 'topic': 'Python'}\n",
    "print(new_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "hjwqgVwcGxV2"
   },
   "outputs": [],
   "source": [
    "config1 = AttributeDict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "btLOL4VLFkts"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "zeros = [0] * config['hidden_size']\n",
    "embedding = np.random.randn(config['vocab_size'], config['hidden_size'])\n",
    "vocab_size = config['vocab_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "yxThUiB3Fh9L"
   },
   "outputs": [],
   "source": [
    "embedding = np.load('tinyllama/model.embed_tokens.weight_weights.npy', mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "j3SS9exQF6e3"
   },
   "outputs": [],
   "source": [
    "class RotaryEmbedding():\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (np.arange(0, self.dim, 2) / self.dim))\n",
    "        self.inv_freq = inv_freq\n",
    "\n",
    "\n",
    "        # Build here to make `torch.jit.trace` work.\n",
    "        self._set_cos_sin_cache(\n",
    "            seq_len=max_position_embeddings\n",
    "        )\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = np.arange(self.max_seq_len_cached)\n",
    "\n",
    "        freqs = np.outer(t, self.inv_freq)\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = np.concatenate((freqs, freqs), axis=-1)\n",
    "        self.cos_cached = np.cos(emb)\n",
    "        self.sin_cached = np.sin(emb)\n",
    "\n",
    "\n",
    "    def __call__(self, x, seq_len=None):\n",
    "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len=seq_len)\n",
    "\n",
    "        return (\n",
    "            self.cos_cached[:seq_len],\n",
    "            self.sin_cached[:seq_len],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "zjGaC0HSGCVT"
   },
   "outputs": [],
   "source": [
    "rot = RotaryEmbedding(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "nloquy33GNVW"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def rotate_half_np(x):\n",
    "    \"\"\"\n",
    "    Rotates half the hidden dims of the input, implemented in NumPy.\n",
    "    Args:\n",
    "        x (numpy.ndarray): Input array.\n",
    "    Returns:\n",
    "        numpy.ndarray: Rotated array.\n",
    "    \"\"\"\n",
    "    # Split the array into two halves along the last dimension\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "\n",
    "    # Reverse the second half and concatenate with the first half\n",
    "    # NumPy's negative indexing reverses the array\n",
    "    return np.concatenate((-x2, x1), axis=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n",
    "    cos = np.expand_dims( cos[position_ids], unsqueeze_dim)\n",
    "    sin = np.expand_dims( sin[position_ids], unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half_np(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half_np(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def gelu_np(input):\n",
    "    return 0.5 * input * (1.0 + np.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * np.power(input, 3.0))))\n",
    "\n",
    "# Example usage\n",
    "input_np = np.random.randn(10)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def silu(x):\n",
    "    return x * (1 / (1 + np.exp(-x)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example activation function mapping (adjust as needed)\n",
    "ACT2FN_np = {\n",
    "    'relu': np.vectorize(lambda x: max(0, x)),\n",
    "    # Add other activation functions as needed\n",
    "    'gelu_new': gelu_np,\n",
    "    'silu': silu\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zVbnldACGh5c",
    "outputId": "54b26c9b-d228-45ab-877e-ac7d744ef4cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 6, 4, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def repeat_kv_np(hidden_states, n_rep):\n",
    "    \"\"\"\n",
    "    Replicates the behavior of torch.repeat_interleave for a specific use-case.\n",
    "    The hidden states go from (batch, num_key_value_heads, seqlen, head_dim)\n",
    "    to (batch, num_attention_heads, seqlen, head_dim) in NumPy.\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "\n",
    "    # Expand and then repeat the array along the third axis\n",
    "    hidden_states_expanded = np.expand_dims(hidden_states, axis=2)\n",
    "    hidden_states_repeated = np.tile(hidden_states_expanded, (1, 1, n_rep, 1, 1))\n",
    "\n",
    "    # Reshape the array to the desired shape\n",
    "    return hidden_states_repeated.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "# Example usage\n",
    "hidden_states = np.random.randn(2, 3, 4, 5)  # Example input tensor with shape (batch, num_key_value_heads, slen, head_dim)\n",
    "n_rep = 2  # Example repetition factor\n",
    "output = repeat_kv_np(hidden_states, n_rep)\n",
    "print(output.shape)  # The shape should be (2, 6, 4, 5) in this example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IGyISflpGmDg",
    "outputId": "964e7b03-a950-49e6-c54b-dd196be236d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.75198482 -0.42175198 -2.04647616]\n",
      " [-0.04557262  1.76708609 -1.98061853]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Linear_np:\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        # Initialize weights and bias\n",
    "        self.weights = np.random.randn(out_features, in_features)\n",
    "        self.bias = np.random.randn(out_features)\n",
    "\n",
    "\n",
    "    def load(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Perform the linear operation (y = xA^T + b)\n",
    "        if self.bias is not None:\n",
    "            return np.dot(x, self.weights.T) + self.bias\n",
    "        return np.dot(x, self.weights.T)\n",
    "\n",
    "# Example usage\n",
    "input_features = 5\n",
    "output_features = 3\n",
    "linear_layer = Linear_np(input_features, output_features)\n",
    "\n",
    "# Example input (batch_size x input_features)\n",
    "x = np.random.randn(2, input_features)\n",
    "\n",
    "# Forward pass\n",
    "output = linear_layer(x)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "vut0pDpCGaSj"
   },
   "outputs": [],
   "source": [
    "class MLP_np():\n",
    "    def __init__(self, config, layer_index):\n",
    "\n",
    "        self.config = config\n",
    "        self.layer_index = layer_index\n",
    "        self.activation_fn = ACT2FN_np[config.hidden_act]\n",
    "\n",
    "\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.gate_proj = Linear_np(config.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.up_proj = Linear_np(config.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.down_proj = Linear_np(self.intermediate_size, config.hidden_size, bias=False)\n",
    "        self.act_fn = ACT2FN_np[config.hidden_act]\n",
    "\n",
    "\n",
    "        weights = np.load(f'tinyllama/model.layers.{layer_index}.mlp.gate_proj.weight_weights.npy', mmap_mode='r')\n",
    "        self.gate_proj.load(weights, bias=None)\n",
    "\n",
    "        weights = np.load(f'tinyllama/model.layers.{layer_index}.mlp.up_proj.weight_weights.npy', mmap_mode='r')\n",
    "        self.up_proj.load(weights, bias=None)\n",
    "\n",
    "        weights = np.load(f'tinyllama/model.layers.{layer_index}.mlp.down_proj.weight_weights.npy', mmap_mode='r')\n",
    "        self.down_proj.load(weights, bias=None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, x) :\n",
    "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "PyxjZ87tGptT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# model.layers.0.input_layernorm.weight torch.Size([4096])\n",
    "# model.layers.0.post_attention_layernorm.weight torch.Size([4096])\n",
    "# model.norm.weight torch.Size([4096])\n",
    "class RMSNorm:\n",
    "    def __init__(self, hidden_size, layer_idx=-1, post=False, eps=1e-6):\n",
    "        \"\"\"\n",
    "        MistralRMSNorm is equivalent to T5LayerNorm, implemented in NumPy.\n",
    "        \"\"\"\n",
    "        self.weight = np.ones(hidden_size)\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "        if layer_idx == -1:\n",
    "\n",
    "            wt = np.load(f'tinyllama/model.norm.weight_weights.npy', mmap_mode='r')\n",
    "            self.weight = wt\n",
    "        else:\n",
    "            if post:\n",
    "                wt = np.load(f'tinyllama/model.layers.{layer_idx}.post_attention_layernorm.weight_weights.npy', mmap_mode='r')\n",
    "            else:\n",
    "                wt = np.load(f'tinyllama/model.layers.{layer_idx}.input_layernorm.weight_weights.npy', mmap_mode='r')\n",
    "\n",
    "            self.weight = wt\n",
    "\n",
    "    def __call__(self, hidden_states):\n",
    "        # Compute variance\n",
    "        variance = np.mean(np.square(hidden_states), axis=-1, keepdims=True)\n",
    "\n",
    "        # Normalize hidden states\n",
    "        hidden_states_normalized = hidden_states * np.reciprocal(np.sqrt(variance + self.variance_epsilon))\n",
    "\n",
    "        # Apply learned weights\n",
    "        return self.weight * hidden_states_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2eizCBVUHC0e",
    "outputId": "805ef93d-c44b-43e7-9617-e128d1b735ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.01501403, -0.0527625 , -0.27735808, ...,  0.04456021,\n",
       "          0.01843926,  0.12067095]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randn(1,1,2048)\n",
    "m = RMSNorm(2048, layer_idx=0, post=True)\n",
    "m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ADyxvWZMHFSK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x along the specified axis.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "# Example usage\n",
    "attn_weights_np = np.random.randn(2, 4, 10, 10).astype(np.float32)  # Example attention weights\n",
    "softmax_attn_weights_np = softmax(attn_weights_np, axis=-1)\n",
    "\n",
    "# If you need to convert back to a specific dtype, like in PyTorch code, you can cast it:\n",
    "# e.g., softmax_attn_weights_np = softmax_attn_weights_np.astype(original_dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mDuhJUWsSoQa",
    "outputId": "731d116a-f37e-4146-eea2-679882edf6ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 2]\n",
      "  [3 4]]\n",
      "\n",
      " [[1 2]\n",
      "  [3 4]]\n",
      "\n",
      " [[1 2]\n",
      "  [3 4]]\n",
      "\n",
      " [[1 2]\n",
      "  [3 4]]\n",
      "\n",
      " [[1 2]\n",
      "  [3 4]]\n",
      "\n",
      " [[1 2]\n",
      "  [3 4]]\n",
      "\n",
      " [[1 2]\n",
      "  [3 4]]\n",
      "\n",
      " [[1 2]\n",
      "  [3 4]]]\n",
      "(8, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original 2x2 matrix\n",
    "matrix_2x2 = np.array([[1, 2],\n",
    "                       [3, 4]])\n",
    "\n",
    "# Repeat the matrix to get an 8x2x2 matrix\n",
    "matrix_8x2x2 = np.repeat(matrix_2x2[np.newaxis, :, :], repeats=8, axis=0)\n",
    "\n",
    "print(matrix_8x2x2)\n",
    "print(matrix_8x2x2.shape)  # Should be (8, 2, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oX8aqaR9RI6O",
    "outputId": "11b60fb9-c2b6-460c-ce49-0446ea63279e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 2.48910188e-01, -3.40282347e+38, -3.40282347e+38,\n",
       "          -3.40282347e+38, -3.40282347e+38],\n",
       "         [ 9.38076493e-01,  4.77504169e-01, -3.40282347e+38,\n",
       "          -3.40282347e+38, -3.40282347e+38],\n",
       "         [ 3.19648611e-01,  2.80959636e-01,  8.94178996e-01,\n",
       "          -3.40282347e+38, -3.40282347e+38],\n",
       "         [ 9.23695647e-01,  9.14898012e-01,  8.64843013e-01,\n",
       "           8.25265678e-01, -3.40282347e+38]]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_masked_context_mask(mask, diagonal=0, dtype=np.float32):\n",
    "    \"\"\"\n",
    "    Apply a context mask to the given mask array.\n",
    "\n",
    "    :param mask: The original mask array (2D or 4D).\n",
    "    :param diagonal: The diagonal offset for the upper triangular part.\n",
    "    :param dtype: The data type to determine the minimum value.\n",
    "    :return: The masked array with applied context mask.\n",
    "    \"\"\"\n",
    "    b,k, s, q = mask.shape\n",
    "    # Create an upper triangular matrix (context mask)\n",
    "    context_mask = 1 - np.tril(np.ones((s,q), dtype=int), k=0)\n",
    "\n",
    "    context_mask = np.repeat(context_mask[np.newaxis, np.newaxis, :, :], repeats=k, axis=1)\n",
    "\n",
    "  # Then, repeat the resulting matrix to get an (8, 32, 2, 2) matrix\n",
    "    context_mask = np.repeat(context_mask, repeats=b, axis=0)\n",
    "\n",
    "\n",
    "    # Apply the context mask to the original mask\n",
    "    masked_mask = np.where(context_mask, np.finfo(dtype).min, mask)\n",
    "\n",
    "    return masked_mask\n",
    "\n",
    "# Example usage\n",
    "mask = np.random.rand(1,1, 4, 5)  # Replace with your actual mask\n",
    "diagonal = 0\n",
    "masked_mask = create_masked_context_mask(mask, diagonal)\n",
    "masked_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "2Y5hSN2yHLNS"
   },
   "outputs": [],
   "source": [
    "class Attention():\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config, layer_idx = None):\n",
    "\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "        self.rope_theta = config.rope_theta\n",
    "        self.is_causal = True\n",
    "\n",
    "        if (self.head_dim * self.num_heads) != self.hidden_size:\n",
    "            raise ValueError(\n",
    "                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n",
    "                f\" and `num_heads`: {self.num_heads}).\"\n",
    "            )\n",
    "\n",
    "        self.q_proj = Linear_np(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = Linear_np(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = Linear_np(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = Linear_np(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n",
    "\n",
    "\n",
    "\n",
    "        self._init_rope()\n",
    "\n",
    "\n",
    "        weights =np.load(f'tinyllama/model.layers.{layer_idx}.self_attn.q_proj.weight_weights.npy', mmap_mode='r')\n",
    "        self.q_proj.load(weights, bias=None)\n",
    "\n",
    "        weights =np.load(f'tinyllama/model.layers.{layer_idx}.self_attn.k_proj.weight_weights.npy', mmap_mode='r')\n",
    "        self.k_proj.load(weights, bias=None)\n",
    "\n",
    "        weights =np.load(f'tinyllama/model.layers.{layer_idx}.self_attn.v_proj.weight_weights.npy', mmap_mode='r')\n",
    "        self.v_proj.load(weights, bias=None)\n",
    "\n",
    "        weights =np.load(f'tinyllama/model.layers.{layer_idx}.self_attn.o_proj.weight_weights.npy', mmap_mode='r')\n",
    "        self.o_proj.load(weights, bias=None)\n",
    "\n",
    "\n",
    "\n",
    "    def _init_rope(self):\n",
    "        self.rotary_emb = RotaryEmbedding(\n",
    "            self.head_dim,\n",
    "            max_position_embeddings=self.max_position_embeddings,\n",
    "            base=self.rope_theta,\n",
    "        )\n",
    "\n",
    "\n",
    "    def _split_heads(self, fused_qkv):\n",
    "        batch_size, seq_length, _ = fused_qkv.shape\n",
    "        reshaped = fused_qkv.reshape(batch_size, seq_length, self.num_heads, 3, self.head_dim)\n",
    "        query = reshaped[..., 0, :]\n",
    "        key = reshaped[..., 1, :]\n",
    "        value = reshaped[..., 2, :]\n",
    "        return query, key, value\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask = None,\n",
    "        position_ids = None,\n",
    "        past_key_value = None,\n",
    "        output_attentions = False,\n",
    "        use_cache = False,\n",
    "    ):\n",
    "        bsz, q_len, _ = hidden_states.shape\n",
    "\n",
    "\n",
    "\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "\n",
    "\n",
    "        query_states = query_states.reshape(bsz, q_len, self.num_heads, self.head_dim).transpose(0, 2, 1, 3)\n",
    "        key_states = key_states.reshape(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(0, 2, 1, 3)\n",
    "        value_states = value_states.reshape(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(0, 2, 1, 3)\n",
    "\n",
    "\n",
    "\n",
    "        kv_seq_len = key_states.shape[-2]\n",
    "        if past_key_value is not None:\n",
    "            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n",
    "        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n",
    "\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"partial_rotation_size\": self.rotary_emb.dim}\n",
    "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "\n",
    "        key_states = repeat_kv_np(key_states, self.num_key_value_groups)\n",
    "        value_states = repeat_kv_np(value_states, self.num_key_value_groups)\n",
    "\n",
    "\n",
    "        # Queries and keys upcast to fp32 is required by Phi-2 to avoid overflow\n",
    "        attn_weights = np.matmul(\n",
    "            query_states, key_states.transpose(0, 1, 3, 2)\n",
    "        ) / np.sqrt(self.head_dim)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.shape != (bsz, 1, q_len, kv_seq_len):\n",
    "                print(bsz, 1, q_len, kv_seq_len)\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = attn_weights + attention_mask\n",
    "\n",
    "        # upcast attention to fp32\n",
    "\n",
    "\n",
    "\n",
    "        attn_weights = create_masked_context_mask(attn_weights, 0)\n",
    "\n",
    "        attn_weights = softmax(attn_weights, axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "        attn_output = np.matmul(attn_weights, value_states)\n",
    "\n",
    "\n",
    "\n",
    "        attn_output = attn_output.transpose(0, 2, 1, 3)\n",
    "\n",
    "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
    "\n",
    "\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights, past_key_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5hpGz0tSHUKY",
    "outputId": "3ac86974-5d4f-4930-d03d-7dd5b01b5120"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 2048)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = Attention(config1, 0)\n",
    "x = np.random.randn(1,2,2048)\n",
    "attn(x)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RObY6mX5EhGK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "zb7-s3SwHZG7"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer():\n",
    "    def __init__(self, config, layer_idx: int):\n",
    "        self.self_attn = Attention(config, layer_idx=layer_idx)\n",
    "        self.mlp = MLP_np(config, layer_index=layer_idx)\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size, layer_idx=layer_idx, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size, layer_idx=layer_idx, post=True, eps=config.rms_norm_eps)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask = None,\n",
    "        position_ids  = None,\n",
    "        output_attentions  = False,\n",
    "        use_cache  = False,\n",
    "        past_key_value  = None,\n",
    "    ) :\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`):\n",
    "                input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "            position_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n",
    "                Indices of positions of each input sequence tokens in the position embeddings. Selected in the range\n",
    "                `[0, config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            use_cache (`bool`, *optional*):\n",
    "                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
    "                (see `past_key_values`).\n",
    "            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n",
    "        \"\"\"\n",
    "\n",
    "        residual = hidden_states\n",
    "\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        # Self Attention\n",
    "        attn_outputs, self_attn_weights, present_key_value = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_value=past_key_value,\n",
    "            output_attentions=output_attentions,\n",
    "            use_cache=use_cache,\n",
    "        )\n",
    "        attn_outputs = attn_outputs\n",
    "\n",
    "\n",
    "        hidden_states = residual + attn_outputs\n",
    "\n",
    "        residual = hidden_states\n",
    "\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states =  residual + hidden_states\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (self_attn_weights,)\n",
    "\n",
    "        if use_cache:\n",
    "            outputs += (present_key_value,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kkuto6UIHj5n",
    "outputId": "643328d6-d9cb-4d20-df73-4700167996d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[-0.18924278, -1.1581644 ,  1.03201109, ...,  0.86278668,\n",
       "          -1.7153241 , -1.87950691],\n",
       "         [ 2.72335772,  0.93399983,  1.31827286, ...,  1.02743189,\n",
       "           0.48744402,  1.10102729]]]),)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = DecoderLayer(config1, 0)\n",
    "dec(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "K4_BXx3RH35X"
   },
   "outputs": [],
   "source": [
    "class Cache:\n",
    "    \"\"\"\n",
    "    Base, abstract class for all caches. The actual data structure is specific to each subclass.\n",
    "    \"\"\"\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        key_states,\n",
    "        value_states,\n",
    "        layer_idx: int,\n",
    "        cache_kwargs= None,\n",
    "    ) :\n",
    "        \"\"\"\n",
    "        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n",
    "\n",
    "        Parameters:\n",
    "            key_states (`torch.Tensor`):\n",
    "                The new key states to cache.\n",
    "            value_states (`torch.Tensor`):\n",
    "                The new value states to cache.\n",
    "            layer_idx (`int`):\n",
    "                The index of the layer to cache the states for.\n",
    "            cache_kwargs (`Dict[str, Any]`, `optional`):\n",
    "                Additional arguments for the cache subclass. These are specific to each subclass and allow new types of\n",
    "                cache to be created.\n",
    "\n",
    "        Return:\n",
    "            A tuple containing the updated key and value states.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Make sure to implement `update` in a subclass.\")\n",
    "\n",
    "    def get_seq_length(self, layer_idx= 0) :\n",
    "        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n",
    "        raise NotImplementedError(\"Make sure to implement `get_seq_length` in a subclass.\")\n",
    "\n",
    "    def get_max_length(self):\n",
    "        \"\"\"Returns the maximum sequence length of the cached states, if there is any.\"\"\"\n",
    "        raise NotImplementedError(\"Make sure to implement `get_max_length` in a subclass.\")\n",
    "\n",
    "    def get_usable_length(self, new_seq_length: int, layer_idx = 0):\n",
    "        \"\"\"Given the sequence length of the new inputs, returns the usable length of the cache.\"\"\"\n",
    "        # Cache without size limit -> all cache is usable\n",
    "        # Cache with size limit -> if the length cache plus the length of the new inputs is larger the maximum cache\n",
    "        #   length, we will need to evict part of the cache (and thus not all cache is usable)\n",
    "        max_length = self.get_max_length()\n",
    "        previous_seq_length = self.get_seq_length(layer_idx)\n",
    "        if max_length is not None and previous_seq_length + new_seq_length > max_length:\n",
    "            return max_length - new_seq_length\n",
    "        return previous_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "plUThvTSH4r4"
   },
   "outputs": [],
   "source": [
    "class DynamicCache(Cache):\n",
    "    \"\"\"\n",
    "    A cache that grows dynamically as more tokens are generated. This is the default for generative models.\n",
    "\n",
    "    It stores the Key and Value states as a list of tensors, one for each layer. The expected shape for each tensor is\n",
    "    `[batch_size, num_heads, seq_len, head_dim]`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) :\n",
    "        self.key_cache = []\n",
    "        self.value_cache= []\n",
    "        self.seen_tokens = 0  # Used in `generate` to keep tally of how many tokens the cache has seen\n",
    "\n",
    "    def __getitem__(self, layer_idx: int) :\n",
    "        \"\"\"\n",
    "        Support for backwards-compatible `past_key_value` indexing, e.g. `past_key_value[0][0].shape[2]` to get the\n",
    "        sequence length.\n",
    "        \"\"\"\n",
    "        if layer_idx < len(self):\n",
    "            return (self.key_cache[layer_idx], self.value_cache[layer_idx])\n",
    "        else:\n",
    "            raise KeyError(f\"Cache only has {len(self)} layers, attempted to access layer with index {layer_idx}\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Support for backwards-compatible `past_key_value` iteration, e.g. `for x in past_key_value:` to iterate over\n",
    "        keys and values\n",
    "        \"\"\"\n",
    "        for layer_idx in range(len(self)):\n",
    "            yield (self.key_cache[layer_idx], self.value_cache[layer_idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Support for backwards-compatible `past_key_value` length, e.g. `len(past_key_value)`. This value corresponds\n",
    "        to the number of layers in the model.\n",
    "        \"\"\"\n",
    "        return len(self.key_cache)\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        key_states,\n",
    "        value_states,\n",
    "        layer_idx: int,\n",
    "        cache_kwargs = None,\n",
    "    ) :\n",
    "        \"\"\"\n",
    "        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n",
    "\n",
    "        Parameters:\n",
    "            key_states (`torch.Tensor`):\n",
    "                The new key states to cache.\n",
    "            value_states (`torch.Tensor`):\n",
    "                The new value states to cache.\n",
    "            layer_idx (`int`):\n",
    "                The index of the layer to cache the states for.\n",
    "            cache_kwargs (`Dict[str, Any]`, `optional`):\n",
    "                Additional arguments for the cache subclass. No additional arguments are used in `DynamicCache`.\n",
    "\n",
    "        Return:\n",
    "            A tuple containing the updated key and value states.\n",
    "        \"\"\"\n",
    "        # Update the number of seen tokens\n",
    "        if layer_idx == 0:\n",
    "            self.seen_tokens += key_states.shape[-2]\n",
    "\n",
    "        # Update the cache\n",
    "        if len(self.key_cache) <= layer_idx:\n",
    "            self.key_cache.append(key_states)\n",
    "            self.value_cache.append(value_states)\n",
    "        else:\n",
    "            self.key_cache[layer_idx] = np.concatenate([self.key_cache[layer_idx], key_states], axis=-2)\n",
    "            self.value_cache[layer_idx] = np.concatenate([self.value_cache[layer_idx], value_states], axis=-2)\n",
    "\n",
    "        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n",
    "\n",
    "    def get_seq_length(self, layer_idx = 0) -> int:\n",
    "        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n",
    "        if len(self.key_cache) <= layer_idx:\n",
    "            return 0\n",
    "        return self.key_cache[layer_idx].shape[-2]\n",
    "\n",
    "    def get_max_length(self) :\n",
    "        \"\"\"Returns the maximum sequence length of the cached states. DynamicCache does not have a maximum length.\"\"\"\n",
    "        return None\n",
    "\n",
    "    def reorder_cache(self, beam_idx):\n",
    "        \"\"\"Reorders the cache for beam search, given the selected beam indices.\"\"\"\n",
    "        for layer_idx in range(len(self.key_cache)):\n",
    "            device = self.key_cache[layer_idx].device\n",
    "            self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx.to(device))\n",
    "            device = self.value_cache[layer_idx].device\n",
    "            self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n",
    "\n",
    "    def to_legacy_cache(self) :\n",
    "        \"\"\"Converts the `DynamicCache` instance into the its equivalent in the legacy cache format.\"\"\"\n",
    "        legacy_cache = ()\n",
    "        for layer_idx in range(len(self)):\n",
    "            legacy_cache += ((self.key_cache[layer_idx], self.value_cache[layer_idx]),)\n",
    "        return legacy_cache\n",
    "\n",
    "    @classmethod\n",
    "    def from_legacy_cache(cls, past_key_values = None) :\n",
    "        \"\"\"Converts a cache in the legacy cache format into an equivalent `DynamicCache`.\"\"\"\n",
    "        cache = cls()\n",
    "        if past_key_values is not None:\n",
    "            for layer_idx in range(len(past_key_values)):\n",
    "                key_states, value_states = past_key_values[layer_idx]\n",
    "                cache.update(key_states, value_states, layer_idx)\n",
    "        return cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "FIuSu2YMHpVe"
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        self.config = config\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = np.load('tinyllama/model.embed_tokens.weight_weights.npy', mmap_mode='r')\n",
    "\n",
    "        self.layers = [DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "\n",
    "        self.final_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self._use_flash_attention_2 = False\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "        # Initialize weights and apply final processing\n",
    "\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embed_tokens = value\n",
    "\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        input_ids = None,\n",
    "        attention_mask = None,\n",
    "        position_ids = None,\n",
    "        past_key_values = None,\n",
    "        inputs_embeds= None,\n",
    "        use_cache= None,\n",
    "        output_attentions = None,\n",
    "        output_hidden_states= None,\n",
    "        return_dict = None,\n",
    "    ) :\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "\n",
    "        return_dict = False\n",
    "\n",
    "        # retrieve input_ids and inputs_embeds\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            batch_size, seq_length = input_ids.shape[:2]\n",
    "        elif inputs_embeds is not None:\n",
    "            batch_size, seq_length = inputs_embeds.shape[:2]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        past_key_values_length = 0\n",
    "\n",
    "\n",
    "\n",
    "        if use_cache:\n",
    "            use_legacy_cache = not isinstance(past_key_values, Cache)\n",
    "            if use_legacy_cache:\n",
    "                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n",
    "            past_key_values_length = past_key_values.get_usable_length(seq_length)\n",
    "\n",
    "        if position_ids is None:\n",
    "\n",
    "            position_ids = np.arange(\n",
    "                past_key_values_length, seq_length + past_key_values_length, dtype=np.int64\n",
    "            )\n",
    "            position_ids = np.expand_dims(position_ids, 0)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens[input_ids]\n",
    "\n",
    "\n",
    "        batch_size, seq_length = inputs_embeds.shape[:2]\n",
    "\n",
    "        attention_mask = None\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        next_decoder_cache = None\n",
    "\n",
    "        for decoder_layer in self.layers:\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "\n",
    "            layer_outputs = decoder_layer(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_value=past_key_values,\n",
    "                output_attentions=output_attentions,\n",
    "                use_cache=use_cache,\n",
    "            )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if use_cache:\n",
    "                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "        hidden_states = self.final_layernorm(hidden_states)\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        next_cache = None\n",
    "        if use_cache:\n",
    "            next_cache = next_decoder_cache.to_legacy_cache() if use_legacy_cache else next_decoder_cache\n",
    "        last_hidden_state=hidden_states\n",
    "        past_key_values=next_cache\n",
    "        hidden_states=all_hidden_states\n",
    "        attentions=all_self_attns\n",
    "        return (\n",
    "            last_hidden_state,\n",
    "            past_key_values,\n",
    "            hidden_states,\n",
    "            attentions\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "_JN1PZScH6hT"
   },
   "outputs": [],
   "source": [
    "class ForCausalLM():\n",
    "\n",
    "\n",
    "    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.__init__ with Llama->Phi,bias=False->bias=True\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        self.model = Model(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = Linear_np(config.hidden_size, config.vocab_size, bias=True)\n",
    "\n",
    "        weights =np.load(f'tinyllama/lm_head.weight_weights.npy', mmap_mode='r')\n",
    "\n",
    "\n",
    "\n",
    "        self.lm_head.load(weights, bias=None)\n",
    "\n",
    "\n",
    "\n",
    "    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.get_input_embeddings\n",
    "    def get_input_embeddings(self):\n",
    "        return self.model.embed_tokens\n",
    "\n",
    "    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_input_embeddings\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.model.embed_tokens = value\n",
    "\n",
    "    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.get_output_embeddings\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_output_embeddings\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_decoder\n",
    "    def set_decoder(self, decoder):\n",
    "        self.model = decoder\n",
    "\n",
    "    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.get_decoder\n",
    "    def get_decoder(self):\n",
    "        return self.model\n",
    "\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        input_ids = None,\n",
    "        attention_mask = None,\n",
    "        position_ids = None,\n",
    "        past_key_values = None,\n",
    "        inputs_embeds = None,\n",
    "        labels = None,\n",
    "        use_cache = None,\n",
    "        output_attentions = None,\n",
    "        output_hidden_states= None,\n",
    "        return_dict = None,\n",
    "    ) :\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = False #return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        #logits = softmax(logits, axis=-1)\n",
    "        logits = logits\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        logits=logits\n",
    "        past_key_values=outputs[1]\n",
    "        hidden_states=outputs[2]\n",
    "        attentions=outputs[3]\n",
    "\n",
    "        return (\n",
    "            loss,\n",
    "            logits,\n",
    "            past_key_values,\n",
    "            hidden_states,\n",
    "            attentions,\n",
    "        )\n",
    "\n",
    "    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.prepare_inputs_for_generation\n",
    "    def prepare_inputs_for_generation(\n",
    "        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n",
    "    ):\n",
    "        if past_key_values is not None:\n",
    "\n",
    "            if isinstance(past_key_values, Cache):\n",
    "                cache_length = past_key_values.get_seq_length()\n",
    "                past_length = past_key_values.seen_tokens\n",
    "                max_cache_length = past_key_values.get_max_length()\n",
    "            else:\n",
    "                cache_length = past_length = past_key_values[0][0].shape[2]\n",
    "                max_cache_length = None\n",
    "\n",
    "            # Keep only the unprocessed tokens:\n",
    "            # 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where\n",
    "            # some of the inputs are exclusively passed as part of the cache (e.g. when passing input_embeds as\n",
    "            # input)\n",
    "            if attention_mask is not None and attention_mask.shape[1] > input_ids.shape[1]:\n",
    "                input_ids = input_ids[:, -(attention_mask.shape[1] - past_length) :]\n",
    "            # 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard\n",
    "            # input_ids based on the past_length.\n",
    "            elif past_length < input_ids.shape[1]:\n",
    "                input_ids = input_ids[:, past_length:]\n",
    "            # 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.\n",
    "\n",
    "            # If we are about to go beyond the maximum cache length, we need to crop the input attention mask.\n",
    "            if (\n",
    "                max_cache_length is not None\n",
    "                and attention_mask is not None\n",
    "                and cache_length + input_ids.shape[1] > max_cache_length\n",
    "            ):\n",
    "                attention_mask = attention_mask[:, -max_cache_length:]\n",
    "\n",
    "        position_ids = kwargs.get(\"position_ids\", None)\n",
    "        if attention_mask is not None and position_ids is None:\n",
    "            # create position_ids on the fly for batch generation\n",
    "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "            if past_key_values:\n",
    "                position_ids = position_ids[:, -input_ids.shape[1] :]\n",
    "\n",
    "        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n",
    "        if inputs_embeds is not None and past_key_values is None:\n",
    "            model_inputs = {\"inputs_embeds\": inputs_embeds}\n",
    "        else:\n",
    "            model_inputs = {\"input_ids\": input_ids}\n",
    "\n",
    "        model_inputs.update(\n",
    "            {\n",
    "                \"position_ids\": position_ids,\n",
    "                \"past_key_values\": past_key_values,\n",
    "                \"use_cache\": kwargs.get(\"use_cache\"),\n",
    "                \"attention_mask\": attention_mask,\n",
    "            }\n",
    "        )\n",
    "        return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "AhJXMvWsIDGL"
   },
   "outputs": [],
   "source": [
    "model = ForCausalLM(config1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "xhhg1_vGII6r"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9kkqdHoMIR-A",
    "outputId": "92d453fa-97af-4cca-8ecd-a49adb3fddd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def sample(probabilities, n):\n",
    "    probabilities = softmax(probabilities)\n",
    "    try:\n",
    "    # Sample index from probabilities, they must sum to 1\n",
    "        r = random.random()\n",
    "        probabilities = probabilities\n",
    "        cdf = 0.0\n",
    "        for i in range(n):\n",
    "            cdf += probabilities[i]\n",
    "            if r < cdf:\n",
    "                return i\n",
    "        return n - 1  # In case of rounding errors\n",
    "    except:\n",
    "        print(probabilities)\n",
    "\n",
    "# Example usage\n",
    "probabilities = [0.1, 0.2, 0.3, 0.4]  # Example list of probabilities\n",
    "n = len(probabilities)  # Number of probabilities\n",
    "index = sample(probabilities, n)\n",
    "print(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ht0_ZEb_Yb2O",
    "outputId": "b87e1902-dc3d-4cec-c57b-6b08f5176d11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "<|user|>: hi \n",
      "<|assistant|>: Sure\n",
      " \n",
      "<|user|>: hi \n",
      "<|assistant|>: Sure,\n",
      " \n",
      "<|user|>: hi \n",
      "<|assistant|>: Sure, here\n",
      " \n",
      "<|user|>: hi \n",
      "<|assistant|>: Sure, here'\n",
      " \n",
      "<|user|>: hi \n",
      "<|assistant|>: Sure, here's\n"
     ]
    }
   ],
   "source": [
    "prompt = 'once upon a time there was little boy'\n",
    "prompt= '''\n",
    "CONTEXT\n",
    "The Great Wall of China is a series of fortifications made of stone, brick, tamped earth, wood, and other materials, generally built along an east-to-west line across the historical northern borders of China to protect the Chinese states and empires against the raids and invasions of the various nomadic groups of the Eurasian Steppe.\n",
    "\n",
    "The primary purpose of the Great Wall of China is\n",
    "'''\n",
    "prompt = '</s> \\n<|user|>: hi \\n<|assistant|>:'\n",
    "\n",
    "inp = tokenizer.encode(prompt, add_special_tokens=False )\n",
    "inps = model.prepare_inputs_for_generation(\n",
    "        inp, past_key_values=None, attention_mask=None, inputs_embeds=None,\n",
    "    )\n",
    "\n",
    "input_ids = tokenizer.encode(prompt)\n",
    "\n",
    "# Generate text token by token\n",
    "\n",
    "for _ in range(5):\n",
    "    outputs = model(np.array([input_ids]))\n",
    "    predictions = outputs[1]\n",
    "\n",
    "    # Get the predicted next token (you could apply more complex strategies here)\n",
    "    next_token_id = np.argmax(predictions[:, -1, :], axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "    input_ids.append(next_token_id[0])\n",
    "    print(tokenizer.decode(input_ids, skip_special_tokens=True))\n",
    "\n",
    "    # Check if the generation reached an end-of-sequence token\n",
    "    if next_token_id.item() == tokenizer.eos_token_id:\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L5pwkbHbc6BG",
    "outputId": "36773c0e-07c0-4ecb-9b3d-4138478e3344"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2278]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(predictions[:, -1, :], axis=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-OHnaUTxc7DU",
    "outputId": "06a70c54-8624-4bec-9dd2-8c123b49000f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2278])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[:, -1, :].numpy(), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k1ZzCHDUcAaw",
    "outputId": "60c2d675-a15e-41d4-b214-8c56841d6027"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1,  2748,  2501,   263,   931,   727,   471,  2217,  8023,\n",
       "         4257,  5457, 29889,  5457,   471,   263,  9796,   322,  2562,\n",
       "         9021,  2278]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "kJra9ajtb36b",
    "outputId": "1b3cec15-3206-48e3-9611-f60b74e935b5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'once upon a time there was little boy named Jack. Jack was a happy and carefree child'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "mWCWl_ltIWCY"
   },
   "outputs": [],
   "source": [
    "def generate(prompt, max_tokens=20, streamer=None):\n",
    "    inp = tokenizer.encode(prompt, add_special_tokens=False )\n",
    "    inps = model.prepare_inputs_for_generation(\n",
    "            inp, past_key_values=None, attention_mask=None, inputs_embeds=None,\n",
    "        )\n",
    "    past_key_values = None\n",
    "    res = prompt\n",
    "    print(prompt, end=\"\")\n",
    "    input_ids = tokenizer.encode(prompt)\n",
    "\n",
    "    # Generate text token by token\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        outputs = model(np.array([input_ids]), use_cache=False)\n",
    "        predictions = outputs[1]\n",
    "\n",
    "        # Get the predicted next token (you could apply more complex strategies here)\n",
    "        next_token_id = np.argmax(predictions[:, -1, :], axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "        input_ids.append(next_token_id[0])\n",
    "        print(tokenizer.decode(input_ids, skip_special_tokens=True), end=\"\")\n",
    "\n",
    "        # Check if the generation reached an end-of-sequence token\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "gJjXUl-zJWjY",
    "outputId": "b6ceab3e-5212-4727-994f-0d9a3b829135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s> \n",
      "<|user|>: hi \n",
      "<|assistant|>: \n",
      "<|user|>: hi \n",
      "<|assistant|>: Sure \n",
      "<|user|>: hi \n",
      "<|assistant|>: Sure, \n",
      "<|user|>: hi \n",
      "<|assistant|>: Sure, here \n",
      "<|user|>: hi \n",
      "<|assistant|>: Sure, here'"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m</s> \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m<|user|>: hi \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m<|assistant|>:\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[41], line 14\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(prompt, max_tokens, streamer)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Generate text token by token\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_tokens):\n\u001b[0;32m---> 14\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Get the predicted next token (you could apply more complex strategies here)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[36], line 66\u001b[0m, in \u001b[0;36mForCausalLM.__call__\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m     63\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;66;03m#return_dict if return_dict is not None else self.config.use_return_dict\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     79\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "Cell \u001b[0;32mIn[35], line 93\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m     91\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[0;32m---> 93\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "Cell \u001b[0;32mIn[31], line 56\u001b[0m, in \u001b[0;36mDecoderLayer.__call__\u001b[0;34m(self, hidden_states, attention_mask, position_ids, output_attentions, use_cache, past_key_value)\u001b[0m\n\u001b[1;32m     53\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m     55\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m---> 56\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m  residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m     58\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "Cell \u001b[0;32mIn[23], line 30\u001b[0m, in \u001b[0;36mMLP_np.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x) :\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[22], line 18\u001b[0m, in \u001b[0;36mLinear_np.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdot(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generate('</s> \\n<|user|>: hi \\n<|assistant|>:', max_tokens=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "WI49uFsTs-NJ"
   },
   "outputs": [],
   "source": [
    "prompt = 'once upon a time there'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "pZ9psqVYscMU"
   },
   "outputs": [],
   "source": [
    "inp = tokenizer.encode(prompt, add_special_tokens=False )\n",
    "inps = model.prepare_inputs_for_generation(\n",
    "            np.array([inp]),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "iiQb75-BscO6"
   },
   "outputs": [],
   "source": [
    "ou1 = model(inps['input_ids'], use_cache=False)[1]\n",
    "ou = model1(torch.tensor(inps['input_ids']), use_cache=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QblWrwlxscRg",
    "outputId": "7dece3fe-f2d5-4fc1-d115-7cb337cf265a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(ou[-1], torch.Tensor(ou1[-1]), rtol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83RnTRNoWAN8",
    "outputId": "9817cc6a-a3ac-498a-9e70-fcc59e3969fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 32000])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ou[-1][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xhs5WdMyscUI",
    "outputId": "3ae8d63c-affa-4068-80c0-906308e3ec0a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32000,)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ou1[-1][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y1g5m9ieubjN",
    "outputId": "0fbc942d-a66c-46aa-a5b9-bb3c6ccf9f1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 32000)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ou1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LAZAaOettPlM",
    "outputId": "69c0e007-462b-4dc7-9ca3-c29a6d4c1eba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29892"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ou[-1][-1].detach().numpy()\n",
    "np.argmax(ou[-1][-1].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HzSl88e6scW-",
    "outputId": "5db6df74-3866-43fa-e160-38aada118850"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    }
   ],
   "source": [
    "val = sample(ou[-1][-1].detach().numpy(), vocab_size)\n",
    "\n",
    "\n",
    "prompt = tokenizer.decode(val, skip_special_tokens=True)\n",
    "res += prompt\n",
    "print(prompt, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "YqfaKEAFJYOq"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model1 = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_DJ8yvIrPFp5",
    "outputId": "2a6f3421-9349-45bc-e024-8a25ca561502"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.2355665 , -0.01450975,  0.18568064, ...,  0.00186091,\n",
       "         -0.14888893, -0.00527345]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = Attention(config1, 0)\n",
    "x = np.random.randn(1,1,2048)\n",
    "inp = torch.Tensor(x)\n",
    "attn(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "8Iq4ZVYLOQvl"
   },
   "outputs": [],
   "source": [
    "inp = torch.Tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t8R-NJugJ3rP",
    "outputId": "7ea6a99f-a7cf-4004-8ffd-b06317f9e716"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.2356, -0.0145,  0.1857,  ...,  0.0019, -0.1489, -0.0053]]],\n",
       "        grad_fn=<UnsafeViewBackward0>),\n",
       " None,\n",
       " None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.model.layers[0].self_attn(hidden_states=inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E_DToMtHQ0cS",
    "outputId": "be9f2deb-ff1f-4937-c744-1a766f74db10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config1.num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "id": "1X3Us2bwaCjF"
   },
   "outputs": [],
   "source": [
    "x = np.random.randn(1,1,2048).astype('f')\n",
    "inp = torch.Tensor(x)\n",
    "\n",
    "a = model.lm_head(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "5RWX0gqZbjfi",
    "outputId": "c48a3cd3-1b80-4f09-af7b-31aed6366fc1"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-233-3d587fba8ec5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_flash_attn_2_enabled\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2233\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "b = model1.lm_head(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fHp6K6KGxW5l",
    "outputId": "8e9cf080-d716-485b-d005-f1091115ef2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23641"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GAAqZ9YSxZeL",
    "outputId": "d5b51518-49e2-4eb2-9697-b1a3981468b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23641"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(b.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cZMX4dCmb0da",
    "outputId": "069ad3fb-1947-4ee2-b86a-4a80be2d020c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randn(1,1,2048).astype('f')\n",
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xAOkpuaZq9d"
   },
   "outputs": [],
   "source": [
    "attn = Attention(config1, 0)\n",
    "x = np.random.randn(1,1,2048).astype('f')\n",
    "\n",
    "inp = torch.Tensor(x)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(22):\n",
    "  attn = model.model.layers[i].self_attn\n",
    "  x = np.random.randn(1,1,2048).astype('f')\n",
    "\n",
    "  inp = torch.Tensor(x)\n",
    "  og = attn(x)[0]\n",
    "  ot = model1.model.layers[i].self_attn(hidden_states=inp)[0]\n",
    "\n",
    "  print(i,   torch.isclose(torch.Tensor(og), ot, rtol=1e-3 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "aBeVGTUkeenn"
   },
   "outputs": [],
   "source": [
    "attns = []\n",
    "for i in range(22):\n",
    "  attn = Attention(config1, i)\n",
    "  attns.append(attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1c8Oa_XeUpV"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(22):\n",
    "  attn = attns[i]\n",
    "  x = np.random.randn(1,1,2048).astype('f')\n",
    "\n",
    "  inp = torch.Tensor(x)\n",
    "  og = attn(x)[0]\n",
    "  ot = model1.model.layers[i].self_attn(hidden_states=inp)[0]\n",
    "\n",
    "  print(i,   torch.isclose(torch.Tensor(og), ot, rtol=1e-3 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "j1uIG4t_U3BY"
   },
   "outputs": [],
   "source": [
    "decoders = []\n",
    "for i in range(config1.num_hidden_layers):\n",
    "  dec = DecoderLayer(config1, i)\n",
    "  decoders.append(dec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lp90oP7wOEvZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "for i in range(22):\n",
    "  dec = decoders[i]\n",
    "  x = np.random.randn(1,1,2048).astype('f')\n",
    "\n",
    "  inp = torch.Tensor(x)\n",
    "  og = dec(x)[0]\n",
    "  ot = model1.model.layers[i](hidden_states=inp)[0]\n",
    "\n",
    "  print(i,   torch.isclose(torch.Tensor(og), ot, rtol=1e-3 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vP2vrxdzRbLN",
    "outputId": "12d7aeee-5f9b-454c-d750-88ee4e1162de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5771,  0.4140, -0.6160,  ...,  1.6984, -1.4095, -0.0606]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(2):\n",
    "  inp = model1.model.layers[i](inp)[0]\n",
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5dFpgrfHPmPv",
    "outputId": "9dab2bd8-6c9b-4a07-bd52-05551c86c7a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.4942, -0.7665, -0.0165,  ...,  0.5256, -0.2423,  0.0855]]],\n",
       "        grad_fn=<AddBackward0>),)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.model.layers[0](hidden_states=inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "kCJAdBUHPpWv"
   },
   "outputs": [],
   "source": [
    "inp = tokenizer.encode('once upon a time', add_special_tokens=False )\n",
    "inp = tokenizer.encode('once upon a time', add_special_tokens=False )\n",
    "inps = model.prepare_inputs_for_generation(\n",
    "            inp, past_key_values=None, attention_mask=None, inputs_embeds=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hvx0vrljrZUH",
    "outputId": "48e3e309-1e29-4104-b044-de4986d94352"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[2748, 2501]]),\n",
       " 'position_ids': None,\n",
       " 'past_key_values': None,\n",
       " 'use_cache': None,\n",
       " 'attention_mask': None}"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "y2uy19mHjo79",
    "outputId": "1e5cf65f-fa18-47a2-9b9a-e184a0670104"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-234-e55dd0018c14>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mou1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mposition_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m             position_ids = torch.arange(\n\u001b[1;32m    874\u001b[0m                 \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "ou1 = model1.model(inps['input_ids'],)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5iGYBfvZyrk-",
    "outputId": "5d0b84c9-8382-4adf-9ebc-bbcd6a4f0510"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "id": "WpBnpMVAyw78"
   },
   "outputs": [],
   "source": [
    "inp = tokenizer.encode('once upon', add_special_tokens=False )\n",
    "inps = model.prepare_inputs_for_generation(\n",
    "            np.array([inp]),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xssqoTo4zBje",
    "outputId": "83c922b0-2cbe-4364-e865-765b2b82591b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[2748, 2501]]),\n",
       " 'position_ids': None,\n",
       " 'past_key_values': None,\n",
       " 'use_cache': None,\n",
       " 'attention_mask': None}"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "id": "D8adKyoO0xNi"
   },
   "outputs": [],
   "source": [
    "x = model.model.get_input_embeddings()[inps['input_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HQPvTaK_6mf2",
    "outputId": "666707c7-25cb-4b40-b52e-a13afa104bf5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 2048)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SSVkRWSQ1apw",
    "outputId": "d897ac02-a0e8-4b80-debc-5a2fdc0eea46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0175, -0.0198, -0.0063,  ..., -0.0039,  0.0028,  0.0035],\n",
       "         [ 0.0019,  0.0018, -0.0049,  ...,  0.0093, -0.0027,  0.0070]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.model.get_input_embeddings()(torch.tensor(inps['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GpvvofJ17BbC",
    "outputId": "c441ca54-6141-4fe4-eb77-befa6feba13b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[2748, 2501]]),\n",
       " 'position_ids': None,\n",
       " 'past_key_values': None,\n",
       " 'use_cache': None,\n",
       " 'attention_mask': None}"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s3NKZZ7d-nRC",
    "outputId": "85053782-89d7-4257-f7a6-91a63ad8ca59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1]])"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_ids = torch.arange(\n",
    "                0, 2, dtype=torch.long\n",
    "            )\n",
    "position_ids = position_ids.unsqueeze(0)\n",
    "position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nX35vkfC-aYq",
    "outputId": "fc475318-36c9-446f-d597-02da8620a922"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps['input_ids'].shape[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "id": "UBJt9Y2xP3tT"
   },
   "outputs": [],
   "source": [
    "ou1 = model.model(inps['input_ids'],)[0]\n",
    "ou = model1.model(torch.tensor(inps['input_ids'])).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "id": "JyXI99Z-IdXz"
   },
   "outputs": [],
   "source": [
    "mask = torch.zeros(1,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "id": "3yuAmn-NIYW_"
   },
   "outputs": [],
   "source": [
    "ou = model1.model(torch.tensor(inps['input_ids']), attention_mask=mask).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "st2yy6bvJU4-",
    "outputId": "77f782b1-41e9-460a-cf58-06890a6df357"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 8)"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m =  np.triu(np.ones((1, 8)) * -1e9, k=1)\n",
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lPKWvJUIL13j",
    "outputId": "e5482536-a1f5-4702-d9a9-cf4c45966e2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.07360245e-01, -3.40282347e+38, -3.40282347e+38,\n",
       "        -3.40282347e+38],\n",
       "       [ 7.78231403e-01,  5.85227053e-01, -3.40282347e+38,\n",
       "        -3.40282347e+38],\n",
       "       [ 9.13163643e-01,  7.81205803e-01,  3.25738826e-01,\n",
       "        -3.40282347e+38],\n",
       "       [ 1.32595720e-01,  6.71988259e-01,  6.83150214e-02,\n",
       "         5.01991165e-01]])"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_masked_context_mask(mask, diagonal=0, dtype=np.float32):\n",
    "    \"\"\"\n",
    "    Apply a context mask to the given mask array.\n",
    "\n",
    "    :param mask: The original mask array (2D or 4D).\n",
    "    :param diagonal: The diagonal offset for the upper triangular part.\n",
    "    :param dtype: The data type to determine the minimum value.\n",
    "    :return: The masked array with applied context mask.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create an upper triangular matrix (context mask)\n",
    "    context_mask = 1 - np.tril(np.ones_like(mask, dtype=int), k=0)\n",
    "\n",
    "\n",
    "    # Apply the context mask to the original mask\n",
    "    masked_mask = np.where(context_mask, np.finfo(dtype).min, mask)\n",
    "\n",
    "    return masked_mask\n",
    "\n",
    "# Example usage\n",
    "mask = np.random.rand(4, 4)  # Replace with your actual mask\n",
    "diagonal = 0\n",
    "masked_mask = create_masked_context_mask(mask, diagonal)\n",
    "masked_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JtSpVfxEN4Ew",
    "outputId": "462e1fb8-4a6b-4956-fd5d-a7a9dc5733ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0],\n",
       "       [1, 1, 0, 0],\n",
       "       [1, 1, 1, 0],\n",
       "       [1, 1, 1, 1]])"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tril(np.ones_like(mask, dtype=int), k=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9NMN2XZhO8AQ",
    "outputId": "793dfe19-ae77-48a7-d8aa-88786d23c356"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gLox6EdeJ8cv",
    "outputId": "9e002ede-63d7-46d7-bacb-9bb10392ee1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0112,  0.1805, -0.0019,  ..., -0.2434,  0.0460,  0.0762],\n",
       "         [ 0.0561,  0.1115,  0.0196,  ..., -0.5309,  0.1469,  0.0058],\n",
       "         [ 0.0305,  0.0511,  0.0558,  ..., -0.2500,  0.0426, -0.0475],\n",
       "         ...,\n",
       "         [ 0.0514,  0.1340, -0.0147,  ..., -0.2421,  0.0670,  0.0383],\n",
       "         [ 0.0742,  0.0647, -0.0473,  ..., -0.4106,  0.0404, -0.0191],\n",
       "         [ 0.0521,  0.1862,  0.0205,  ..., -0.4150,  0.0413,  0.0671]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.model(torch.tensor(inps['input_ids']), attention_mask=torch.tensor(modified_mask)).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "im3yZXSkIbzB",
    "outputId": "fa7ac163-4990-40e5-9713-692bf26f65b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1165e-02,  1.8055e-01, -1.8866e-03,  ..., -2.4341e-01,\n",
       "           4.5975e-02,  7.6172e-02],\n",
       "         [ 3.7167e-01,  8.5975e-02,  4.4363e-01,  ..., -2.5829e+00,\n",
       "           3.8793e-01, -2.0166e-01],\n",
       "         [ 1.7158e+00,  8.1196e-02, -2.3189e-02,  ..., -1.8756e+00,\n",
       "          -5.0874e-01,  1.1068e+00],\n",
       "         ...,\n",
       "         [ 2.9602e-01, -9.0776e-01, -2.9161e-01,  ..., -5.0614e+00,\n",
       "           3.5623e-01, -4.4828e-01],\n",
       "         [-2.2318e-01, -5.3769e-01, -1.4666e-01,  ..., -5.4657e-01,\n",
       "           3.1356e-01, -1.3549e-01],\n",
       "         [-3.9934e-02,  1.8208e-02, -1.5282e-01,  ..., -4.5241e+00,\n",
       "           1.3718e-01, -3.5677e-01]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.model(torch.tensor(inps['input_ids'])).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-dXSsCGyI_9X",
    "outputId": "fc66f99a-ad67-4a97-9034-a157c1bf586b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1165e-02,  1.8055e-01, -1.8866e-03,  ..., -2.4341e-01,\n",
       "           4.5975e-02,  7.6172e-02],\n",
       "         [ 3.7167e-01,  8.5975e-02,  4.4363e-01,  ..., -2.5829e+00,\n",
       "           3.8793e-01, -2.0166e-01],\n",
       "         [ 1.7158e+00,  8.1196e-02, -2.3189e-02,  ..., -1.8756e+00,\n",
       "          -5.0874e-01,  1.1068e+00],\n",
       "         ...,\n",
       "         [ 2.9602e-01, -9.0776e-01, -2.9161e-01,  ..., -5.0614e+00,\n",
       "           3.5623e-01, -4.4828e-01],\n",
       "         [-2.2318e-01, -5.3769e-01, -1.4666e-01,  ..., -5.4657e-01,\n",
       "           3.1356e-01, -1.3549e-01],\n",
       "         [-3.9934e-02,  1.8208e-02, -1.5282e-01,  ..., -4.5241e+00,\n",
       "           1.3718e-01, -3.5677e-01]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.model(torch.tensor(inps['input_ids'])).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "id": "lvxT6MHCCNC7"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class LlamaRMSNorm1(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        LlamaRMSNorm is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "id": "DzHCfAD-CJb9"
   },
   "outputs": [],
   "source": [
    "model1.model.norm = LlamaRMSNorm1(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sTaUGaoVx2s2",
    "outputId": "0eead21d-3027-4385-c28c-90d815e9a183"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-3.15384308, -1.24769725, -0.37631961, ...,  3.86259941,\n",
       "          1.18668998, -0.153979  ],\n",
       "        [-3.14749838, -1.33593831, -0.57573195, ...,  3.75710779,\n",
       "          1.10185667, -0.38560823]]])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ou1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6IKxmVFyqSnS",
    "outputId": "31ae8fd0-6db0-4e6a-9ef4-ac1bafedcb12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1164e-02,  1.8055e-01, -1.8863e-03,  ..., -2.4341e-01,\n",
       "           4.5975e-02,  7.6172e-02],\n",
       "         [ 3.7168e-01,  8.5976e-02,  4.4363e-01,  ..., -2.5829e+00,\n",
       "           3.8793e-01, -2.0166e-01]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUOKkfh0qY6w",
    "outputId": "9384dc8a-ab4f-464c-dc3a-2630eb3d59e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32000,)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ou1[-1][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "XC1qgL6CQGHB"
   },
   "outputs": [],
   "source": [
    "x = np.random.randn(1,1,2048)\n",
    "y = x\n",
    "inp = torch.Tensor(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "id": "JRi9zpb-7ZbS"
   },
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0r0JgZ9SB_gC",
    "outputId": "40152f54-aa04-4cd1-b594-83b19b2469e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.01745605, -0.01977539, -0.0062561 , ..., -0.00393677,\n",
       "          0.00280762,  0.00346375],\n",
       "        [ 0.0019455 ,  0.00180054, -0.00491333, ...,  0.00933838,\n",
       "         -0.00265503,  0.00704956]]], dtype=float32)"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "EMdhqbNq6vJf",
    "outputId": "8e5989ca-94ec-4385-82e0-6eac74e9d398"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-348-f392a1ef8d1d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-5cdbda125235>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/memmap.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mmemmap\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mmap\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "hs = copy.deepcopy(x)\n",
    "model.model(hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "dfeh_t6eB4yn",
    "outputId": "7f3dc85b-2e1d-44bd-f0d0-a49fb8584e3a"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected m1 and m2 to have the same dtype, but got: double != float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-393-960f82c22e41>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    673\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m             \u001b[0mquery_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m             \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected m1 and m2 to have the same dtype, but got: double != float"
     ]
    }
   ],
   "source": [
    "hs = copy.deepcopy(x)\n",
    "hs = torch.tensor(hs)\n",
    "model1.model.layers[0](hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wexC2tg97eft",
    "outputId": "46556ec5-f03a-4229-ee8d-7c1e88a27332"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.43325806, -0.59864963, -0.16895729, ...,  1.70678401,\n",
       "          0.55277126, -0.06969217],\n",
       "        [-1.42975007, -0.64070808, -0.25837512, ...,  1.65944496,\n",
       "          0.51303097, -0.17445325]]])"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(22):\n",
    "  hs = model.model.layers[i](hs, use_cache=False)[0]\n",
    "\n",
    "hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7mHLHeif8Trz",
    "outputId": "43c5b024-39e2-4b3b-c4e5-ea4dd4c40917"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-3.15384308, -1.24769725, -0.37631961, ...,  3.86259941,\n",
       "          1.18668998, -0.153979  ],\n",
       "        [-3.14749838, -1.33593831, -0.57573195, ...,  3.75710779,\n",
       "          1.10185667, -0.38560823]]])"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.final_layernorm(hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0tNszHh8RVG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QmrX9oQATk-d",
    "outputId": "4d67151a-7f14-401e-9cb7-829e5130032c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.4333, -0.5986, -0.1690,  ...,  1.7068,  0.5528, -0.0697],\n",
       "         [-1.4297, -0.6407, -0.2584,  ...,  1.6594,  0.5130, -0.1745]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs = copy.deepcopy(x)\n",
    "hs = torch.tensor(hs)\n",
    "for i in range(22):\n",
    "  hs = model1.model.layers[i](hs)[0]\n",
    "hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5JNjjikp8bEN",
    "outputId": "409854e8-0348-48da-b7ad-34c8c0d8905f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.1538, -1.2477, -0.3763,  ...,  3.8626,  1.1867, -0.1540],\n",
       "         [-3.1475, -1.3359, -0.5757,  ...,  3.7571,  1.1019, -0.3856]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.model.norm(hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "id": "8-QlQs57_lYu"
   },
   "outputs": [],
   "source": [
    "hs = copy.deepcopy(x)\n",
    "hs = torch.tensor(hs)\n",
    "batch_size, seq_length = hs.shape[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "id": "m8WqaH2t_lbU"
   },
   "outputs": [],
   "source": [
    "position_ids = torch.arange(\n",
    "                0, seq_length + 0, dtype=torch.long\n",
    "            )\n",
    "position_ids = position_ids.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OheMXoN-Acg5",
    "outputId": "6f2c9326-300b-4ea5-be46-473509eb33dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0175, -0.0198, -0.0063,  ..., -0.0039,  0.0028,  0.0035],\n",
       "         [ 0.0019,  0.0018, -0.0049,  ...,  0.0093, -0.0027,  0.0070]]])"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "id": "d8t1X5Lx_ld9"
   },
   "outputs": [],
   "source": [
    "hidden_states = hs\n",
    "for decoder_layer in model1.model.layers:\n",
    "\n",
    "    layer_outputs = decoder_layer(\n",
    "        hidden_states,\n",
    "        position_ids=position_ids\n",
    "    )\n",
    "\n",
    "    hidden_states = layer_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-qeBp_iu_lgj",
    "outputId": "be30a946-a19e-4af2-851f-bba610ef311e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.4333, -0.5986, -0.1690,  ...,  1.7068,  0.5528, -0.0697],\n",
       "         [-1.4297, -0.6407, -0.2584,  ...,  1.6594,  0.5130, -0.1745]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gg9AqSVD_ljK",
    "outputId": "9c2831d6-ee3e-4e08-8e1e-98429107f58a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.1538, -1.2477, -0.3763,  ...,  3.8626,  1.1867, -0.1540],\n",
       "         [-3.1475, -1.3359, -0.5757,  ...,  3.7571,  1.1019, -0.3856]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.model.norm(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "phwWcYmo_llx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6430ZFF_loY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6IyAULiv_lrO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfpGrV95_luG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HW6S8pR_lxQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-W3tWtvUUF10",
    "outputId": "92d3652f-0378-400f-ae71-cae2dab1ace2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True,  ..., True, True, True]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "m = RMSNorm(2048, layer_idx=10)\n",
    "m(x)\n",
    "\n",
    "torch.isclose(torch.Tensor(m(x)), model1.model.layers[10].input_layernorm(inp), rtol=1e-3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oSdVkrufUIwJ",
    "outputId": "e2f9f686-7f0a-4851-e1ed-94aa8471ee0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[[-0.94062658,  0.21811317,  0.38043982, ...,  1.18543976,\n",
      "          2.22400013, -1.09295196]]]),)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9406,  0.2181,  0.3804,  ...,  1.1854,  2.2240, -1.0930]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randn(1,1,2048)\n",
    "y = x\n",
    "inp = torch.Tensor(x)\n",
    "\n",
    "\n",
    "print(decoders[0](x))\n",
    "model1.model.layers[0](inp)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AMYy8dL3V85e",
    "outputId": "29918f20-1171-4a93-e977-665a70bd76d2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-55-86fc63ca5fd1>:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  torch.isclose(torch.Tensor(decoders[0](x)),model1.model.layers[0](inp)[0], rtol=1e-2 )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[True, True, True,  ..., True, True, True]]]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(torch.Tensor(decoders[0](x)),model1.model.layers[0](inp)[0], rtol=1e-2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BUhi5lCwWayg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
